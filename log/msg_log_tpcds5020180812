2018-08-12 12:09:48: Connected to database [gabsam] with user [tpcds50]
2018-08-12 12:09:48: Connection pool instantiated with [1] connections
2018-08-12 12:09:50: ('spark.cores.max', '20')
2018-08-12 12:09:50: ('spark.executor.memory', '2g')
2018-08-12 12:09:50: ('spark.driver.port', '9548')
2018-08-12 12:09:50: ('spark.executor.id', 'driver')
2018-08-12 12:09:50: ('spark.python.worker.reuse', 'True')
2018-08-12 12:09:50: ('spark.driver.maxResultSize', '1g')
2018-08-12 12:09:50: ('spark.app.id', 'app-20180812120950-0003')
2018-08-12 12:09:50: ('spark.sql.shuffle.partitions', '80')
2018-08-12 12:09:50: ('spark.default.parallelism', '80')
2018-08-12 12:09:50: ('spark.executor.instances', '20')
2018-08-12 12:09:50: ('spark.logConf', 'False')
2018-08-12 12:09:50: ('spark.rdd.compress', 'True')
2018-08-12 12:09:50: ('spark.driver.host', 'tempvcenter.rs2.com')
2018-08-12 12:09:50: ('spark.driver.memory', '2g')
2018-08-12 12:09:50: ('spark.serializer.objectStreamReset', '100')
2018-08-12 12:09:50: ('spark.executor.cores', '1')
2018-08-12 12:09:50: ('spark.master', 'spark://cisk.rs2.com:7077')
2018-08-12 12:09:50: ('spark.submit.deployMode', 'client')
2018-08-12 12:09:50: ('spark.executorEnv.PYTHONPATH', '$PYTHONPATH:/home/gabriels/ICS5200')
2018-08-12 12:09:50: ('spark.ui.showConsoleProgress', 'true')
2018-08-12 12:09:50: ('spark.app.name', 'ICS5200')
2018-08-12 12:09:51: Generating TPC-DS data for volumes of [50]..
2018-08-12 12:17:09: Exception caught whilst establishing connection to database! [DPI-1047: 64-bit Oracle Client library cannot be loaded: "libclntsh.so: cannot open shared object file: No such file or directory". See https://oracle.github.io/odpi/doc/installation.html#linux for help]
2018-08-12 12:21:07: Connected to database [gabsam] with user [tpcds50]
2018-08-12 12:21:07: Connection pool instantiated with [1] connections
2018-08-12 12:21:09: ('spark.driver.port', '21922')
2018-08-12 12:21:09: ('spark.cores.max', '20')
2018-08-12 12:21:09: ('spark.executor.memory', '2g')
2018-08-12 12:21:09: ('spark.driver.host', 'cisk.rs2.com')
2018-08-12 12:21:09: ('spark.executor.id', 'driver')
2018-08-12 12:21:09: ('spark.python.worker.reuse', 'True')
2018-08-12 12:21:09: ('spark.driver.maxResultSize', '1g')
2018-08-12 12:21:09: ('spark.default.parallelism', '80')
2018-08-12 12:21:09: ('spark.sql.shuffle.partitions', '80')
2018-08-12 12:21:09: ('spark.executor.instances', '20')
2018-08-12 12:21:09: ('spark.logConf', 'False')
2018-08-12 12:21:09: ('spark.rdd.compress', 'True')
2018-08-12 12:21:09: ('spark.driver.memory', '2g')
2018-08-12 12:21:09: ('spark.serializer.objectStreamReset', '100')
2018-08-12 12:21:09: ('spark.app.id', 'app-20180812122109-0004')
2018-08-12 12:21:09: ('spark.executor.cores', '1')
2018-08-12 12:21:09: ('spark.master', 'spark://cisk.rs2.com:7077')
2018-08-12 12:21:09: ('spark.submit.deployMode', 'client')
2018-08-12 12:21:09: ('spark.executorEnv.PYTHONPATH', '$PYTHONPATH:/home/gabriels/ICS5200')
2018-08-12 12:21:09: ('spark.ui.showConsoleProgress', 'true')
2018-08-12 12:21:09: ('spark.app.name', 'ICS5200')
2018-08-12 12:21:09: Generating TPC-DS data for volumes of [50]..
