{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule TPC-DS 1 Feature Selection\n",
    "\n",
    "This notebook is dedicated to dataset profiling. In this notebook, feature selection techniques will be implemented so as to categorize which features belay the most information to address the problem at hand - Workload Prediction. Due to the vast feature space which have been gathered during a workload's execution, manual techniques at determining which are most detrimental is not sufficient. \n",
    "\n",
    "Therefore the following work puts emphasis on automated techniques so as to determine out of the vast feature space which are most important to base future models upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Module Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpcds='TPCDS1'\n",
    "#\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_snapshot.csv'\n",
    "rep_hist_sysmetric_summary_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_sysmetric_summary.csv'\n",
    "rep_hist_sysstat_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_sysstat.csv'\n",
    "rep_vsql_plan_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_vsql_plan.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "rep_hist_sysmetric_summary_df = pd.read_csv(rep_hist_sysmetric_summary_path)\n",
    "rep_hist_sysstat_df = pd.read_csv(rep_hist_sysstat_path)\n",
    "rep_vsql_plan_df = pd.read_csv(rep_vsql_plan_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "rep_hist_snapshot_headers = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "rep_hist_sysmetric_summary_headers = prettify_header(rep_hist_sysmetric_summary_df.columns.values)\n",
    "rep_hist_sysstat_headers = prettify_header(rep_hist_sysstat_df.columns.values)\n",
    "rep_vsql_plan_headers = prettify_header(rep_vsql_plan_df.columns.values)\n",
    "#\n",
    "# Replace original headers with a prettified version of the same column list\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path, names=rep_hist_snapshot_headers)\n",
    "rep_hist_sysmetric_summary_df = pd.read_csv(rep_hist_sysmetric_summary_path, names=rep_hist_sysmetric_summary_headers)\n",
    "rep_hist_sysstat_df = pd.read_csv(rep_hist_sysstat_path, names=rep_hist_sysstat_headers)\n",
    "rep_vsql_plan_df = pd.read_csv(rep_vsql_plan_path, names=rep_vsql_plan_headers)\n",
    "#\n",
    "rep_hist_snapshot_df.drop(rep_hist_snapshot_df.index[0],inplace=True)\n",
    "rep_hist_sysmetric_summary_df.drop(rep_hist_sysmetric_summary_df.index[0],inplace=True)\n",
    "rep_hist_sysstat_df.drop(rep_hist_sysstat_df.index[0],inplace=True)\n",
    "rep_vsql_plan_df.drop(rep_vsql_plan_df.index[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "\n",
    "The correlation of resources consumed (y) per snapshot (X) define our feature space. Since the objective here is to attempt to predict what resources will be incurred ahead of time, the problem can be defined as a number of questions:\n",
    "\n",
    "* Q: What resources can I predict to be in usage at point N in time?\n",
    "* Q: What resources should I be predicting that accurately portray a schedule's workload?\n",
    "* Q: What knowledge/data do I have ahead of time which I can use to base my predictions off?\n",
    "\n",
    "Due to the vast feature space in the available metrics monitored and captured during a workload's execution, it is important to rank which attribute is most beneficial than others. Additionally, it is important to analyze such features individually, and considerate of other features in two types of analysis:\n",
    "\n",
    "* Univariate Analysis\n",
    "* Multivariate Analysis\n",
    "\n",
    "Furthermore, multiple types of feature ranking / analysis techniques ara available, amongst which will be considered:\n",
    "\n",
    "* Filter Methods\n",
    "* Wrapper Methods\n",
    "* Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "We apply a number of preprocessing techniques to the presented dataframes, particularly to normalize and/or scale feature vectors into a more suitable representation for downstream estimators:\n",
    "\n",
    "Relative Links:\n",
    "* http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "* https://machinelearningmastery.com/improve-model-accuracy-with-data-pre-processing/\n",
    "* https://machinelearningmastery.com/normalize-standardize-time-series-data-python/\n",
    "\n",
    "## Visualizing Feature Distribution & Skewness\n",
    "\n",
    "In order to decide between a normalization strategy, it is important to understand the underlying data spread. Understanding of dataset mean, variance, skewness on a per column/feature basis helps determine whether a standardization or normalization strategy should be utilized on the datasets.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a normalization preprocessing technique for data distribution that can closely approximate minimum and maximum observable values per column:\n",
    "\n",
    "<i>\"Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\"</i>\n",
    "\n",
    "Normalization formula is stated as follows: $$y=(x-min)/(max-min)$$\n",
    "\n",
    "### Standardization\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a standardization preprocessing technique for data distributions that observe a Gaussian spread, with a mean of 0 and a standard deviation of 1 (approximately close to these values):\n",
    "\n",
    "<i>\"Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\"</i>\n",
    "\n",
    "Standardization formula is stated as follows: $$y=(x-mean)/StandardDeviation$$\n",
    "Mean defined as: $$mean=sum(x)/count(x)$$\n",
    "Standard Deviation defined as: $$StandardDeviation=sqrt(sum((x-mean)^2)/count(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi2 Test on Features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
