{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "This section treats the envisaged dataset as a time series problem. Proposed techniques / methodology here is to:\n",
    "* Covert the featured label into a discrete column value type, instead of continuous.\n",
    "* Time Shift the provided datasets with varied 'lag' values.\n",
    "* Combine all 3 matrices into a agglomorated matrix of 61 + 162 + 1179 features (1402). Duplicate columns ('SNAP_ID') will be reduced to a single one.\n",
    "* Slicing agglomorated matrix into Features/Labels.\n",
    "* Splitting of train/validation/test set.\n",
    "* Perform feature selection on the agglomorated matrix, dropping redundant features and finding the  optimum number of features. (Multivariate analysis - through a wrapper approach)\n",
    "* Feed dataset into a number of machine learning models\n",
    "\n",
    "Applicable links:\n",
    "* https://machinelearningmastery.com/how-to-scale-data-for-long-short-term-memory-networks-in-python/\n",
    "* https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "* https://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/\n",
    "* https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "* https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
    "* https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/\n",
    "\n",
    "### Module Installation and Importing Libraries\n",
    "\n",
    "https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 0.19.1\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "statsmodels: 0.9.0\n",
      "sklearn: 0.18.1\n",
      "theano: 1.0.3\n",
      "tensorflow: 1.11.0\n",
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# statsmodels\n",
    "import statsmodels\n",
    "print('statsmodels: %s' % statsmodels.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# theano\n",
    "import theano\n",
    "print('theano: %s' % theano.__version__)\n",
    "# tensorflow\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "# keras\n",
    "import keras as ke\n",
    "print('keras: %s' % ke.__version__)\n",
    "# math\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment\n",
    "\n",
    "This cell also contains the network structure.\n",
    "\n",
    "https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=1 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "test_split=.3 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "sub_sample_start=350 # Denotes frist 0..n samples (Used for plotting purposes)\n",
    "y_label = ['CPU_TIME_DELTA','OPTIMIZER_COST','EXECUTIONS_DELTA','ELAPSED_TIME_DELTA'] # Denotes which label to use for time series experiments\n",
    "#\n",
    "# Forest Config\n",
    "parallel_degree = 10\n",
    "n_estimators = 100\n",
    "#\n",
    "# LSTM Network Structure eg: \n",
    "#Layers -> [Input, Hidden, Hidden, Output]\n",
    "epochs=50\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2960\u001b[0m                 \u001b[1;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2961\u001b[1;33m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2962\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-5ee777d85df0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrep_hist_sysmetric_summary_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_sysmetric_summary_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mrep_hist_sysstat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_sysstat_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_snapshot.csv'\n",
    "rep_hist_sysmetric_summary_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_sysmetric_summary.csv'\n",
    "rep_hist_sysstat_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_sysstat.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "rep_hist_sysmetric_summary_df = pd.read_csv(rep_hist_sysmetric_summary_path)\n",
    "rep_hist_sysstat_df = pd.read_csv(rep_hist_sysstat_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "rep_hist_sysmetric_summary_df.columns = prettify_header(rep_hist_sysmetric_summary_df.columns.values)\n",
    "rep_hist_sysstat_df.columns = prettify_header(rep_hist_sysstat_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting tables and changing matrix shapes\n",
    "\n",
    "Changes all dataframe shapes to be similar to each other, where in a number of snap_id timestamps are cojoined with instance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Header Lengths [Before Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "#\n",
    "# Table REP_HIST_SYSMETRIC_SUMMARY\n",
    "rep_hist_sysmetric_summary_df = rep_hist_sysmetric_summary_df.pivot(index='SNAP_ID', columns='METRIC_NAME', values='AVERAGE')\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "rep_hist_sysmetric_summary_df[['SNAP_ID']] = rep_hist_sysmetric_summary_df[['SNAP_ID']].astype(int)\n",
    "#rep_hist_sysmetric_summary_df = rep_hist_sysstat_df.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "rep_hist_sysmetric_summary_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "#\n",
    "# Table REP_HIST_SYSSTAT\n",
    "rep_hist_sysstat_df = rep_hist_sysstat_df.pivot(index='SNAP_ID', columns='STAT_NAME', values='VALUE')\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "rep_hist_sysstat_df[['SNAP_ID']] = rep_hist_sysstat_df[['SNAP_ID']].astype(int)\n",
    "#rep_hist_sysstat_df = rep_hist_sysstat_df.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "rep_hist_sysstat_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "#\n",
    "rep_hist_sysmetric_summary_df.rename(str.upper, inplace=True, axis='columns')\n",
    "rep_hist_sysstat_df.rename(str.upper, inplace=True, axis='columns')\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT)\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID','DBID','INSTANCE_NUMBER']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "#\n",
    "print('\\nHeader Lengths [After Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "#\n",
    "# DF Shape\n",
    "print('\\nDataframe shapes:\\nTable [REP_HIST_SNAPSHOT] - ' + str(rep_hist_snapshot_df.shape))\n",
    "print('Table [REP_HIST_SYSMETRIC_SUMMARY] - ' + str(rep_hist_sysmetric_summary_df.shape))\n",
    "print('Table [REP_HIST_SYSSTAT] - ' + str(rep_hist_sysstat_df.shape))\n",
    "#\n",
    "# print(rep_hist_snapshot_df.groupby(['SNAP_ID']).count())\n",
    "# print(rep_hist_sysmetric_summary_df.groupby(['SNAP_ID']).count())\n",
    "# print(rep_hist_sysstat_df.groupby(['SNAP_ID']).count())\n",
    "# print(len(rep_hist_snapshot_df['SNAP_ID'].unique()))\n",
    "# print(len(rep_hist_sysmetric_summary_df['SNAP_ID'].unique()))\n",
    "# print(len(rep_hist_sysstat_df['SNAP_ID'].unique()))\n",
    "# print(rep_hist_sysmetric_summary_df.tail())\n",
    "# print(rep_hist_sysstat_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "#\n",
    "print('N/A Columns\\n')\n",
    "print('\\n REP_HIST_SNAPSHOT Features ' + str(len(rep_hist_snapshot_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_snapshot_df,headers=rep_hist_snapshot_df.columns)) + \"\\n\")\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY Features ' + str(len(rep_hist_sysmetric_summary_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_sysmetric_summary_df,headers=rep_hist_sysmetric_summary_df.columns)) + \"\\n\")\n",
    "print('REP_HIST_SYSSTAT Features ' + str(len(rep_hist_sysstat_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_sysstat_df,headers=rep_hist_sysstat_df.columns)) + \"\\n\")\n",
    "#\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "#\n",
    "# Populating NaN values with amount '0'\n",
    "rep_hist_snapshot_df = fill_na(df=rep_hist_snapshot_df)\n",
    "rep_hist_sysmetric_summary_df = fill_na(df=rep_hist_sysmetric_summary_df)\n",
    "rep_hist_sysstat_df = fill_na(df=rep_hist_sysstat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging frames\n",
    "\n",
    "This part merges the following pandas data frame into a single frame:\n",
    "* REP_HIST_SNAPSHOT\n",
    "* REP_HIST_SYSMETRIC_SUMMARY\n",
    "* REP_HIST_SYSSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(rep_hist_snapshot_df, rep_hist_sysmetric_summary_df, on ='SNAP_ID')\n",
    "df = pd.merge(df, rep_hist_sysstat_df, on ='SNAP_ID')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['SNAP_ID'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point precision conversion\n",
    "\n",
    "Each column is converted into a column of type values which are floating point for higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.astype('float32', inplace=True)\n",
    "df = np.round(df, 3) # rounds to 3 dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "print('Before')\n",
    "print(df.shape)\n",
    "df = drop_flatline_columns(df=df)\n",
    "print('\\nAfter')\n",
    "print(df.shape)\n",
    "dropped_columns_df = [ 'PLAN_HASH_VALUE',\n",
    "                       'OPTIMIZER_ENV_HASH_VALUE',\n",
    "                       'LOADED_VERSIONS',\n",
    "                       'VERSION_COUNT',\n",
    "                       'PARSING_SCHEMA_ID',\n",
    "                       'PARSING_USER_ID']\n",
    "df.drop(columns=dropped_columns_df, inplace=True)\n",
    "print('\\nAfter')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Labels\n",
    "\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/\n",
    "\n",
    "### Histograms\n",
    "\n",
    "A simple and commonly used plot to quickly check the distribution of a sample of data is the histogram.\n",
    "\n",
    "In the histogram, the data is divided into a pre-specified number of groups called bins. The data is then sorted into each bin and the count of the number of observations in each bin is retained.\n",
    "\n",
    "The plot shows the bins across the x-axis maintaining their ordinal relationship, and the count in each bin on the y-axis.\n",
    "\n",
    "A sample of data has a Gaussian distribution of the histogram plot, showing the familiar bell shape.\n",
    "\n",
    "### Quantile-Quantile Plot\n",
    "\n",
    "Another popular plot for checking the distribution of a data sample is the quantile-quantile plot, Q-Q plot, or QQ plot for short.\n",
    "\n",
    "This plot generates its own sample of the idealized distribution that we are comparing with, in this case the Gaussian distribution. The idealized samples are divided into groups (e.g. 5), called quantiles. Each data point in the sample is paired with a similar member from the idealized distribution at the same cumulative distribution.\n",
    "\n",
    "The resulting points are plotted as a scatter plot with the idealized value on the x-axis and the data sample on the y-axis.\n",
    "\n",
    "A perfect match for the distribution will be shown by a line of dots on a 45-degree angle from the bottom left of the plot to the top right. Often a line is drawn on the plot to help make this expectation clear. Deviations by the dots from the line shows a deviation from the expected distribution.\n",
    "\n",
    "### Shapiro-Wilk Test\n",
    "\n",
    "The Shapiro-Wilk test evaluates a data sample and quantifies how likely it is that the data was drawn from a Gaussian distribution, named for Samuel Shapiro and Martin Wilk.\n",
    "\n",
    "In practice, the Shapiro-Wilk test is believed to be a reliable test of normality, although there is some suggestion that the test may be suitable for smaller samples of data, e.g. thousands of observations or fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shapiro_wilk(data):\n",
    "    # normality test\n",
    "    stat, p = sc.stats.shapiro(data)\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Sample looks Gaussian (fail to reject H0)')\n",
    "    else:\n",
    "        print('Sample does not look Gaussian (reject H0)')\n",
    "#\n",
    "for label in y_label:\n",
    "    df.plot.scatter(x='SNAP_ID',\n",
    "                    y=label,\n",
    "                    c='DarkBlue')\n",
    "    plt.show()\n",
    "    df[label].hist(bins=10)\n",
    "    plt.show()\n",
    "    qqplot(df[label], line='s')\n",
    "    plt.show()\n",
    "    shapiro_wilk(data=df[label])\n",
    "    plt.show()\n",
    "    df[label].plot.line()\n",
    "    plt.show()\n",
    "    print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection - Standard Deviation Method\n",
    "\n",
    "Detection and transformation of outliers, categorized as more than 3 standard deviations away.\n",
    "\n",
    "If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers.\n",
    "\n",
    "The Gaussian distribution has the property that the standard deviation from the mean can be used to reliably summarize the percentage of values in the sample.\n",
    "\n",
    "For example, within one standard deviation of the mean will cover 68% of the data.\n",
    "\n",
    "So, if the mean is 50 and the standard deviation is 5, as in the test dataset above, then all data in the sample between 45 and 55 will account for about 68% of the data sample. We can cover more of the data sample if we expand the range as follows:\n",
    "\n",
    "* 1 Standard Deviation from the Mean: 68%\n",
    "* 2 Standard Deviations from the Mean: 95%\n",
    "* 3 Standard Deviations from the Mean: 99.7%\n",
    "\n",
    "A value that falls outside of 3 standard deviations is part of the distribution, but it is an unlikely or rare event at approximately 1 in 370 samples.\n",
    "\n",
    "Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution. For smaller samples of data, perhaps a value of 2 standard deviations (95%) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9%) can be used.\n",
    "\n",
    "More infor here: https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_outliers_std(df=None, headers=None):\n",
    "    \"\"\"\n",
    "    Detect and return which rows are considered outliers within the dataset, determined by :quartile_limit (99%)\n",
    "    \"\"\"\n",
    "    outlier_rows = [] # This list of lists consists of elements of the following notation [column,rowid]\n",
    "    for header in headers:\n",
    "        outlier_count = 0\n",
    "        try:\n",
    "            # calculate summary statistics\n",
    "            data_mean, data_std = df[header].mean(), df[header].std()\n",
    "            # identify outliers\n",
    "            cut_off = data_std * 3\n",
    "            lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "            #\n",
    "            series_row = (df[df[header] > upper].index)\n",
    "            outlier_count += len(list(np.array(series_row)))\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "            #\n",
    "            series_row = (df[df[header] < lower].index)\n",
    "            outlier_count += len(list(np.array(series_row)))\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "            print(header + ' - [' + str(outlier_count) + '] outliers')\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    #\n",
    "    unique_outlier_rows = []\n",
    "    for col, rowid in outlier_rows:\n",
    "        unique_outlier_rows.append([col,rowid])\n",
    "    return unique_outlier_rows\n",
    "#\n",
    "#Printing outliers to screen\n",
    "outliers = get_outliers_std(df=df,\n",
    "                            headers=y_label)\n",
    "print('Total Outliers: [' + str(len(outliers)) + ']\\n')\n",
    "for label in y_label:\n",
    "    min_val = df[label].min()\n",
    "    max_val = df[label].max()\n",
    "    mean_val = df[label].mean()\n",
    "    std_val = df[label].std()\n",
    "    print('Label[' + label + '] - Min[' + str(min_val) + '] - Max[' + str(max_val) + '] - Mean[' + str(mean_val) + '] - Std[' + str(std_val) + ']')\n",
    "print('\\n---------------------------------------------\\n')\n",
    "for i in range(len(outliers)):\n",
    "    print('Header [' + str(outliers[i][0]) + '] - Location [' + str(outliers[i][1]) + '] - Value [' + str(df.iloc[outliers[i][1]][outliers[i][0]]) + ']') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection - Interquartile Range Method\n",
    "\n",
    "https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/\n",
    "\n",
    "Not all data is normal or normal enough to treat it as being drawn from a Gaussian distribution.\n",
    "\n",
    "A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short.\n",
    "\n",
    "The IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot.\n",
    "\n",
    "Remember that percentiles can be calculated by sorting the observations and selecting values at specific indices. The 50th percentile is the middle value, or the average of the two middle values for an even number of examples. If we had 10,000 samples, then the 50th percentile would be the average of the 5000th and 5001st values.\n",
    "\n",
    "We refer to the percentiles as quartiles (“quart” meaning 4) because the data is divided into four groups via the 25th, 50th and 75th values.\n",
    "\n",
    "The IQR defines the middle 50% of the data, or the body of the data.\n",
    "\n",
    "The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile. The common value for the factor k is the value 1.5. A factor k of 3 or more can be used to identify values that are extreme outliers or “far outs” when described in the context of box and whisker plots.\n",
    "\n",
    "On a box and whisker plot, these limits are drawn as fences on the whiskers (or the lines) that are drawn from the box. Values that fall outside of these values are drawn as dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers_quartile(df=None, headers=None):\n",
    "    \"\"\"\n",
    "    Detect and return which rows are considered outliers within the dataset, determined by :quartile_limit (99%)\n",
    "    \"\"\"\n",
    "    outlier_rows = [] # This list of lists consists of elements of the following notation [column,rowid]\n",
    "    for header in headers:\n",
    "        outlier_count = 0\n",
    "        try:\n",
    "            q25, q75 = np.percentile(df[header], 25), np.percentile(df[header], 75)\n",
    "            iqr = q75 - q25\n",
    "            cut_off = iqr * .6 # This values needs to remain as it. It was found to be a good value so as to capture the relavent outlier data\n",
    "            lower, upper = q25 - cut_off, q75 + cut_off\n",
    "            #\n",
    "            series_row = (df[df[header] > upper].index)\n",
    "            outlier_count += len(list(np.array(series_row)))\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "            #\n",
    "            series_row = (df[df[header] < lower].index)\n",
    "            outlier_count += len(list(np.array(series_row)))\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "            print(header + ' - [' + str(outlier_count) + '] outliers')\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    #\n",
    "    unique_outlier_rows = []\n",
    "    for col, rowid in outlier_rows:\n",
    "        unique_outlier_rows.append([col,rowid])\n",
    "    return unique_outlier_rows\n",
    "#\n",
    "#Printing outliers to screen\n",
    "outliers = get_outliers_quartile(df=df,\n",
    "                                 headers=y_label)\n",
    "print('Total Outliers: [' + str(len(outliers)) + ']\\n')\n",
    "for label in y_label:\n",
    "    min_val = df[label].min()\n",
    "    max_val = df[label].max()\n",
    "    mean_val = df[label].mean()\n",
    "    std_val = df[label].std()\n",
    "    print('Label[' + label + '] - Min[' + str(min_val) + '] - Max[' + str(max_val) + '] - Mean[' + str(mean_val) + '] - Std[' + str(std_val) + ']')\n",
    "print('\\n---------------------------------------------\\n')\n",
    "for i in range(len(outliers)):\n",
    "    print('Header [' + str(outliers[i][0]) + '] - Location [' + str(outliers[i][1]) + '] - Value [' + str(df.iloc[outliers[i][1]][outliers[i][0]]) + ']') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_outliers(df=None, headers=None):\n",
    "    \"\"\"\n",
    "    This method uses the interquartile method to edit all outliers to std.\n",
    "    \"\"\"\n",
    "    outliers = get_outliers_quartile(df=df,\n",
    "                                     headers=y_label)\n",
    "    for label in y_label:\n",
    "        min_val = df[label].min()\n",
    "        max_val = df[label].max()\n",
    "        mean_val = df[label].mean()\n",
    "        std_val = df[label].std()\n",
    "        #\n",
    "        for i in range(len(outliers)):\n",
    "            if label == outliers[i][0]:\n",
    "                df[label].iloc[outliers[i][1]] = mean_val + std_val\n",
    "                # print('Header [' + str(outliers[i][0]) + '] - Location [' + str(outliers[i][1]) + '] - Value [' + str(df.iloc[outliers[i][1]][outliers[i][0]]) + ']')\n",
    "    return df\n",
    "#\n",
    "print(\"DF with outliers: \" + str(df.shape))\n",
    "df = edit_outliers(df=df,\n",
    "                   headers=y_label)\n",
    "print(\"DF with edited outliers: \" + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_normalized_values = scaler.fit_transform(df.values)\n",
    "#\n",
    "df = pd.DataFrame(data=df_normalized_values, columns=df.columns)\n",
    "del df_normalized_values\n",
    "print(str(df.shape))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging Labels\n",
    "\n",
    "Removes the label column, and adds it at the beginning of the matrix for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_label.append('SNAP_ID')\n",
    "y_df = df[y_label]\n",
    "del y_label[-1]\n",
    "df.drop(columns=y_label, inplace=True)\n",
    "print(\"Label \" + str(y_label) + \" shape: \" + str(y_df.shape))\n",
    "print(\"Feature matrix shape: \" + str(df.shape))\n",
    "#\n",
    "# Merging labels and features in respective order\n",
    "df = pd.merge(y_df,df,on='SNAP_ID',sort=False,left_on=None, right_on=None)\n",
    "print('Merged Labels + Vectors: ' + str(df.shape))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n_in=1):\n",
    "    if n_in == 0:\n",
    "        return df\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    for header in headers:\n",
    "        if \"(t)\" in header:\n",
    "            dropped_headers.append(header)\n",
    "    #\n",
    "    for i in range(1,n_in):\n",
    "        for header in headers:\n",
    "            if \"(t-\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1)        \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, 1)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_df_column_names = shifted_df.columns[len(df.columns):len(df.columns) + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nVectors')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n_in=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket Function\n",
    "\n",
    "Takes a value and converts it into a bucket value (of 10 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_value(amount):\n",
    "    \"\"\"\n",
    "    Assumes that amount is decimal value. If so, return 1st value after decimal.\n",
    "    \"\"\"\n",
    "    amount = float(amount)\n",
    "    if amount < 0:\n",
    "        amount = 0.01\n",
    "    if amount > 1:\n",
    "        amount = 0.99\n",
    "    amount = str(amount)\n",
    "    amount = amount.split('.')\n",
    "    amount = amount[1][0]\n",
    "    return float(amount)\n",
    "#print(discretize_value(amount=0.26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Split Train / Validation / Test\n",
    "\n",
    "Split main feature/label matrix/vector into 2 subsets. \n",
    "* First subset will be used for training the model\n",
    "* The other subset is split into two further subsets:\n",
    "    - Model Validation\n",
    "    - Model Testing\n",
    "    \n",
    "The training test itself is further reshaped, so as to satisfy the format required by LSTMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "X_validate = X_validate.values\n",
    "X_test = X_test.values\n",
    "y_validate = y_validate.values\n",
    "y_test = y_test.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)))\n",
    "#\n",
    "# Plotting label over time for train, test\n",
    "# print(y_train)\n",
    "# print(y_train[:,0])\n",
    "for i in range(0, len(y_validate[0])):\n",
    "    #\n",
    "    # Whole Sample\n",
    "    plt.rcParams['figure.figsize'] = [20, 15]\n",
    "    plt.plot(y_train[:,i])\n",
    "    validate_and_test = list(y_validate[:,i]) + list(y_test[:,i])\n",
    "    plt.plot([None for j in y_train[:,i]] + [x for x in validate_and_test])\n",
    "    plt.title(y_label[i])\n",
    "    plt.show()\n",
    "    #\n",
    "    # Sub-Sample (100 range)\n",
    "    plt.rcParams['figure.figsize'] = [20, 15]\n",
    "    plt.plot([j for j in y_train[sub_sample_start:sub_sample_start+100,i]])\n",
    "    plt.title(y_label[i] + \" Close Up - From \" + str(sub_sample_start) + \" onwards sample\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "Random Forests are an improvement over bagged decision trees.\n",
    "\n",
    "A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions.\n",
    "\n",
    "Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.\n",
    "\n",
    "Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation.\n",
    "\n",
    "https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest Class (Regression + Classification)\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, mode, n_estimators, max_depth=None,parallel_degree=1):\n",
    "        self.mode = self.__validate(mode)\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.parallel_degree=parallel_degree\n",
    "        if self.mode == 'regression':\n",
    "            self.model = RandomForestRegressor(max_depth=self.max_depth, \n",
    "                                               n_estimators=self.n_estimators,\n",
    "                                               n_jobs=self.parallel_degree)\n",
    "        elif self.mode == 'classification':\n",
    "            self.model = RandomForestClassifier(max_depth=self.max_depth,\n",
    "                                                n_estimators=self.n_estimators,\n",
    "                                                n_jobs=self.parallel_degree)\n",
    "    #\n",
    "    def __validate(self, mode):\n",
    "        mode = mode.lower()\n",
    "        if mode not in ('classification','regression'):\n",
    "            raise ValueError('Specified mode is incorrect!')\n",
    "        return mode\n",
    "    #\n",
    "    def fit_model(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fits training data to target labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train,y_train)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, plot=False):\n",
    "        \"\"\"\n",
    "        Runs test data through previously trained model, and evaluate differently depending if a regression of classification model\n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        if self.mode == 'regression':\n",
    "            #\n",
    "            # RMSE Evaluation\n",
    "            rmse = math.sqrt(mean_squared_error(y, yhat))\n",
    "            print('Test RFR: %.3f\\n-----------------------------\\n\\n' % rmse)\n",
    "            #\n",
    "            # F1-Score Evaluation\n",
    "            for i in range(len(y_labels)):\n",
    "                yv_c, yhat_c = [], [] \n",
    "                for val in y[:,i]:\n",
    "                    yv_c.append(discretize_value(amount=val))\n",
    "                for val in yhat[:,i]:\n",
    "                    yhat_c.append(discretize_value(amount=val))\n",
    "                f1 = f1_score(yv_c, yhat_c, average='micro') # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "                print('Test FScore [' + y_labels[i] + ']: ' +  str(f1))\n",
    "            #\n",
    "        elif self.mode == 'classification':\n",
    "            #\n",
    "            # F1-Score Evaluation\n",
    "            for i in range(len(y_labels)):\n",
    "                f1 = f1_score(y[:,i], yhat[:,i], average='micro') # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "                print('Test FScore [' + y_labels[i] + ']: ' +  str(f1))\n",
    "        #\n",
    "        if plot:\n",
    "            for i in range(0, len(y[0])):\n",
    "                plt.rcParams['figure.figsize'] = [20, 15]\n",
    "                plt.plot(y[:,i], label='actual')\n",
    "                plt.plot(yhat[:,i], label='predicted')\n",
    "                plt.legend(['actual', 'predicted'], loc='upper left')\n",
    "                plt.title(y_labels[i])\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "# X_train = X_train.values\n",
    "# y_train = y_train.values\n",
    "# print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "# print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "# #\n",
    "# X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "# X_validate = X_validate.values\n",
    "# X_test = X_test.values\n",
    "# y_validate = y_validate.values\n",
    "# y_test = y_test.values\n",
    "# print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "# print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "# print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "# print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)))\n",
    "# #\n",
    "# model = RandomForest(mode='regression',\n",
    "#                      n_estimators=n_estimators,\n",
    "#                      parallel_degree=parallel_degree)\n",
    "# model.fit_model(X_train=X_train,\n",
    "#                 y_train=y_train)\n",
    "# model.predict_and_evaluate(X=X_validate,\n",
    "#                            y=y_validate,\n",
    "#                            y_labels=y_label,\n",
    "#                            plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model (Walk Forward Validation / Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_model(X_train=X_validate,\n",
    "#                 y_train=y_validate)\n",
    "# model.predict_and_evaluate(X=X_test,\n",
    "#                            y=y_test,\n",
    "#                            y_labels=y_label,\n",
    "#                            plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Regression\n",
    "### Designing the network\n",
    "\n",
    "- The first step is to define your network.\n",
    "- Neural networks are defined in Keras as a sequence of layers. The container for these layers is the **Sequential class**.\n",
    "- The first step is to create an instance of the Sequential class. Then you can create your layers and add them in the order that they should be connected.\n",
    "- The LSTM recurrent layer comprised of memory units is called LSTM().\n",
    "- A fully connected layer that often follows LSTM layers and is used for outputting a prediction is called Dense().\n",
    "- The first layer in the network must define the number of inputs to expect.\n",
    "- Input must be three-dimensional, comprised of samples, timesteps, and features.\n",
    "    - **Samples:** These are the rows in your data.\n",
    "    - **Timesteps:** These are the past observations for a feature, such as lag variables.\n",
    "    - **Features:** These are columns in your data.\n",
    "- Assuming your data is loaded as a NumPy array, you can convert a 2D dataset to a 3D dataset using the reshape() function in NumPy.\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "* https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/\n",
    "* https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, X_train, y_train, layers, mode='regression', optimizer='sgd'):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X_train - Training data used to train the model\n",
    "        :param y_train - Test data used to test the model\n",
    "        :param layers - A list of values, where in each value denotes a layer, and the number of neurons for that layer\n",
    "        :param loss_function - Function used to measure fitting of model (predicted from actual)\n",
    "        :param optimizer - Function used to optimize the model (eg: Gradient Descent)\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.model = ke.models.Sequential()\n",
    "        #\n",
    "        if len(layers) < 1:\n",
    "            raise ValueError('Layer Count is Empty!')\n",
    "        elif len(layers) == 1:\n",
    "            self.model.add(ke.layers.LSTM(layers[0], input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        else:\n",
    "            for i in range(0,len(layers)-2):\n",
    "                self.model.add(ke.layers.LSTM(layers[i], input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "            self.model.add(ke.layers.LSTM(layers[i]))\n",
    "        #\n",
    "        # self.model.add(ke.layers.Dense(y_train.shape[1]))\n",
    "        self.model.add(ke.layers.Dense(layers[-1]))\n",
    "        self.model.add(ke.layers.Activation('sigmoid'))\n",
    "        #\n",
    "        if self.mode == 'regression':\n",
    "            self.loss_func = 'mae'\n",
    "        elif self.mode == 'classification':\n",
    "            self.loss_func = 'categorical_crossentropy'\n",
    "        else:\n",
    "            self.loss_func = None\n",
    "        #\n",
    "        if self.mode == 'regression':\n",
    "            self.model.compile(loss=self.loss_func, optimizer=optimizer)\n",
    "        elif self.mode == 'classification':\n",
    "            self.model.compile(loss=self.loss_func, optimizer=optimizer, metrics=['accuracy'])\n",
    "        #\n",
    "        # Map Discretize function to matrices\n",
    "        self.vecfunc = np.vectorize(discretize_value)\n",
    "        print(self.model.summary())\n",
    "    #\n",
    "    def fit_model(self, X_train, X_test, y_train, y_test, epochs=50, batch_size=50, verbose=2, shuffle=False, plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "        \"\"\"\n",
    "        history = self.model.fit(x=X_train, \n",
    "                                 y=y_train, \n",
    "                                 epochs=epochs, \n",
    "                                 batch_size=batch_size, \n",
    "                                 validation_data=(X_test,y_test), \n",
    "                                 verbose=verbose, \n",
    "                                 shuffle=shuffle)\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            if self.mode == 'regression':\n",
    "                plt.plot(history.history['loss'], label='train')\n",
    "                plt.plot(history.history['val_loss'], label='validation')\n",
    "            elif self.mode == 'classification':\n",
    "                plt.plot(history.history['acc'], label='train')\n",
    "                plt.plot(history.history['val_acc'], label='validation')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, plot=False):\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # RMSE Evaluation\n",
    "        if self.mode == 'regression':\n",
    "            rmse = math.sqrt(mean_squared_error(y, yhat))\n",
    "            print('Test ' + self.loss_func + ': %.3f\\n-----------------------------\\n\\n' % rmse)\n",
    "            #\n",
    "            # F1-Score Evaluation\n",
    "            for i in range(len(y_labels)):\n",
    "                yv_c, yhat_c = [], [] \n",
    "                for val in y[:,i]:\n",
    "                    yv_c.append(discretize_value(amount=val))\n",
    "                for val in yhat[:,i]:\n",
    "                    yhat_c.append(discretize_value(amount=val))\n",
    "                f1 = f1_score(yv_c, yhat_c, average='micro') # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "                print('Test FScore [' + y_labels[i] + ']: ' +  str(f1))\n",
    "        elif self.mode == 'classification':\n",
    "            yhat=self.vecfunc(yhat)\n",
    "            #\n",
    "            # F1-Score Evaluation\n",
    "            for i in range(len(y_labels)):\n",
    "                f1 = f1_score(y[:,i], yhat[:,i], average='micro') # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "                print('Test FScore [' + y_labels[i] + ']: ' +  str(f1))\n",
    "        #\n",
    "        if plot:\n",
    "            for i in range(0, len(y[0])):\n",
    "                plt.rcParams['figure.figsize'] = [20, 15]\n",
    "                plt.plot(y[:,i], label='actual')\n",
    "                plt.plot(yhat[:,i], label='predicted')\n",
    "                plt.legend(['actual', 'predicted'], loc='upper left')\n",
    "                plt.title(y_labels[i])\n",
    "                plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of predictive modeling problem imposes constraints on the type of loss function that can be used.\n",
    "\n",
    "Below is a list of the metrics that you can use in Keras on regression problems.\n",
    "* Mean Squared Error: mean_squared_error, MSE or mse\n",
    "* Mean Absolute Error: mean_absolute_error, MAE, mae\n",
    "* Mean Absolute Percentage Error: mean_absolute_percentage_error, MAPE, mape\n",
    "* Cosine Proximity: cosine_proximity, cosine\n",
    "\n",
    "The most common optimization algorithm is stochastic gradient descent, but Keras also supports a suite of other state-of-the-art optimization algorithms that work well with little or no configuration.\n",
    "\n",
    "Perhaps the most commonly used optimization algorithms because of their generally better performance are:\n",
    "\n",
    "* Stochastic Gradient Descent, or ‘sgd‘, that requires the tuning of a learning rate and momentum.\n",
    "* ADAM, or ‘adam‘, that requires the tuning of learning rate.\n",
    "* RMSprop, or ‘rmsprop‘, that requires the tuning of learning rate.\n",
    "\n",
    "More here: https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "X_validate = X_validate.values\n",
    "X_test = X_test.values\n",
    "y_validate = y_validate.values\n",
    "y_test = y_test.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)))\n",
    "#\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_validate = X_validate.reshape((X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "#    \n",
    "model = LSTM(X_train=X_train,\n",
    "             y_train=y_train,\n",
    "             layers=[X_train.shape[2],\n",
    "                     int(X_train.shape[2]/1.5),\n",
    "                     len(y_label)],\n",
    "             mode='regression')\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size,\n",
    "                verbose=2, \n",
    "                shuffle=False,\n",
    "                plot=True)\n",
    "model.predict_and_evaluate(X=X_validate,\n",
    "                           y=y_validate,\n",
    "                           y_labels=y_label,\n",
    "                           plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model (Walk Forward Validation / Testing)\n",
    "\n",
    "* https://machinelearningmastery.com/update-lstm-networks-training-time-series-forecasting/\n",
    "* https://machinelearningmastery.com/instability-online-learning-stateful-lstm-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_model(X_train=X_validate,\n",
    "                X_test=X_test,\n",
    "                y_train=y_validate,\n",
    "                y_test=y_test,\n",
    "                epochs=epochs, \n",
    "                batch_size=1, # Incremental batch size fitting\n",
    "                verbose=2, \n",
    "                shuffle=False,\n",
    "                plot=True)\n",
    "model.predict_and_evaluate(X=X_test,\n",
    "                           y=y_test,\n",
    "                           y_labels=y_label,\n",
    "                           plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Training\n",
    "\n",
    "This section converts the established features from the continuous domain into the discrete domain. Continous values will be converted into discrete, and used to train the model using such values (Utilizes bucket function).\n",
    "\n",
    "* 0  - 10\n",
    "* 11 - 20\n",
    "* 21 - 30\n",
    "* 31 - 40\n",
    "* 41 - 50\n",
    "* 51 - 60\n",
    "* 61 - 70\n",
    "* 71 - 80\n",
    "* 81 - 90\n",
    "* 91 - 100\n",
    "\n",
    "Classify Features X into buckets of 10 (1-100% : intervals of 10%), in the attempts to convert this regression problem into one of classification.\n",
    "\n",
    "https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/\n",
    "\n",
    "### RandomForest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "# X_train = X_train.values\n",
    "# y_train = y_train.values\n",
    "# print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "# print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "# #\n",
    "# X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "# X_validate = X_validate.values\n",
    "# X_test = X_test.values\n",
    "# y_validate = y_validate.values\n",
    "# y_test = y_test.values\n",
    "# print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "# print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "# print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "# print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)) + \"\\n------------------------------\")\n",
    "# #\n",
    "# # Map Discretize function to matrices\n",
    "# vecfunc = np.vectorize(discretize_value)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "# X_train=vecfunc(X_train)\n",
    "# y_train=vecfunc(y_train)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "# print('------------------------------------------------------------')\n",
    "# print(X_validate)\n",
    "# print(y_validate)\n",
    "# X_validate=vecfunc(X_validate)\n",
    "# y_validate=vecfunc(y_validate)\n",
    "# print(X_validate)\n",
    "# print(y_validate)\n",
    "# print('------------------------------------------------------------')\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "# X_test=vecfunc(X_test)\n",
    "# y_test=vecfunc(y_test)\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "# #\n",
    "# # Train on discrete data (Train > Validation)\n",
    "# model = RandomForest(mode='classification',\n",
    "#                      n_estimators=n_estimators,\n",
    "#                      parallel_degree=parallel_degree)\n",
    "# model.fit_model(X_train=X_train,\n",
    "#                 y_train=y_train)\n",
    "# model.predict_and_evaluate(X=X_validate,\n",
    "#                            y=y_validate,\n",
    "#                            y_labels=y_label,\n",
    "#                            plot=True)\n",
    "# #\n",
    "# # Train on discrete data (Train + Validation > Test)\n",
    "# model.fit_model(X_train=X_validate,\n",
    "#                 y_train=y_validate)\n",
    "# model.predict_and_evaluate(X=X_test,\n",
    "#                            y=y_test,\n",
    "#                            y_labels=y_label,\n",
    "#                            plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Classification\n",
    "\n",
    "Below is a list of the metrics that you can use in Keras on classification problems.\n",
    "\n",
    "* Binary Accuracy: binary_accuracy, acc\n",
    "* Categorical Accuracy: categorical_accuracy, acc\n",
    "* Sparse Categorical Accuracy: sparse_categorical_accuracy\n",
    "* Top k Categorical Accuracy: top_k_categorical_accuracy (requires you specify a k parameter)\n",
    "* Sparse Top k Categorical Accuracy: sparse_top_k_categorical_accuracy (requires you specify a k parameter)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n",
    "* https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "X_validate = X_validate.values\n",
    "X_test = X_test.values\n",
    "y_validate = y_validate.values\n",
    "y_test = y_test.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)) + \"\\n------------------------------\")\n",
    "#\n",
    "# Map Discretize function to matrices\n",
    "vecfunc = np.vectorize(discretize_value)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "X_train=vecfunc(X_train)\n",
    "y_train=vecfunc(y_train)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate)\n",
    "print(y_validate)\n",
    "X_validate=vecfunc(X_validate)\n",
    "y_validate=vecfunc(y_validate)\n",
    "print(X_validate)\n",
    "print(y_validate)\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "X_test=vecfunc(X_test)\n",
    "y_test=vecfunc(y_test)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "#\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_validate = X_validate.reshape((X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "discrete_model = LSTM(X_train=X_train,\n",
    "                      y_train=y_train,\n",
    "                      layers=[X_train.shape[2],\n",
    "                              int(X_train.shape[2]/1.5),\n",
    "                              len(y_label)],\n",
    "                      mode='classification')\n",
    "discrete_model.fit_model(X_train=X_train,\n",
    "                         X_test=X_validate,\n",
    "                         y_train=y_train,\n",
    "                         y_test=y_validate,\n",
    "                         epochs=epochs, \n",
    "                         batch_size=batch_size,\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_validate,\n",
    "                                    y=y_validate,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "discrete_model.fit_model(X_train=X_validate,\n",
    "                         X_test=X_test,\n",
    "                         y_train=y_validate,\n",
    "                         y_test=y_test,\n",
    "                         epochs=epochs, \n",
    "                         batch_size=1, # Incremental batch size fitting\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_test,\n",
    "                                    y=y_test,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
