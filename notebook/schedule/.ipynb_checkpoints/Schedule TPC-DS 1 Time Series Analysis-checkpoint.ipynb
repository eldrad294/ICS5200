{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "This section treats the envisaged dataset as a time series problem. Proposed techniques / methodology here is to:\n",
    "* Covert the featured label into a discrete column value type, instead of continuous.\n",
    "* Time Shift the provided datasets with varied 'lag' values.\n",
    "* Combine all 3 matrices into a agglomorated matrix of 61 + 162 + 1179 features (1402). Duplicate columns ('SNAP_ID') will be reduced to a single one.\n",
    "* Slicing agglomorated matrix into Features/Labels.\n",
    "* Splitting of train/validation/test set.\n",
    "* Perform feature selection on the agglomorated matrix, dropping redundant features and finding the  optimum number of features. (Multivariate analysis - through a wrapper approach)\n",
    "* Feed dataset into a number of machine learning models\n",
    "\n",
    "Applicable links:\n",
    "* https://machinelearningmastery.com/how-to-scale-data-for-long-short-term-memory-networks-in-python/\n",
    "* https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "* https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "* https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header Lengths [Before Pivot]\n",
      "REP_HIST_SNAPSHOT: 88\n",
      "REP_HIST_SYSMETRIC_SUMMARY: 26\n",
      "REP_HIST_SYSSTAT: 16\n",
      "\n",
      "Header Lengths [After Pivot]\n",
      "REP_HIST_SNAPSHOT: 77\n",
      "REP_HIST_SYSMETRIC_SUMMARY: 162\n",
      "REP_HIST_SYSSTAT: 1179\n",
      "\n",
      "Dataframe shapes:\n",
      "Table [REP_HIST_SNAPSHOT] - (172, 77)\n",
      "Table [REP_HIST_SYSMETRIC_SUMMARY] - (172, 162)\n",
      "Table [REP_HIST_SYSSTAT] - (172, 1179)\n"
     ]
    }
   ],
   "source": [
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "debug_mode=True # Determines whether to plot graphs or not, useful for development purposes \n",
    "low_quartile_limit = 0 # Lower Quartile threshold to detect outliers\n",
    "upper_quartile_limit = 1 # Upper Quartile threshold to detect outliers\n",
    "lag=0 # Time Series shift / Lag Step. Each lag value equates to 1 minute\n",
    "test_split=.3 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "y_label = 'CPU_TIME_DELTA' # Denotes which label to use for time series experiments\n",
    "#\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_snapshot.csv'\n",
    "rep_hist_sysmetric_summary_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_sysmetric_summary.csv'\n",
    "rep_hist_sysstat_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_sysstat.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "rep_hist_sysmetric_summary_df = pd.read_csv(rep_hist_sysmetric_summary_path)\n",
    "rep_hist_sysstat_df = pd.read_csv(rep_hist_sysstat_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "rep_hist_sysmetric_summary_df.columns = prettify_header(rep_hist_sysmetric_summary_df.columns.values)\n",
    "rep_hist_sysstat_df.columns = prettify_header(rep_hist_sysstat_df.columns.values)\n",
    "#\n",
    "print('Header Lengths [Before Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "#\n",
    "# Table REP_HIST_SYSMETRIC_SUMMARY\n",
    "rep_hist_sysmetric_summary_df = rep_hist_sysmetric_summary_df.pivot(index='SNAP_ID', columns='METRIC_NAME', values='AVERAGE')\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "rep_hist_sysmetric_summary_df[['SNAP_ID']] = rep_hist_sysmetric_summary_df[['SNAP_ID']].astype(int)\n",
    "rep_hist_sysmetric_summary_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "#\n",
    "# Table REP_HIST_SYSSTAT\n",
    "rep_hist_sysstat_df = rep_hist_sysstat_df.pivot(index='SNAP_ID', columns='STAT_NAME', values='VALUE')\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "rep_hist_sysstat_df[['SNAP_ID']] = rep_hist_sysstat_df[['SNAP_ID']].astype(int)\n",
    "rep_hist_sysstat_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "#\n",
    "# Refreshing columns with pivoted columns\n",
    "def convert_list_to_upper(col_list):\n",
    "    \"\"\"\n",
    "    Takes a string and converts elements to upper\n",
    "    \"\"\"\n",
    "    upper_col_list = []\n",
    "    for col in col_list:\n",
    "        upper_col_list.append(col.upper())\n",
    "    return upper_col_list\n",
    "#\n",
    "rep_hist_sysmetric_summary_df.rename(str.upper, inplace=True, axis='columns')\n",
    "rep_hist_sysstat_df.rename(str.upper, inplace=True, axis='columns')\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT)\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "#\n",
    "print('\\nHeader Lengths [After Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "#\n",
    "# DF Shape\n",
    "print('\\nDataframe shapes:\\nTable [REP_HIST_SNAPSHOT] - ' + str(rep_hist_snapshot_df.shape))\n",
    "print('Table [REP_HIST_SYSMETRIC_SUMMARY] - ' + str(rep_hist_sysmetric_summary_df.shape))\n",
    "print('Table [REP_HIST_SYSSTAT] - ' + str(rep_hist_sysstat_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Frames\n",
    "\n",
    "This part merges the following pandas data frame into a single frame:\n",
    "* REP_HIST_SNAPSHOT\n",
    "* REP_HIST_SYSMETRIC_SUMMARY\n",
    "* REP_HIST_SYSSTAT\n",
    "\n",
    "In addition, this step isolates the label column from the remainder of the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 1416)\n",
      "Label [CPU_TIME_DELTA] shape: (172, 1)\n",
      "Feature matrix shape: (172, 1415)\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(rep_hist_snapshot_df, rep_hist_sysmetric_summary_df, on='SNAP_ID')\n",
    "df = pd.merge(df, rep_hist_sysstat_df, on='SNAP_ID')\n",
    "print(df.shape)\n",
    "#\n",
    "y_df = df[[y_label]]\n",
    "X_df = df.drop(columns=[y_label])\n",
    "print(\"Label [\" + y_label + \"] shape: \" + str(y_df.shape))\n",
    "print(\"Feature matrix shape: \" + str(X_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Before: (172, 1415)\n",
      "After: (172, 1415)\n",
      "Index(['var1(t)', 'var2(t)', 'var3(t)', 'var4(t)', 'var5(t)'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=False):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "shape = X_df.shape\n",
    "X_df = series_to_supervised(data=X_df,n_in=lag)\n",
    "print(type(X_df))\n",
    "print('Before: ' + str(shape) + '\\nAfter: ' + str(X_df.shape))\n",
    "print(X_df.columns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous to Discrete Conversion\n",
    "\n",
    "This section converts the established 'y_label' continuous column into a discrete version. Values are binned into 10 categories (a percentage measure): \n",
    "* 0  - 10\n",
    "* 11 - 20\n",
    "* 21 - 30\n",
    "* 31 - 40\n",
    "* 41 - 50\n",
    "* 51 - 60\n",
    "* 61 - 70\n",
    "* 71 - 80\n",
    "* 81 - 90\n",
    "* 91 - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape before discretization: (172, 1)\n",
      "CPU_TIME_DELTA min: 0\n",
      "CPU_TIME_DELTA max: CPU_TIME_DELTA    1266827638\n",
      "dtype: int64\n",
      "Label shape after discretization: (172, 1)\n",
      "[0 1 8 3 5 2 7 6 9 4]\n"
     ]
    }
   ],
   "source": [
    "def discretize_label(df=None, bin_total=10):\n",
    "    \"\"\"\n",
    "    Converts pandas column into a range of bins (converts data from contiguous to discrete)\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError('Dataframe was not specified!')\n",
    "    if bin_total < 1:\n",
    "        raise ValueError('Bin Amounts must be at least 1!')\n",
    "    #\n",
    "    label_max, label_min = y_df.max(), 0\n",
    "    interval = float((label_max - label_min) / bin_total)\n",
    "    print(y_label + ' min: ' + str(label_min))\n",
    "    print(y_label + ' max: ' + str(label_max))\n",
    "    discrete_bins = []\n",
    "    for val in y_df.values[:,0]:\n",
    "        val = float(val)\n",
    "        for i in range(bin_total):\n",
    "            if (val > (interval * i)) and (val <= (interval * (i+1))):\n",
    "                discrete_bins.append(i)\n",
    "                break\n",
    "    return pd.DataFrame(data=discrete_bins,columns=df.columns)\n",
    "#\n",
    "print(\"Label shape before discretization: \" + str(y_df.shape))\n",
    "y_df = discretize_label(df=y_df, bin_total=10)\n",
    "print(\"Label shape after discretization: \" + str(y_df.shape))\n",
    "print(y_df[y_label].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Split Train / Validation / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape [(120, 1415)]\n",
      "X_validate shape [(52, 1415)]\n",
      "y_train shape [(120, 1)]\n",
      "y_validate shape [(52, 1)]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"]\")\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"]\")\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"]\")\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
