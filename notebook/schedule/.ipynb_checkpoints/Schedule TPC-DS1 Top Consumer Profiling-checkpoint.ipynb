{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule Top Consumer Profiling\n",
    "\n",
    "This notebook deals with outlier detection, particularly from the SQL domain. This work attributes itself to detection of high consumers within a database system, particularly flagging those SQL which stand out in terms of computation time/resources required to execute.\n",
    "\n",
    "Due to the high dimensionality of the available data points, unsupervised machine learning techniques will be applied to this problem, so as to isolate data anamolies and flag them as potential bottlenecks.\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "y_label = ['CPU_TIME_DELTA','OPTIMIZER_COST','EXECUTIONS_DELTA','ELAPSED_TIME_DELTA']\n",
    "y_label2 = ['COST','CARDINALITY','BYTES','IO_COST','TEMP_SPACE','TIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/TPCDS1/rep_hist_snapshot.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a874c888f718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrep_vsql_plan_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtpcds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/rep_vsql_plan.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrep_hist_snapshot_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_snapshot_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mrep_vsql_plan_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_vsql_plan_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/TPCDS1/rep_hist_snapshot.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "# rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_hist_snapshot.csv'\n",
    "# rep_vsql_plan_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_vsql_plan.csv'\n",
    "rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/rep_hist_snapshot.csv'\n",
    "rep_vsql_plan_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/rep_vsql_plan.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "rep_vsql_plan_df = pd.read_csv(rep_vsql_plan_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "rep_vsql_plan_df.columns = prettify_header(rep_vsql_plan_df.columns.values)\n",
    "print(rep_hist_snapshot_df.columns)\n",
    "print('------------------------------------------')\n",
    "print(rep_vsql_plan_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting tables and changing matrix shapes\n",
    "\n",
    "Changes all dataframe shapes to be similar to each other, where in a number of snap_id timestamps are cojoined with instance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Header Lengths [Before Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_VSQL_PLAN: ' + str(len(rep_vsql_plan_df.columns)))\n",
    "#\n",
    "# Group By Values by SNAP_ID, PLAN_HASH_VALUE , sum all metrics (for table REP_HIST_SNAPSHOT)\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID','PLAN_HASH_VALUE','DBID','INSTANCE_NUMBER']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "#\n",
    "# Group By Values by PLAN_HASH_VALUE,TIMESTAMP, sum all metrics (for table REP_VSQL_PLAN)\n",
    "rep_vsql_plan_df = rep_vsql_plan_df.groupby(['TIMESTAMP','SQL_ID','ID','DBID','CON_DBID']).sum()\n",
    "rep_vsql_plan_df.reset_index(inplace=True)\n",
    "#\n",
    "print('\\nHeader Lengths [After Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_VSQL_PLAN: ' + str(len(rep_vsql_plan_df.columns)) + \"\\n\")\n",
    "print(rep_hist_snapshot_df.columns)\n",
    "print(rep_vsql_plan_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "#\n",
    "print('N/A Columns\\n')\n",
    "print('\\n REP_HIST_SNAPSHOT Features ' + str(len(rep_hist_snapshot_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_snapshot_df,headers=rep_hist_snapshot_df.columns)) + \"\\n\")\n",
    "print('REP_VSQL_PLAN Features ' + str(len(rep_vsql_plan_df.columns)) + ': ' + str(get_na_columns(df=rep_vsql_plan_df,headers=rep_vsql_plan_df.columns)) + \"\\n\")\n",
    "#\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "#\n",
    "# Populating NaN values with amount '0'\n",
    "rep_hist_snapshot_df = fill_na(df=rep_hist_snapshot_df)\n",
    "rep_vsql_plan_df = fill_na(df=rep_vsql_plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of:\n",
    "\n",
    "* REP_HIST_SNAPSHOT - SNAP_ID\n",
    "* REP_VSQL_PLAN - TIMESTAMP, SQL_ID, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_hist_snapshot_df.sort_values(by=['SNAP_ID'], ascending=True, inplace=True)\n",
    "rep_vsql_plan_df.sort_values(by=['TIMESTAMP','SQL_ID','ID'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode(df, features):\n",
    "#     encoder_dict={} # Used to keep track of respective encoders, in case it is required to decoded labels further down the line\n",
    "#     for f in features:\n",
    "#         for col in df.columns:\n",
    "#             col = str(col)\n",
    "#             if col.lower() == f.lower()\n",
    "#                 le = preprocessing.LabelEncoder()\n",
    "#                 df[col].values = le.fit_transform(df[col].values)\n",
    "#                 encoder_dict[col] = le\n",
    "#     return df, le\n",
    "# #\n",
    "# encoded_labels_hist_snapshot = []\n",
    "# encoded_labels_vsql_plan = ['OPERATION',\n",
    "#                             'OPTIONS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point precision conversion\n",
    "\n",
    "Each column is converted into a column of type values which are floating point for higher precision, and rounded to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in rep_hist_snapshot_df.columns:\n",
    "    try:\n",
    "        rep_hist_snapshot_df[col].astype('float32',inplace=True)\n",
    "    except:\n",
    "        rep_hist_snapshot_df.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')\n",
    "#\n",
    "print('-------------------------------------------------------------')\n",
    "#\n",
    "for col in rep_vsql_plan_df.columns:\n",
    "    try:\n",
    "        rep_vsql_plan_df[col].astype('float32',inplace=True)\n",
    "    except:\n",
    "        rep_vsql_plan_df.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')\n",
    "#\n",
    "rep_hist_snapshot_df = np.round(rep_hist_snapshot_df, 3) # rounds to 3 dp\n",
    "rep_vsql_plan_df = np.round(rep_vsql_plan_df, 3) # rounds to 3 dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before')\n",
    "print(rep_hist_snapshot_df.shape)\n",
    "print(rep_vsql_plan_df.shape)\n",
    "#\n",
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "rep_hist_snapshot_df = drop_flatline_columns(df=rep_hist_snapshot_df)\n",
    "rep_vsql_plan_df = drop_flatline_columns(df=rep_vsql_plan_df)\n",
    "#\n",
    "dropped_columns_rep_hist_snapshot = ['SNAP_ID',\n",
    "                                       'PLAN_HASH_VALUE',\n",
    "                                       'OPTIMIZER_ENV_HASH_VALUE',\n",
    "                                       'LOADED_VERSIONS',\n",
    "                                       'VERSION_COUNT',\n",
    "                                       'PARSING_SCHEMA_ID',\n",
    "                                       'PARSING_USER_ID']\n",
    "dropped_columns_rep_vsql_plan = ['PLAN_HASH_VALUE',\n",
    "                                 'ID',\n",
    "                                 'OBJECT#',\n",
    "                                 'PARENT_ID',\n",
    "                                 'SEARCH_COLUMNS']\n",
    "rep_hist_snapshot_df.drop(columns=dropped_columns_rep_hist_snapshot, inplace=True)\n",
    "rep_vsql_plan_df.drop(columns=dropped_columns_rep_vsql_plan, inplace=True)\n",
    "#\n",
    "print('After')\n",
    "print(rep_hist_snapshot_df.shape)\n",
    "print(rep_vsql_plan_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guaging Outliers (REP_HIST_SNAPSHOT)\n",
    "\n",
    "Uses the following labels and plots them, so as to showcase the presence of outliers:\n",
    "* CPU_TIME_DELTA\n",
    "* OPTIMIZER_COST\n",
    "* EXECUTIONS_DELTA\n",
    "* ELAPSED_TIME_DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_label = ['CPU_TIME_DELTA','OPTIMIZER_COST','EXECUTIONS_DELTA','ELAPSED_TIME_DELTA']\n",
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "rep_hist_snapshot_df.plot.scatter(x='CPU_TIME_DELTA',\n",
    "                                  y='OPTIMIZER_COST',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='CPU_TIME_DELTA',\n",
    "                                  y='EXECUTIONS_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='CPU_TIME_DELTA',\n",
    "                                  y='ELAPSED_TIME_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "plt.show()\n",
    "print('--------------------------------------------------------')\n",
    "rep_hist_snapshot_df.plot.scatter(x='OPTIMIZER_COST',\n",
    "                                  y='CPU_TIME_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='OPTIMIZER_COST',\n",
    "                                  y='EXECUTIONS_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='OPTIMIZER_COST',\n",
    "                                  y='ELAPSED_TIME_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "plt.show()\n",
    "print('--------------------------------------------------------')\n",
    "rep_hist_snapshot_df.plot.scatter(x='EXECUTIONS_DELTA',\n",
    "                                  y='CPU_TIME_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='EXECUTIONS_DELTA',\n",
    "                                  y='OPTIMIZER_COST',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='EXECUTIONS_DELTA',\n",
    "                                  y='ELAPSED_TIME_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "plt.show()\n",
    "print('--------------------------------------------------------')\n",
    "rep_hist_snapshot_df.plot.scatter(x='ELAPSED_TIME_DELTA',\n",
    "                                  y='CPU_TIME_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='ELAPSED_TIME_DELTA',\n",
    "                                  y='OPTIMIZER_COST',\n",
    "                                  c='DarkBlue')\n",
    "rep_hist_snapshot_df.plot.scatter(x='ELAPSED_TIME_DELTA',\n",
    "                                  y='EXECUTIONS_DELTA',\n",
    "                                  c='DarkBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.boxplot(rep_hist_snapshot_df['CPU_TIME_DELTA'].values)\n",
    "plt.title('CPU_TIME_DELTA')\n",
    "plt.show()\n",
    "plt.boxplot(rep_hist_snapshot_df['OPTIMIZER_COST'].values)\n",
    "plt.title('OPTIMIZER_COST')\n",
    "plt.show()\n",
    "plt.boxplot(rep_hist_snapshot_df['EXECUTIONS_DELTA'].values)\n",
    "plt.title('EXECUTIONS_DELTA')\n",
    "plt.show()\n",
    "plt.boxplot(rep_hist_snapshot_df['ELAPSED_TIME_DELTA'].values)\n",
    "plt.title('ELAPSED_TIME_DELTA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "label = 'CPU_TIME_DELTA'\n",
    "rep_hist_snapshot_df2 = rep_hist_snapshot_df.sort_values(by=label, ascending=False)\n",
    "rep_hist_snapshot_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'OPTIMIZER_COST'\n",
    "rep_hist_snapshot_df2 = rep_hist_snapshot_df.sort_values(by=label, ascending=False)\n",
    "rep_hist_snapshot_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'EXECUTIONS_DELTA'\n",
    "rep_hist_snapshot_df2 = rep_hist_snapshot_df.sort_values(by=label, ascending=False,)\n",
    "rep_hist_snapshot_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'ELAPSED_TIME_DELTA'\n",
    "rep_hist_snapshot_df2 = rep_hist_snapshot_df.sort_values(by=label, ascending=False)\n",
    "rep_hist_snapshot_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guaging Outliers (REP_VSQL_PLAN)\n",
    "\n",
    "Uses the following labels and plots them, so as to showcase the presence of outliers:\n",
    "* COST\n",
    "* CARDINALITY\n",
    "* BYTES\n",
    "* CPU_COST\n",
    "* IO_COST\n",
    "* TEMP_SPACE\n",
    "* TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "label = 'COST'\n",
    "rep_vsql_plan_df2 = rep_vsql_plan_df.sort_values(by=label, ascending=False)\n",
    "rep_vsql_plan_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'CARDINALITY'\n",
    "rep_vsql_plan_df2 = rep_vsql_plan_df.sort_values(by=label, ascending=False)\n",
    "rep_vsql_plan_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'BYTES'\n",
    "rep_vsql_plan_df2 = rep_vsql_plan_df.sort_values(by=label, ascending=False)\n",
    "rep_vsql_plan_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'IO_COST'\n",
    "rep_vsql_plan_df2 = rep_vsql_plan_df.sort_values(by=label, ascending=False)\n",
    "rep_vsql_plan_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'TEMP_SPACE'\n",
    "rep_vsql_plan_df2 = rep_vsql_plan_df.sort_values(by=label, ascending=False)\n",
    "rep_vsql_plan_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "label = 'TIME'\n",
    "rep_vsql_plan_df2 = rep_vsql_plan_df.sort_values(by=label, ascending=False)\n",
    "rep_vsql_plan_df2[[label]][0:limit].plot(kind='bar', title =label, figsize=(20, 15), legend=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_label2 = ['COST','CARDINALITY','BYTES','IO_COST','TEMP_SPACE','TIME']\n",
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "rep_vsql_plan_df.plot.scatter(x='COST',\n",
    "                              y='CARDINALITY',\n",
    "                              c='DarkBlue')\n",
    "rep_vsql_plan_df.plot.scatter(x='COST',\n",
    "                              y='BYTES',\n",
    "                              c='DarkBlue')\n",
    "rep_vsql_plan_df.plot.scatter(x='COST',\n",
    "                              y='IO_COST',\n",
    "                              c='DarkBlue')\n",
    "rep_vsql_plan_df.plot.scatter(x='COST',\n",
    "                              y='TEMP_SPACE',\n",
    "                              c='DarkBlue')\n",
    "rep_vsql_plan_df.plot.scatter(x='COST',\n",
    "                              y='TIME',\n",
    "                              c='DarkBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.boxplot(rep_vsql_plan_df['COST'].values)\n",
    "plt.title('COST')\n",
    "plt.show()\n",
    "plt.boxplot(rep_vsql_plan_df['CARDINALITY'].values)\n",
    "plt.title('CARDINALITY')\n",
    "plt.show()\n",
    "plt.boxplot(rep_vsql_plan_df['BYTES'].values)\n",
    "plt.title('BYTES')\n",
    "plt.show()\n",
    "plt.boxplot(rep_vsql_plan_df['IO_COST'].values)\n",
    "plt.title('IO_COST')\n",
    "plt.show()\n",
    "plt.boxplot(rep_vsql_plan_df['TEMP_SPACE'].values)\n",
    "plt.title('TEMP_SPACE')\n",
    "plt.show()\n",
    "plt.boxplot(rep_vsql_plan_df['TIME'].values)\n",
    "plt.title('TIME')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering (K=2)\n",
    "\n",
    "Attempts at clustering data (Separation between inliers and outliers). Initial attempts will target K=2, and then visualize centroid to gauge their effectiveness in the achieved clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_col_pos(df, target_label):\n",
    "    \"\"\"\n",
    "    Iterates over column, and retrieves position of col in dataset\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    index = -1\n",
    "    for i in range(0,len(columns)):\n",
    "        if columns[i].lower() == target_label.lower():\n",
    "            index = i\n",
    "            break\n",
    "    return index\n",
    "#\n",
    "K = 2\n",
    "kmeans_hist = KMeans(n_clusters=K, random_state=0).fit(rep_hist_snapshot_df.values)\n",
    "print(kmeans_hist)\n",
    "print(kmeans_hist.labels_)\n",
    "unique, counts = np.unique(kmeans_hist.labels_, return_counts=True)\n",
    "print('Unique: ' + str(unique))\n",
    "print('Counts: ' + str(counts))\n",
    "print(kmeans_hist.cluster_centers_)\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "##################################\n",
    "plt.scatter(x=rep_hist_snapshot_df.values[:,get_col_pos(rep_hist_snapshot_df, 'ELAPSED_TIME_DELTA')],\n",
    "            y=rep_hist_snapshot_df.values[:,get_col_pos(rep_hist_snapshot_df, 'CPU_TIME_DELTA')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_hist_snapshot_df, 'ELAPSED_TIME_DELTA')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_hist_snapshot_df, 'CPU_TIME_DELTA')],\n",
    "            c='r')\n",
    "plt.title('ELAPSED_TIME_DELTA vs CPU_TIME_DELTA')\n",
    "plt.xlabel('ELAPSED_TIME_DELTA')\n",
    "plt.ylabel('CPU_TIME_DELTA')\n",
    "plt.show()\n",
    "##################################\n",
    "plt.scatter(x=rep_hist_snapshot_df.values[:,get_col_pos(rep_hist_snapshot_df, 'ELAPSED_TIME_DELTA')],\n",
    "            y=rep_hist_snapshot_df.values[:,get_col_pos(rep_hist_snapshot_df, 'OPTIMIZER_COST')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_hist_snapshot_df, 'ELAPSED_TIME_DELTA')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_hist_snapshot_df, 'OPTIMIZER_COST')],\n",
    "            c='r')\n",
    "plt.title('ELAPSED_TIME_DELTA vs OPTIMIZER_COST')\n",
    "plt.xlabel('ELAPSED_TIME_DELTA')\n",
    "plt.ylabel('OPTIMIZER_COST')\n",
    "plt.show()\n",
    "##################################\n",
    "plt.scatter(x=rep_hist_snapshot_df.values[:,get_col_pos(rep_hist_snapshot_df, 'ELAPSED_TIME_DELTA')],\n",
    "            y=rep_hist_snapshot_df.values[:,get_col_pos(rep_hist_snapshot_df, 'EXECUTIONS_DELTA')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_hist_snapshot_df, 'ELAPSED_TIME_DELTA')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_hist_snapshot_df, 'EXECUTIONS_DELTA')],\n",
    "            c='r')\n",
    "plt.title('ELAPSED_TIME_DELTA vs EXECUTIONS_DELTA')\n",
    "plt.xlabel('ELAPSED_TIME_DELTA')\n",
    "plt.ylabel('EXECUTIONS_DELTA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "kmeans_vsql = KMeans(n_clusters=K, random_state=0).fit(rep_vsql_plan_df.values)\n",
    "print(kmeans_vsql)\n",
    "print(kmeans_vsql.labels_)\n",
    "unique, counts = np.unique(kmeans_vsql.labels_, return_counts=True)\n",
    "print('Unique: ' + str(unique))\n",
    "print('Counts: ' + str(counts))\n",
    "print(kmeans_vsql.cluster_centers_)\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "##################################\n",
    "plt.scatter(x=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'CARDINALITY')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'CARDINALITY')],\n",
    "            c='r')\n",
    "plt.title('COST vs CARDINALITY')\n",
    "plt.xlabel('COST')\n",
    "plt.ylabel('CARDINALITY')\n",
    "plt.show()\n",
    "##################################\n",
    "plt.scatter(x=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'BYTES')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'BYTES')],\n",
    "            c='r',)\n",
    "plt.title('COST vs BYTES')\n",
    "plt.xlabel('COST')\n",
    "plt.ylabel('BYTES')\n",
    "plt.show()\n",
    "##################################\n",
    "plt.scatter(x=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'IO_COST')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'IO_COST')],\n",
    "            c='r')\n",
    "plt.title('COST vs IO_COST')\n",
    "plt.xlabel('COST')\n",
    "plt.ylabel('IO_COST')\n",
    "plt.show()\n",
    "##################################\n",
    "plt.scatter(x=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'TEMP_SPACE')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'TEMP_SPACE')],\n",
    "            c='r')\n",
    "plt.title('COST vs TEMP_SPACE')\n",
    "plt.xlabel('COST')\n",
    "plt.ylabel('TEMP_SPACE')\n",
    "plt.show()\n",
    "##################################\n",
    "plt.scatter(x=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=rep_vsql_plan_df.values[:,get_col_pos(rep_vsql_plan_df, 'TIME')],\n",
    "            c='b')\n",
    "plt.scatter(x=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'COST')],\n",
    "            y=kmeans_hist.cluster_centers_[:,get_col_pos(rep_vsql_plan_df, 'TIME')],\n",
    "            c='r')\n",
    "plt.title('COST vs TIME')\n",
    "plt.xlabel('COST')\n",
    "plt.ylabel('TIME')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(kmeans_hist.labels_)):\n",
    "    if kmeans_hist.labels_[i] == 0:\n",
    "        print(rep_hist_snapshot_df.iloc[i])\n",
    "        break\n",
    "print('----------------------------------------')\n",
    "for i in range(len(kmeans_hist.labels_)):\n",
    "    if kmeans_hist.labels_[i] == 1:\n",
    "        print(rep_hist_snapshot_df.iloc[i])\n",
    "        break\n",
    "print('----------------------------------------')\n",
    "print('----------------------------------------')\n",
    "print('----------------------------------------')\n",
    "for i in range(len(kmeans_vsql.labels_)):\n",
    "    if kmeans_vsql.labels_[i] == 0:\n",
    "        print(rep_vsql_plan_df.iloc[i])\n",
    "        break\n",
    "print('----------------------------------------')\n",
    "for i in range(len(kmeans_vsql.labels_)):\n",
    "    if kmeans_vsql.labels_[i] == 1:\n",
    "        print(rep_vsql_plan_df.iloc[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Validation\n",
    "\n",
    "So as to verify the success of the clustering attempts, the achieved clustering labels require to be compared\n",
    "to what is assumed to be the actual label predictions. These 'actual' clusters will be assumed to coincide with\n",
    "the data matrix average - if a particular data vector is larger/smaller than the mean threshold, it will coincide in one cluster or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidateKMeans:\n",
    "    \"\"\"\n",
    "    Wrapper class for the KMeans algorithm, so as to validate the clustering it has achieved\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, X, k):\n",
    "        self.X = X\n",
    "        self.k = k\n",
    "        self.model = KMeans(n_clusters=self.k, random_state=0, init='k-means++',n_jobs=2)\n",
    "        self.model.fit(self.X)\n",
    "        self.__y_labels = self.model.labels_\n",
    "        self.scorings = None\n",
    "        print(self.model)\n",
    "    #        \n",
    "    def __get_threshold_vector(self):\n",
    "        mean = np.mean(self.X.values)\n",
    "        std = np.std(self.X.values)\n",
    "        std3 = np.multiply(std, 3)\n",
    "        return np.add(mean, std3)\n",
    "    #\n",
    "    def __calculate_expected_labels(self):\n",
    "        \"\"\"\n",
    "        Estimates label clustering by comparing them to a threshold mean value. These labels\n",
    "        will be used to gauge a scoring for the unsupervised clustering achieved by the K-Means algorithm.\n",
    "        \"\"\"\n",
    "        mean_vect = self.__get_threshold_vector()\n",
    "        mean_labels = []\n",
    "        for vector in self.X.values:\n",
    "            if np.greater(vector, mean_vect).any():\n",
    "                mean_labels.append(1)\n",
    "            else:\n",
    "                mean_labels.append(0)\n",
    "        return mean_labels\n",
    "    #\n",
    "    def outlier_score_precision(self):\n",
    "        if self.scorings is None or len(self.scorings) == 0:\n",
    "            raise ValueError('Scorings list is empty!')\n",
    "        elif len(self.scorings) > 2:\n",
    "            raise ValueError('Scorings list length is greater than 2! Must be composed of the following structure [scoring1, scoring2]')\n",
    "        return self.scorings[1]/self.scorings[0]\n",
    "    #\n",
    "    def label_centroids(self):\n",
    "        centroids = self.model.cluster_centers_\n",
    "        mean_vect = self.__get_threshold_vector()\n",
    "        categorized_labels = [] # [[Self_Classified_Label,Centroid_Label],[Self_Classified_Label,Centroid_Label],...]\n",
    "        for i in range(len(centroids)):\n",
    "            if np.greater(centroids[i], mean_vect).any():\n",
    "                categorized_labels.append([1,i])\n",
    "            else:\n",
    "                categorized_labels.append([0,i])\n",
    "        return categorized_labels\n",
    "    #\n",
    "    def evaluate_clusters(self):\n",
    "        y = self.__calculate_expected_labels()\n",
    "        yhat = []\n",
    "        labelled_centroids = self.label_centroids()\n",
    "        print('Labeled Centroids: ' + str(labelled_centroids))\n",
    "        #\n",
    "        for label in self.__y_labels:\n",
    "            for x, i in labelled_centroids:\n",
    "                if label == i:\n",
    "                    yhat.append(x)\n",
    "                    break\n",
    "        #\n",
    "        print('Total Clusters [' + str(self.k) + ']\\nDistribution:')\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print('Expected Label Distribution')\n",
    "        for i in range(len(unique)):\n",
    "            print('Label [' + str(unique[i]) + '] -> Count [' + str(counts[i]) + ']')\n",
    "            if unique[i] == 1:\n",
    "                self.scorings.append(counts[i])\n",
    "        unique, counts = np.unique(yhat, return_counts=True)\n",
    "        print('Clustered Label Distribution')\n",
    "        for i in range(len(unique)):\n",
    "            print('Label [' + str(unique[i]) + '] -> Count [' + str(counts[i]) + ']')\n",
    "            if unique[i] == 1:\n",
    "                self.scorings.append(counts[i])\n",
    "        #\n",
    "        print(\"\\n----\\nAccuracy: \" + str(accuracy_score(y, yhat)))\n",
    "        print(\"Precision: \" + str(precision_score(y, yhat, average='micro')))\n",
    "        print(\"Recall: \" + str(recall_score(y, yhat, average='micro')))\n",
    "        print(\"F-Score: \" + str(f1_score(y, yhat, average='micro')))\n",
    "        print(\"Outlier Score Precision [\" + str(self.outlier_score_precision()) + \"]1n----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhausting K\n",
    "\n",
    "Iterating over a number of K values, whilst gauging K under different number of combinations. Each K denotes the number of clusters as to group the data with. In turn, each cluster is then further categorized into 2 groups, those pertaining to:\n",
    "* Inliers\n",
    "* Outliers\n",
    "\n",
    "Accuracy, Precision, Recall & FScore metrics will be used to evaluate the effectiveness of each K-Means choice, with each experiment executed 3 times to anticipate for random variants of centroid positioning (Initial positioning is handled by K-Means++). Clustered amounts will be compared to a rough, hard placed metric, which determines any points to be outliers if they contain a data point at the 99th % standard deviation.\n",
    "\n",
    "An additional metric (apart from those mentioned above) will be used during the evaluation of this experiment. Particular focus will be given to the number of clustered outlier points, discounting inliers all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaust_k_possibilities(df):\n",
    "    \"\"\"\n",
    "    Method which attempts to exhaust a number of K options for the input pandas dataframe.\n",
    "    K Attempts will be attempted in steps of 2, so as to speed up the K finding process.\n",
    "\n",
    "    :param - df (Dataframe of type Pandas)\n",
    "    \"\"\"\n",
    "    k_experiment_scorings = [] # k, score\n",
    "    for k in range(2, len(df.columns), 2):\n",
    "        print('Experiment start -------------[' + str(k) + ']-------------')\n",
    "        experiment_scorings = []\n",
    "        for i in range(3):\n",
    "            validInstance = ValidateKMeans(df, k)\n",
    "            validInstance.evaluate_clusters()\n",
    "            experiment_scorings.append(validInstance.outlier_score_precision())\n",
    "        experiment_scorings.append([k, sum(k_experiment_scorings)/len(k_experiment_scorings)])\n",
    "        print('Experiment end -------------[' + str(k) + ']-------------')\n",
    "    #\n",
    "    final_score, final_k = 0,0\n",
    "    for k,score in k_experiment_scorings:\n",
    "        if score > final_score:\n",
    "            final_k = k\n",
    "            final_score = score\n",
    "    print('\\n\\nExperiment Conclusion: K[' + str(final_k) + '] - score[' + str(final_score) + ']')\n",
    "#\n",
    "print('Experiment: REP_HIST_SNAPSHOT K-MEANS GRID SEARCH')\n",
    "exhaust_k_possibilities(df=rep_hist_snapshot_df)\n",
    "print('Experiment: REP_VSQL_PLAN K-MEANS GRID SEARCH')\n",
    "exhaust_k_possibilities(df=rep_vsql_plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest Outlier Detection\n",
    "\n",
    "This section moves past K-Means clustering prediction, and attempts to detect / flag outliers using the Isolation Forest ensemble algorithm.\n",
    "\n",
    "Return the anomaly score of each sample using the IsolationForest algorithm\n",
    "\n",
    "The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n",
    "\n",
    "This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# REP_HIST_SNAPSHOT\n",
    "iforest_rep_hist_snapshot = IsolationForest(n_estimators=100, max_samples=256, contamination=0.2, random_state=0)\n",
    "iforest_rep_hist_snapshot.fit(rep_hist_snapshot_df.values)\n",
    "print(iforest_rep_hist_snapshot)\n",
    "scores = iforest_rep_hist_snapshot.decision_function(rep_hist_snapshot_df.values)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(scores, bins=50);\n",
    "plt.title('Isolation Forest Scorings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# REP_VSQL_PLAN\n",
    "iforest_rep_vsql_plan = IsolationForest(n_estimators=100, max_samples=256, contamination=0.2, random_state=0)\n",
    "iforest_rep_vsql_plan.fit(rep_vsql_plan_df.values)\n",
    "print(iforest_rep_vsql_plan)\n",
    "scores = iforest_rep_vsql_plan.decision_function(rep_vsql_plan_df.values)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(scores, bins=50);\n",
    "plt.title('Isolation Forest Scorings')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
