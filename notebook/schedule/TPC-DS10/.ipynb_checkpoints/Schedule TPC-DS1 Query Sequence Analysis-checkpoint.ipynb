{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database.\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# keras\n",
    "import keras as ke\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.utils import np_utils\n",
    "print('keras: %s' % ke.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=3 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "#\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "y_label = ['SQL_ID'] # Denotes which label to use for time series experiments\n",
    "#\n",
    "# Forest Config\n",
    "parallel_degree = 1\n",
    "n_estimators = 10\n",
    "#\n",
    "# Net Config\n",
    "batch_size=10\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "#\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (2230, 90)\n",
      "Shape After Aggregation: (1923, 78)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID','DBID','INSTANCE_NUMBER','SQL_ID']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(rep_hist_snapshot_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Empty Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A Columns\n",
      "\n",
      "\n",
      " REP_HIST_SNAPSHOT Features 78: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "#\n",
    "print('N/A Columns\\n')\n",
    "print('\\n REP_HIST_SNAPSHOT Features ' + str(len(rep_hist_snapshot_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_snapshot_df,headers=rep_hist_snapshot_df.columns)) + \"\\n\")\n",
    "#\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "#\n",
    "# Populating NaN values with amount '0'\n",
    "df = fill_na(df=rep_hist_snapshot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1923, 78)\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=['SNAP_ID'], ascending=True, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point precision conversion\n",
    "\n",
    "Each column is converted into a column of type values which are floating point for higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldnt convert column [SQL_ID]\n",
      "(1923, 78)\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    try:\n",
    "        df[column].astype('float32', inplace=True)\n",
    "        df[column] = np.round(df[column], 3) # rounds to 3 dp\n",
    "    except:\n",
    "        print('Couldnt convert column [' + column + ']')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before column drop:\n",
      "(1923, 78)\n",
      "\n",
      "Shape before changes: [(1923, 78)]\n",
      "Shape after changes: [(1923, 63)]\n",
      "Dropped a total [15]\n",
      "\n",
      "After flatline column drop:\n",
      "(1923, 63)\n",
      "\n",
      "After additional column drop:\n",
      "(1923, 52)\n"
     ]
    }
   ],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "print('Before column drop:')\n",
    "print(df.shape)\n",
    "df = drop_flatline_columns(df=df)\n",
    "print('\\nAfter flatline column drop:')\n",
    "print(df.shape)\n",
    "dropped_columns_df = [ 'PLAN_HASH_VALUE',\n",
    "                       'OPTIMIZER_ENV_HASH_VALUE',\n",
    "                       'LOADED_VERSIONS',\n",
    "                       'VERSION_COUNT',\n",
    "                       'PARSING_SCHEMA_ID',\n",
    "                       'PARSING_USER_ID',\n",
    "                       'CON_DBID',\n",
    "                       'SNAP_LEVEL',\n",
    "                       'SNAP_FLAG',\n",
    "                       'COMMAND_TYPE',\n",
    "                       'FLAG']\n",
    "df.drop(columns=dropped_columns_df, inplace=True)\n",
    "print('\\nAfter additional column drop:')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "Under the assumption that outliers have been capped/transformed, data is now passed through a min-max transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1923, 52)\n",
      "           SQL_ID  SNAP_ID  OPTIMIZER_COST  SHARABLE_MEM  FETCHES_TOTAL  \\\n",
      "0   03ggjrmy0wa1w      0.0    2.117521e-10      0.000188   2.555851e-08   \n",
      "26  8mdz49zkajhw3      0.0    2.018526e-10      0.000274   3.949952e-08   \n",
      "27  8t26unxsrxj72      0.0    7.243516e-11      0.000101   9.294005e-09   \n",
      "28  93n8wp5a8xyxn      0.0    6.905485e-11      0.000198   3.485252e-08   \n",
      "29  9ggx4p02d346a      0.0    3.138857e-12      0.000466   2.323501e-09   \n",
      "\n",
      "    FETCHES_DELTA  END_OF_FETCH_COUNT_TOTAL  END_OF_FETCH_COUNT_DELTA  \\\n",
      "0        0.000023              4.341988e-07                  0.000387   \n",
      "26       0.000023              6.710345e-07                  0.000387   \n",
      "27       0.000023              1.578905e-07                  0.000387   \n",
      "28       0.000023              5.920892e-07                  0.000387   \n",
      "29       0.000023              3.947262e-08                  0.000387   \n",
      "\n",
      "    SORTS_TOTAL  SORTS_DELTA              ...                \\\n",
      "0      0.000079     0.070267              ...                 \n",
      "26     0.000122     0.070267              ...                 \n",
      "27     0.000029     0.070267              ...                 \n",
      "28     0.000108     0.070267              ...                 \n",
      "29     0.000000     0.000000              ...                 \n",
      "\n",
      "    PHYSICAL_READ_REQUESTS_TOTAL  PHYSICAL_READ_REQUESTS_DELTA  \\\n",
      "0                       0.000306                      0.004264   \n",
      "26                      0.000442                      0.003986   \n",
      "27                      0.000066                      0.002528   \n",
      "28                      0.000406                      0.000000   \n",
      "29                      0.000017                      0.002609   \n",
      "\n",
      "    PHYSICAL_READ_BYTES_TOTAL  PHYSICAL_READ_BYTES_DELTA  \\\n",
      "0                    0.000529                   0.009933   \n",
      "26                   0.000778                   0.009439   \n",
      "27                   0.000114                   0.005856   \n",
      "28                   0.005268                   0.000000   \n",
      "29                   0.000036                   0.007355   \n",
      "\n",
      "    PHYSICAL_WRITE_REQUESTS_TOTAL  PHYSICAL_WRITE_REQUESTS_DELTA  \\\n",
      "0                             0.0                            0.0   \n",
      "26                            0.0                            0.0   \n",
      "27                            0.0                            0.0   \n",
      "28                            0.0                            0.0   \n",
      "29                            0.0                            0.0   \n",
      "\n",
      "    PHYSICAL_WRITE_BYTES_TOTAL  PHYSICAL_WRITE_BYTES_DELTA  \\\n",
      "0                          0.0                         0.0   \n",
      "26                         0.0                         0.0   \n",
      "27                         0.0                         0.0   \n",
      "28                         0.0                         0.0   \n",
      "29                         0.0                         0.0   \n",
      "\n",
      "    IO_OFFLOAD_RETURN_BYTES_TOTAL  IO_OFFLOAD_RETURN_BYTES_DELTA  \n",
      "0                        0.813362                            0.0  \n",
      "26                       0.813362                            0.0  \n",
      "27                       0.813362                            0.0  \n",
      "28                       0.840637                            0.0  \n",
      "29                       0.813362                            0.0  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Keep reference of label before normalizaing\n",
    "y_df = df[y_label]\n",
    "df.drop(columns=y_label, inplace=True)\n",
    "#\n",
    "# Normalize values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_normalized_values = scaler.fit_transform(df.values)\n",
    "#\n",
    "# Carry normalized values (numpy array) to pandas df\n",
    "df = pd.DataFrame(data=df_normalized_values, columns=df.columns)\n",
    "del df_normalized_values\n",
    "#\n",
    "# Combine back labels to normalized values\n",
    "df = pd.concat([y_df, df], axis=1, join_axes=[y_df.index])\n",
    "#\n",
    "print(str(df.shape))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging Labels\n",
    "\n",
    "Removes the label column, and adds it at the beginning of the matrix for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Column Switch: (1923, 52)\n",
      "Label ['SQL_ID'] shape: (1923, 1)\n",
      "Feature matrix shape: (1923, 51)\n",
      "After Column Switch: (1923, 52)\n",
      "           SQL_ID  SNAP_ID  OPTIMIZER_COST  SHARABLE_MEM  FETCHES_TOTAL  \\\n",
      "0   03ggjrmy0wa1w      0.0    2.117521e-10      0.000188   2.555851e-08   \n",
      "26  8mdz49zkajhw3      0.0    2.018526e-10      0.000274   3.949952e-08   \n",
      "27  8t26unxsrxj72      0.0    7.243516e-11      0.000101   9.294005e-09   \n",
      "28  93n8wp5a8xyxn      0.0    6.905485e-11      0.000198   3.485252e-08   \n",
      "29  9ggx4p02d346a      0.0    3.138857e-12      0.000466   2.323501e-09   \n",
      "\n",
      "    FETCHES_DELTA  END_OF_FETCH_COUNT_TOTAL  END_OF_FETCH_COUNT_DELTA  \\\n",
      "0        0.000023              4.341988e-07                  0.000387   \n",
      "26       0.000023              6.710345e-07                  0.000387   \n",
      "27       0.000023              1.578905e-07                  0.000387   \n",
      "28       0.000023              5.920892e-07                  0.000387   \n",
      "29       0.000023              3.947262e-08                  0.000387   \n",
      "\n",
      "    SORTS_TOTAL  SORTS_DELTA              ...                \\\n",
      "0      0.000079     0.070267              ...                 \n",
      "26     0.000122     0.070267              ...                 \n",
      "27     0.000029     0.070267              ...                 \n",
      "28     0.000108     0.070267              ...                 \n",
      "29     0.000000     0.000000              ...                 \n",
      "\n",
      "    PHYSICAL_READ_REQUESTS_TOTAL  PHYSICAL_READ_REQUESTS_DELTA  \\\n",
      "0                       0.000306                      0.004264   \n",
      "26                      0.000442                      0.003986   \n",
      "27                      0.000066                      0.002528   \n",
      "28                      0.000406                      0.000000   \n",
      "29                      0.000017                      0.002609   \n",
      "\n",
      "    PHYSICAL_READ_BYTES_TOTAL  PHYSICAL_READ_BYTES_DELTA  \\\n",
      "0                    0.000529                   0.009933   \n",
      "26                   0.000778                   0.009439   \n",
      "27                   0.000114                   0.005856   \n",
      "28                   0.005268                   0.000000   \n",
      "29                   0.000036                   0.007355   \n",
      "\n",
      "    PHYSICAL_WRITE_REQUESTS_TOTAL  PHYSICAL_WRITE_REQUESTS_DELTA  \\\n",
      "0                             0.0                            0.0   \n",
      "26                            0.0                            0.0   \n",
      "27                            0.0                            0.0   \n",
      "28                            0.0                            0.0   \n",
      "29                            0.0                            0.0   \n",
      "\n",
      "    PHYSICAL_WRITE_BYTES_TOTAL  PHYSICAL_WRITE_BYTES_DELTA  \\\n",
      "0                          0.0                         0.0   \n",
      "26                         0.0                         0.0   \n",
      "27                         0.0                         0.0   \n",
      "28                         0.0                         0.0   \n",
      "29                         0.0                         0.0   \n",
      "\n",
      "    IO_OFFLOAD_RETURN_BYTES_TOTAL  IO_OFFLOAD_RETURN_BYTES_DELTA  \n",
      "0                        0.813362                            0.0  \n",
      "26                       0.813362                            0.0  \n",
      "27                       0.813362                            0.0  \n",
      "28                       0.840637                            0.0  \n",
      "29                       0.813362                            0.0  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Before Column Switch: ' + str(df.shape))\n",
    "y_df = df[y_label]\n",
    "df.drop(columns=y_label, inplace=True)\n",
    "print(\"Label \" + str(y_label) + \" shape: \" + str(y_df.shape))\n",
    "print(\"Feature matrix shape: \" + str(df.shape))\n",
    "#\n",
    "# Merging labels and features in respective order\n",
    "df = pd.concat([y_df, df], axis=1, sort=False)\n",
    "print('After Column Switch: ' + str(df.shape))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before label encoding: (1923, 52)\n",
      "After label encoding: (1923, 52)\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "379\n",
      "['01d5n1nm17r2h' '01tp87bk1t2zv' '03ggjrmy0wa1w' '04kug40zbu4dm'\n",
      " '06dymzb481vnd' '06g9mhm5ba7tt' '09vrdx888wvvb' '0a08ug2qc1j82'\n",
      " '0a7q9v9nd2qc1' '0aq14dznn91rg' '0f60bzgt9127c' '0ga8vk4nftz45'\n",
      " '0hdquu87pydzk' '0hhmdwwgxbw0r' '0jj0ct4x4gy27' '0kcbwucxmazcp'\n",
      " '0kkhhb2w93cx0' '0m78skf1mudnb' '0qbzfjt00pbsx' '0v3dvmc22qnam'\n",
      " '0vcz1xqfrykmw' '0w26sk6t6gq98' '0x6ks1umjn6k6' '0y080mnfaqk3u'\n",
      " '0ym9wzzys5zax' '130r442w3nfny' '13a9r2xkx1bxb' '13ys8ux8xvrbm'\n",
      " '14f5ngrj3cc5h' '14kx436hrv7cc' '193ncz0tf25hw' '1aajwypydy0wu'\n",
      " '1c3x1d7pc0cmt' '1fn8v91f0arf0' '1gfaj4z5hn1kf' '1hxfbnas8xr2j'\n",
      " '1jhyrdp21f2q6' '1k33bhcnpbrwx' '1kz16yhs993h2' '1ms8wj24sqny4'\n",
      " '1p5grz1gs7fjq' '1pgnzc6zf7ctc' '1pv23p59mjs0v' '1r7b985mxqj71'\n",
      " '1rpgk59t8pvs6' '1u97hwfu7dcmz' '1v2b661suttyp' '1wk0t84cwr5ps'\n",
      " '1wpns6pagm2qj' '1wq6da7n103qd' '1wz811srf8xh8' '2046z7kdh823h'\n",
      " '20bqsr6btd9x9' '20vv6ttajyjzq' '248suhpf1dbya' '24u3pyk3mymm4'\n",
      " '26jdypa362wv9' '279ragg4g2fb9' '27ws0jh2nqdwj' '288tccrcj5fyj'\n",
      " '297gc42f2hwdd' '29jd5f9tamanf' '29mjaymwt5p6d' '2afh4r7z1rfv6'\n",
      " '2cpvayu3hyq91' '2d5v40f700a52' '2h0gb24h6zpnu' '2hnpu9m861609'\n",
      " '2j25hzq35w45h' '2j5bk3tn2zt0g' '2mp99nzd9u1qp' '2pz0tqbv91m11'\n",
      " '2sqcyuffj0pnb' '2tzs6v7mwnznt' '2ugyr59f69wwg' '2vkaf4z6ktk9c'\n",
      " '2vv49kn276y0z' '2wdrw5tqputaq' '2wuhkcaz4uhs5' '30rvbvzuuxvvd'\n",
      " '32qq8k1n8ynn9' '33cmj01uf5zfb' '33vw5865cwyyn' '3419gsthd5szh'\n",
      " '350d18utrk6cu' '35j72wf33gaqa' '360qzju916m57' '38243c4tqrkxm'\n",
      " '39m4sx9k63ba2' '39nyc1pykjg41' '3bmwxjakz3ufw' '3c1kubcdjnppq'\n",
      " '3dqrj325buvt8' '3j3q8xkn9qvrv' '3ja745d9jkpc8' '3kwn98m1bt3jp'\n",
      " '3kywng531fcxu' '3m6xpgmgktw3d' '3m8smr0v7v1m6' '3rd3sp2ak89rc'\n",
      " '3ru3r9twxsazy' '3sqgkcng6vx8r' '3t6crmxzc7gwj' '3tjqbzbjtv7kr'\n",
      " '3un99a0zwp4vd' '3wkn3gt9zn7ka' '3y6pgnk2ubw7g' '41ncsfu26db1w'\n",
      " '45zq5xs59sjsj' '46q97td4vcwzk' '47r1y8yn34jmj' '49s332uhbnsma'\n",
      " '4b4wp0a8dvkf0' '4bmxgstv2nr07' '4cgbvpjc134nu' '4fv4r99k9v51q'\n",
      " '4g1u6kabran4u' '4j2p57b4stn2a' '4mmqayxa4hwb0' '4q1rzhn63sgpt'\n",
      " '4r62dbqxw2r10' '4rn02xyj9tujk' '4tmd72wfn52m4' '4u268zn6r57tm'\n",
      " '4v69jmg5x7mvg' '4vcvqy7vvy5zm' '4w3rn6bvafgfc' '4xkj9bw91x8tf'\n",
      " '4yb79gnqp1v90' '512674nmuhtg6' '52p0tj20muz3f' '52xfvkwy3u9xx'\n",
      " '54qdvyrqsg8m6' '55u3mjpyxzv5g' '593hanvpdtfkd' '59zh1b9759nf4'\n",
      " '5g88vmdgd99f7' '5h7w8ykwtb2xt' '5h7xwu611bxv2' '5hrvvu1r771m5'\n",
      " '5jxkb7r2kn817' '5nn31913tabu9' '5pf6jgsps9dtk' '5pkjgqjxz0kd8'\n",
      " '5pxcsstbnzj04' '5qgz1p0cut7mx' '5ry53w1yug897' '5wwkp7spyq2fn'\n",
      " '5y3x5cyp1xn74' '61mh8ky0utssp' '622ufbrgvxdc7' '646bq4xw23aw0'\n",
      " '64fq7u4a68nr1' '664j0xx9u5wvu' '68uhsqwvcfqab' '68xv41yva21a3'\n",
      " '6ajkhukk78nsr' '6ccr6xx4tq5yy' '6d0rbkgfab2xa' '6fvfqaw68q59b'\n",
      " '6hawp1pp037tk' '6kckz8dk87m45' '6n2qqv1brfhpp' '6nc5c1qczss8j'\n",
      " '6ntz6ynyaytgf' '6u001adh62r0f' '6zcux9jb78w36' '6zs29hb3gpcf5'\n",
      " '6zzvnuw5p3tat' '71uursqtj1j2m' '73tu7n31zn5pz' '74anujtt8zw4h'\n",
      " '76ds5wxsv7f5t' '76prtdxhjck3p' '76ur6b49gfsu8' '77mtwmnd4c4pk'\n",
      " '785wb90xs3r0t' '7d6r5amubkffq' '7fbzhzg6ysu25' '7frp5qgh6jpjn'\n",
      " '7gy3y7kjsbfuv' '7h8x6dzapnsua' '7hys3h7ysgf9m' '7hzbv7ux8uhmt'\n",
      " '7j630dz60gqa3' '7jpt4cpfvcy1k' '7m8xtjmn5zv0g' '7mu447a2r8bb8'\n",
      " '7tchj0bmt6tn1' '7ts87fuvk3dhz' '7u49y06aqxg1s' '7u88drn7gy4fw'\n",
      " '7ub921xztd5pt' '7uzyw6t048658' '7vtvbg7s3zcyp' '7w116jy6ysqpm'\n",
      " '80bz53g2s4306' '84aqqjbf6dkt3' '84ntdbh48ctu9' '84zqd7a3ap5ud'\n",
      " '85cmvvurya34f' '865qwpcdyggkk' '86kwhy1f0bttn' '876ytpt4g7dza'\n",
      " '87gaftwrm2h68' '87gtj5jaq4a3t' '88uytsd7r41y9' '8adcur42pddxj'\n",
      " '8axdfz2khnyzs' '8cx2kbtd2w4dn' '8cxgpdw3qqxqg' '8k0qd372mh9td'\n",
      " '8mdz49zkajhw3' '8nhg2pdrzs3ww' '8r5cuhx03m68d' '8swypbbr0m372'\n",
      " '8t26unxsrxj72' '8u809k64x3nzd' '8ws5dkvuusafp' '8yk1b0cdrs5fg'\n",
      " '8ymq1gb13rfab' '8zc85a8249x81' '909ysvbz0uqd8' '934ur8r7tqbjx'\n",
      " '93mj5tk40ap8f' '93n8wp5a8xyxn' '93z4cu03tf4z1' '95y5r5ksf94b1'\n",
      " '96g93hntrzjtr' '97pbnmc9h3ny4' '9ddxxas8hu8sv' '9dnnpagwcg2cu'\n",
      " '9ffbbqqfbgxyu' '9ffht8tuysgx9' '9ggx4p02d346a' '9rmg1ukgcyzpv'\n",
      " '9tgj4g8y4rwy8' '9tpnr4jz10j3y' '9ua42c6f2qs7s' '9vmcsc3prvxpa'\n",
      " '9x8gaksqvta15' '9y7s2afnkqs9d' '9zg9qd9bm4spu' 'a2ttbs5gf0xpu'\n",
      " 'a5ym25h9gmsvr' 'a6fy23us0jz84' 'a6g97rawd3ggv' 'a6ygk0r9s5xuj'\n",
      " 'abr26jpdfxwhy' 'ac805sb6btmg3' 'af28xbfcdrkbb' 'aggcw7yk1a7s6'\n",
      " 'akk9bpbxjkj2g' 'aksaf74grz97n' 'amd5g84rhgp0h' 'antwva8smnnmt'\n",
      " 'as8sqm1c84n07' 'asvzxj61dc5vs' 'at1ygf4rd7cvj' 'at2h5x34yj2c4'\n",
      " 'au8ztarrm6vvs' 'avmf520g02n1j' 'b0r7k3v44k1z8' 'b0stf459a3121'\n",
      " 'b2hr39jj45854' 'b4j3z1g2nwfys' 'b8cjbq1au6kz8' 'b94dthkxngzzr'\n",
      " 'b9c6ffh8tc71f' 'bb926a5dcb8kr' 'bcbpkhm3cq424' 'bj5v9w48937nu'\n",
      " 'bkq9pjcfvm9vn' 'bkw3supfuskbc' 'bndp6dxkmkxyc' 'bp5uk9gssq3kc'\n",
      " 'brsaqch9hhutd' 'bvkckyya5hyqx' 'bvwv6jvt9b0mj' 'bwg61ruy67kmv'\n",
      " 'bwsf4tnh0gcgv' 'c1jdamyak9mqm' 'c1x7a33hxwfyy' 'c20kr1p071kkn'\n",
      " 'c27s55jww516q' 'c2bxq4kd3uj1t' 'c3tb6vfmqkgf1' 'c68zcykp5x303'\n",
      " 'c6awqs517jpj0' 'c9umxngkc3byq' 'cat054a4rkjqr' 'cb21bacyh3c7d'\n",
      " 'cbhzzp1d7dhkc' 'cd1q0zw912c1p' 'cfk18fw3fynvs' 'cfsnf5tz2q74a'\n",
      " 'cgnmvpx2g5jq2' 'ch4vvuszvnv79' 'chbj5w1vwums1' 'cjq93m442uprp'\n",
      " 'cjty8qc84r509' 'ck9067jjt5ht5' 'ckfvfhzy0qrws' 'cqrk7q7f6nk3d'\n",
      " 'crt85hc6g7yrb' 'ct4bbu3duky6v' 'ct6c4h224pxgz' 'cvn54b7yz0s8u'\n",
      " 'cw6vxf0kbz3v1' 'cwzcht2y54876' 'd08vu8gv7xb6w' 'd0dnpw2mcxqxh'\n",
      " 'd134mqkq6kgbu' 'd2tvgg49y2ap6' 'd2zx61xsr2xfn' 'd4hnzu9x4k2gy'\n",
      " 'd5jd9b2xct7xz' 'd6fsyfkfqdd0p' 'd6vqfmt62hypx' 'd6vwqbw6r2ffk'\n",
      " 'd7w1dugmzb9n9' 'da5g5ca8pmau7' 'dasnrr294mm1k' 'dcmtw8yvz8v20'\n",
      " 'ddc2dxdzc9ugm' 'dfffkcnqfystw' 'dg7h5qtrw6dr7' 'dh7u76hwz04bq'\n",
      " 'dm7wjadqjyqna' 'dr23gzy8yj2n8' 'dvpfc386d6dmm' 'dxv968j0352kb'\n",
      " 'dyk4dprp70d74' 'f0f68t7c4105q' 'f0h5rpzmhju11' 'f30jhmhnc9s4k'\n",
      " 'f4498zktd6sv1' 'f80h0xb1qvbsk' 'f9hk3q1y2b8nt' 'fc0va0vju750z'\n",
      " 'fg4skgcja2cyj' 'fgcvncf0ab9dh' 'fhf8upax5cxsz' 'fkc81h2686aqc'\n",
      " 'fnghrcwz4mfb3' 'fr04r8ayb5nxz' 'frjd8zfy2jfdq' 'fs4p95w7yg25b'\n",
      " 'fsprw99ahsvyn' 'ft6n0dg4unah5' 'fur78fafyqrkn' 'fv9zrr2b2b61w'\n",
      " 'fx7sjdj48pn6z' 'fykucbhdkfqv9' 'g0b4snpj74cv5' 'g22f2h8s9vfrz'\n",
      " 'g2gc9wj14hqw9' 'g3bwhq7gjqatj' 'g41n1r60y3d71' 'g4gp07gt2z920'\n",
      " 'g4y6nw3tts7cc' 'g5sbyzcg89kg1' 'g67y24u07t83k' 'g6dr457smp5xj'\n",
      " 'g7mt7ptq286u7' 'g7vkhf01vmwx4' 'gdh0vpjkmbtjw' 'gdhz78y24tvnz'\n",
      " 'gh5w0gcyfaujs' 'ghkg6v5k2uc6g' 'gkjkxbzzptg00' 'gmjt6pxkza5g2'\n",
      " 'gqcb78ccdpmmf' 'grwydz59pu6mc' 'gsfnqdfcvy33q' 'gsqxm6zz3w2by'\n",
      " 'gu5x4z494njku' 'gvcadr1hm4arv' 'gxbdzk4g9tzrx']\n",
      "    SQL_ID  SNAP_ID  OPTIMIZER_COST  SHARABLE_MEM  FETCHES_TOTAL  \\\n",
      "0        2      0.0    2.117521e-10      0.000188   2.555851e-08   \n",
      "26     212      0.0    2.018526e-10      0.000274   3.949952e-08   \n",
      "27     216      0.0    7.243516e-11      0.000101   9.294005e-09   \n",
      "28     225      0.0    6.905485e-11      0.000198   3.485252e-08   \n",
      "29     234      0.0    3.138857e-12      0.000466   2.323501e-09   \n",
      "\n",
      "    FETCHES_DELTA  END_OF_FETCH_COUNT_TOTAL  END_OF_FETCH_COUNT_DELTA  \\\n",
      "0        0.000023              4.341988e-07                  0.000387   \n",
      "26       0.000023              6.710345e-07                  0.000387   \n",
      "27       0.000023              1.578905e-07                  0.000387   \n",
      "28       0.000023              5.920892e-07                  0.000387   \n",
      "29       0.000023              3.947262e-08                  0.000387   \n",
      "\n",
      "    SORTS_TOTAL  SORTS_DELTA              ...                \\\n",
      "0      0.000079     0.070267              ...                 \n",
      "26     0.000122     0.070267              ...                 \n",
      "27     0.000029     0.070267              ...                 \n",
      "28     0.000108     0.070267              ...                 \n",
      "29     0.000000     0.000000              ...                 \n",
      "\n",
      "    PHYSICAL_READ_REQUESTS_TOTAL  PHYSICAL_READ_REQUESTS_DELTA  \\\n",
      "0                       0.000306                      0.004264   \n",
      "26                      0.000442                      0.003986   \n",
      "27                      0.000066                      0.002528   \n",
      "28                      0.000406                      0.000000   \n",
      "29                      0.000017                      0.002609   \n",
      "\n",
      "    PHYSICAL_READ_BYTES_TOTAL  PHYSICAL_READ_BYTES_DELTA  \\\n",
      "0                    0.000529                   0.009933   \n",
      "26                   0.000778                   0.009439   \n",
      "27                   0.000114                   0.005856   \n",
      "28                   0.005268                   0.000000   \n",
      "29                   0.000036                   0.007355   \n",
      "\n",
      "    PHYSICAL_WRITE_REQUESTS_TOTAL  PHYSICAL_WRITE_REQUESTS_DELTA  \\\n",
      "0                             0.0                            0.0   \n",
      "26                            0.0                            0.0   \n",
      "27                            0.0                            0.0   \n",
      "28                            0.0                            0.0   \n",
      "29                            0.0                            0.0   \n",
      "\n",
      "    PHYSICAL_WRITE_BYTES_TOTAL  PHYSICAL_WRITE_BYTES_DELTA  \\\n",
      "0                          0.0                         0.0   \n",
      "26                         0.0                         0.0   \n",
      "27                         0.0                         0.0   \n",
      "28                         0.0                         0.0   \n",
      "29                         0.0                         0.0   \n",
      "\n",
      "    IO_OFFLOAD_RETURN_BYTES_TOTAL  IO_OFFLOAD_RETURN_BYTES_DELTA  \n",
      "0                        0.813362                            0.0  \n",
      "26                       0.813362                            0.0  \n",
      "27                       0.813362                            0.0  \n",
      "28                       0.840637                            0.0  \n",
      "29                       0.813362                            0.0  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "### Label Encoding\n",
    "#\n",
    "# Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "print(\"Before label encoding: \" + str(df.shape))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['SQL_ID'])\n",
    "df['SQL_ID'] = le.transform(df['SQL_ID'])\n",
    "print(\"After label encoding: \" + str(df.shape) + \"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "print(len(le.classes_))\n",
    "print(le.classes_)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-3)', 'var2(t-3)', 'var3(t-3)', 'var4(t-3)', 'var5(t-3)',\n",
      "       'var6(t-3)', 'var7(t-3)', 'var8(t-3)', 'var9(t-3)', 'var10(t-3)',\n",
      "       ...\n",
      "       'var43(t+3)', 'var44(t+3)', 'var45(t+3)', 'var46(t+3)', 'var47(t+3)',\n",
      "       'var48(t+3)', 'var49(t+3)', 'var50(t+3)', 'var51(t+3)', 'var52(t+3)'],\n",
      "      dtype='object', length=361)\n",
      "(1917, 361)\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(1917, 3)\n",
      "\n",
      "-------------\n",
      "Features After Time Shift\n",
      "Index(['var1(t-3)', 'var2(t-3)', 'var3(t-3)', 'var4(t-3)', 'var5(t-3)',\n",
      "       'var6(t-3)', 'var7(t-3)', 'var8(t-3)', 'var9(t-3)', 'var10(t-3)',\n",
      "       ...\n",
      "       'var43(t)', 'var44(t)', 'var45(t)', 'var46(t)', 'var47(t)', 'var48(t)',\n",
      "       'var49(t)', 'var50(t)', 'var51(t)', 'var52(t)'],\n",
      "      dtype='object', length=208)\n",
      "(1917, 208)\n",
      "\n",
      "-------------\n",
      "Labels After Time Shift\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(1917, 3)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #     for header in headers:\n",
    "    #         if \"(t)\" in header:\n",
    "    #             dropped_headers.append(header)\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_row = []\n",
    "for i in range(lag+1,(lag*2)+2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns)*i:len(df.columns)*i + len(y_label)]\n",
    "    y_row.append(y_df_column_names)\n",
    "y_df_column_names = []   \n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "#\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "print('\\n-------------\\nLabels After Time Shift')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Classification (Many To Many)\n",
    "\n",
    "Classification attemps using RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random Forest\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest Class (Regression + Classification)\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, mode, n_estimators, max_depth=None,parallel_degree=1):\n",
    "        self.mode = self.__validate(mode)\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.parallel_degree=parallel_degree\n",
    "        if self.mode == 'regression':\n",
    "            #\n",
    "            raise NotImplemented('Regression subtype is not supported for this experiment!')\n",
    "        elif self.mode == 'classification':\n",
    "            self.model = RandomForestClassifier(max_depth=self.max_depth,\n",
    "                                                n_estimators=self.n_estimators,\n",
    "                                                n_jobs=self.parallel_degree)\n",
    "    #\n",
    "    def __validate(self, mode):\n",
    "        mode = mode.lower()\n",
    "        if mode not in ('classification','regression'):\n",
    "            raise ValueError('Specified mode is incorrect!')\n",
    "        return mode\n",
    "    #\n",
    "    def fit_model(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fits training data to target labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train,y_train)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, plot=False):\n",
    "        \"\"\"\n",
    "        Runs test data through previously trained model, and evaluate differently depending if a regression of classification model\n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        if self.mode == 'regression':\n",
    "            #\n",
    "            raise NotImplemented('Regression subtype is not supported for this experiment!')\n",
    "            #\n",
    "        elif self.mode == 'classification':\n",
    "            #\n",
    "            # F1-Score Evaluation\n",
    "            for i in range(y.shape[1]):\n",
    "                print(str(len(y[:, i])))\n",
    "                print(str(len(yhat[:, i])))\n",
    "                print(y[:, i])\n",
    "                print(yhat[:, i])\n",
    "                f1 = f1_score(y[:,i], yhat[:,i], average='micro') # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "                print('Test FScore ' + y_labels[0] + ' with LAG value [' + str(i) + ']: ' +  str(f1))\n",
    "        #\n",
    "        if plot:\n",
    "            for i in range(0, len(y[0])):\n",
    "                plt.rcParams['figure.figsize'] = [20, 15]\n",
    "                plt.plot(y[:,i], label='actual')\n",
    "                plt.plot(yhat[:,i], label='predicted')\n",
    "                plt.legend(['actual', 'predicted'], loc='upper left')\n",
    "                plt.title(y_labels[i%len(y_labels)] + \" +\" + str(math.ceil((i+1)/len(y_label))))\n",
    "                plt.show()\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, estimator, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'estimator', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'estimator': estimator,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "# X_train = X_train.values\n",
    "# y_train = y_train.values\n",
    "# print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "# print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "# #\n",
    "# X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "# X_validate = X_validate.values\n",
    "# X_test = X_test.values\n",
    "# y_validate = y_validate.values\n",
    "# y_test = y_test.values\n",
    "# print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "# print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "# print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "# print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)) + \"\\n------------------------------\")\n",
    "# #\n",
    "# print(X_train[0:5])\n",
    "# print(y_train[0:5])\n",
    "# print('------------------------------------------------------------')\n",
    "# print(X_validate[0:5])\n",
    "# print(y_validate[0:5])\n",
    "# print('------------------------------------------------------------')\n",
    "# print(X_test[0:5])\n",
    "# print(y_test[0:5])\n",
    "# #\n",
    "# # Train on discrete data (Train > Validation)\n",
    "# print('Training + Validation')\n",
    "# model = RandomForest(mode='classification',\n",
    "#                      n_estimators=n_estimators,\n",
    "#                      parallel_degree=parallel_degree)\n",
    "# model.fit_model(X_train=X_train,\n",
    "#                 y_train=y_train)\n",
    "# model.predict_and_evaluate(X=X_validate,\n",
    "#                            y=y_validate,\n",
    "#                            y_labels=y_label,\n",
    "#                            plot=True)\n",
    "# #\n",
    "# # Train on discrete data (Train + Validation > Test)\n",
    "# print('\\n\\nTraining + Testing')\n",
    "# model.fit_model(X_train=X_validate,\n",
    "#                 y_train=y_validate)\n",
    "# model.predict_and_evaluate(X=X_test,\n",
    "#                            y=y_test,\n",
    "#                            y_labels=y_label,\n",
    "#                            plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Regression (Many to Many Approach)\n",
    "### Designing the network\n",
    "\n",
    "- The first step is to define your network.\n",
    "- Neural networks are defined in Keras as a sequence of layers. The container for these layers is the **Sequential class**.\n",
    "- The first step is to create an instance of the Sequential class. Then you can create your layers and add them in the order that they should be connected.\n",
    "- The LSTM recurrent layer comprised of memory units is called LSTM().\n",
    "- A fully connected layer that often follows LSTM layers and is used for outputting a prediction is called Dense().\n",
    "- The first layer in the network must define the number of inputs to expect.\n",
    "- Input must be three-dimensional, comprised of samples, timesteps, and features.\n",
    "    - **Samples:** These are the rows in your data.\n",
    "    - **Timesteps:** These are the past observations for a feature, such as lag variables.\n",
    "    - **Features:** These are columns in your data.\n",
    "- Assuming your data is loaded as a NumPy array, you can convert a 2D dataset to a 3D dataset using the reshape() function in NumPy.\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "* https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "* https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/\n",
    "* https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "* https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# KerasModel Class\n",
    "class KerasModel:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    def __init__(self, X_train, y_train, classification_classes, optimizer):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X_train - Training data used to train the model\n",
    "        :param y_train - Test data used to test the model\n",
    "        :param layers - A list of values, where in each value denotes a layer, and the number of neurons for that layer\n",
    "        :param loss_function - Function used to measure fitting of model (predicted from actual)\n",
    "        :param optimizer - Function used to optimize the model (eg: Gradient Descent)\n",
    "        \"\"\"\n",
    "        self.model = ke.models.Sequential()\n",
    "        #self.model.add(Embedding(classification_classes, 10, input_length=X_train.shape[1]))\n",
    "        #self.model.add(Flatten())\n",
    "#         self.model.add(ke.layers.LSTM(X_train.shape[2], input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "#         self.model.add(ke.layers.Dense(X_train.shape[2], activation='softmax'))\n",
    "        #self.model.add(ke.layers.LSTM(layers[i], input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "        #self.model.add(ke.layers.LSTM(layers[i]))\n",
    "        #\n",
    "        #self.model.add(Flatten())\n",
    "        #\n",
    "        self.model.add(ke.layers.LSTM(256, input_shape=(X_train[1], X_train.shape[2])))\n",
    "        self.model.add(ke.layers.Dropout(0.2))\n",
    "        self.model.add(ke.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "        self.loss_func = 'sparse_categorical_crossentropy'\n",
    "        self.model.compile(loss=self.loss_func, optimizer=optimizer, metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "\n",
    "    #\n",
    "    def fit_model(self, X_train, X_test, y_train, y_test, epochs=50, batch_size=50, verbose=2, shuffle=False,\n",
    "                  plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "        \"\"\"\n",
    "        history = self.model.fit(x=X_train,\n",
    "                                 y=y_train,\n",
    "                                 epochs=epochs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 validation_data=(X_test, y_test),\n",
    "                                 verbose=verbose,\n",
    "                                 shuffle=shuffle)\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(history.history['acc'], label='train')\n",
    "            plt.plot(history.history['val_acc'], label='validation')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, plot=False):\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # F1-Score Evaluation\n",
    "        for i in range(y.shape[1]):\n",
    "            print(str(len(y[:, i])))\n",
    "            print(str(len(yhat[:, i])))\n",
    "            print(y[:, i])\n",
    "            print(yhat[:, i])\n",
    "            f1 = f1_score(y[:, i], \n",
    "                          yhat[:, i],\n",
    "                          average='micro')  # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "            print('Test FScore ' + y_labels[0] + ' with LAG value [' + str(i) + ']: ' +  str(f1))\n",
    "        #\n",
    "        if plot:\n",
    "            for i in range(0, y[0]):\n",
    "                plt.rcParams['figure.figsize'] = [20, 15]\n",
    "                plt.plot(y[:, i], label='actual')\n",
    "                plt.plot(yhat[:, i], label='predicted')\n",
    "                plt.legend(['actual', 'predicted'], loc='upper left')\n",
    "                plt.title(y_labels[i%len(y_labels)] + \" +\" + str(math.ceil((i+1)/len(y_label))))\n",
    "                plt.show()\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, batch, depth, epoch, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'batch', 'depth', 'epoch', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'batch': batch,\n",
    "                             'depth':depth,\n",
    "                             'epoch':epoch,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape [(1533, 208)] Type - [<class 'numpy.ndarray'>]\n",
      "y_train shape [(1533, 3)] Type - [<class 'numpy.ndarray'>]\n",
      "X_validate shape [(192, 208)] Type - [<class 'numpy.ndarray'>]\n",
      "y_validate shape [(192, 3)] Type - [<class 'numpy.ndarray'>]\n",
      "X_test shape [(192, 208)] Type - [<class 'numpy.ndarray'>]\n",
      "y_test shape [(192, 3)] Type - [<class 'numpy.ndarray'>]\n",
      "\n",
      "\n",
      "[[3.08000000e+02 2.69230769e-01 1.81570799e-10 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [3.09000000e+02 1.00000000e+00 2.02818446e-11 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [2.65000000e+02 6.53846154e-01 6.33010857e-09 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [3.05000000e+02 5.76923077e-01 2.12959368e-10 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [2.03000000e+02 1.92307692e-01 1.42286798e-09 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]]\n",
      "[[319. 196. 375.]\n",
      " [342. 339. 337.]\n",
      " [234. 227. 221.]\n",
      " [333. 352. 354.]\n",
      " [106.  98.  73.]]\n",
      "------------------------------------------------------------\n",
      "[[2.72000000e+02 3.46153846e-01 7.96786752e-12 ... 1.52489389e-04\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [4.00000000e+00 9.23076923e-01 1.01093164e-07 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [1.06000000e+02 1.92307692e-01 1.21908373e-09 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [3.90000000e+01 2.30769231e-01 2.65595584e-12 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [2.14000000e+02 8.84615385e-01 2.41450531e-13 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]]\n",
      "[[234. 196. 228.]\n",
      " [186. 106. 110.]\n",
      " [ 40. 115.  39.]\n",
      " [  4.   1. 196.]\n",
      " [363. 302. 204.]]\n",
      "------------------------------------------------------------\n",
      "[[6.80000000e+01 5.76923077e-01 2.10544863e-10 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [2.40000000e+01 8.84615385e-01 9.28860193e-10 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [3.11000000e+02 8.84615385e-01 4.82901062e-13 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [1.56000000e+02 8.07692308e-01 1.05996783e-10 ... 1.52489389e-04\n",
      "  8.13361937e-01 0.00000000e+00]\n",
      " [2.42000000e+02 1.00000000e+00 4.94587268e-09 ... 0.00000000e+00\n",
      "  8.13361937e-01 0.00000000e+00]]\n",
      "[[169. 152. 136.]\n",
      " [  1.  39. 124.]\n",
      " [203. 332. 196.]\n",
      " [139. 148. 134.]\n",
      " [204. 363. 360.]]\n",
      "\n",
      "Reshaping Training Frames\n",
      "X_train shape [(1533, 1, 208)] Type - [<class 'numpy.ndarray'>]\n",
      "X_validate shape [(192, 1, 208)] Type - [<class 'numpy.ndarray'>]\n",
      "X_test shape [(192, 1, 208)] Type - [<class 'numpy.ndarray'>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: only size-1 arrays can be converted to Python scalars.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    946\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    481\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ae192614d1a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m                             \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                             \u001b[0mclassification_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                             optimizer='sgd')\n\u001b[0m\u001b[0;32m     44\u001b[0m discrete_model.fit_model(X_train=X_train,\n\u001b[0;32m     45\u001b[0m                          \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-c3200151cc6f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X_train, y_train, classification_classes, optimizer)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#self.model.add(Flatten())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    159\u001b[0m                         \u001b[0mbatch_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                         name=layer.name + '_input')\n\u001b[0m\u001b[0;32m    162\u001b[0m                     \u001b[1;31m# This will build the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                              input_tensor=tensor)\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                                          name=self.name)\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   1744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   6034\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6035\u001b[0m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6036\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6037\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   6038\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[1;31mTypeError\u001b[0m: Error converting shape to a TensorShape: only size-1 arrays can be converted to Python scalars."
     ]
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "#\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - [\" + str(type(y_train)) + \"]\")\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "#\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - [\" + str(type(y_validate)) + \"]\")\n",
    "#\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - [\" + str(type(y_test)) + \"]\")\n",
    "print(\"\\n\")\n",
    "#\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate[0:5])\n",
    "print(y_validate[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "#\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_validate = X_validate.reshape((X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "discrete_model = KerasModel(X_train=X_train,\n",
    "                            y_train=y_train,\n",
    "                            classification_classes=len(le.classes_),\n",
    "                            optimizer='sgd')\n",
    "discrete_model.fit_model(X_train=X_train,\n",
    "                         X_test=X_validate,\n",
    "                         y_train=y_train,\n",
    "                         y_test=y_validate,\n",
    "                         epochs=epochs, \n",
    "                         batch_size=batch_size,\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_validate,\n",
    "                                    y=y_validate,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "discrete_model.fit_model(X_train=X_validate,\n",
    "                         X_test=X_test,\n",
    "                         y_train=y_validate,\n",
    "                         y_test=y_test,\n",
    "                         epochs=epochs, \n",
    "                         batch_size=1, # Incremental batch size fitting\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_test,\n",
    "                                    y=y_test,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
