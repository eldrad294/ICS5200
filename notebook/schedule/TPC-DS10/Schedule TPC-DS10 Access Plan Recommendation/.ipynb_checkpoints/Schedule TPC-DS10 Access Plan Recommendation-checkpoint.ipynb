{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule Access Plan Recommendation\n",
    "\n",
    "This notebook is dedicated to model fitting in terms of database access plans. Access plans here are a representation for a particular query syntax, composed of underlying relational operators as to how data will be retrieved. This experiment deals with plan aggregation. Particularly, plans will be aggregated (summed) into a vector representation, which will then be gauged as an inlier or an outlier. Therefore the owrk contained within this experiment deals with flagging query access plans as eligable for further analysis - in our case, further analysis as to which optimizer statistics should be gathered.\n",
    "\n",
    "It should however be noted that this experiment poses a limitation. Access plans here are merely aggregated into a singular vector, representative of the underlying query cost. This aggregation does not indicate which sub-elements of the query are denoted as excessively expensive - this work is carried out in another experiment entirely.\n",
    "\n",
    "## Query to Access Plan Representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each query undergoes a set of processing steps when executed against a database instance. Apart from syntax and semantic checks which are carried out on the input syntax, a number of access plans are generated. Only one plan is chosen, based on the Cost Based Optimizer's decision in an effort to choose the least expensive one.\n",
    "\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\"><img src='Images/Query_translation.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"padding-bottom:0.5em;\" /><b>Query Translation Process</b></div>\n",
    "\n",
    "The query access plan is composed in a tree-like structure, where in each node of the tree acts as a row source. Each step of the access plan either retrieves rows from the database, or accepts rows from one or more row sources as input. The following example is extracted from - https://docs.oracle.com/database/121/TGSQL/tgsql_sqlproc.htm#TGSQL186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "SELECT e.last_name, j.job_title, d.department_name \n",
    "FROM   hr.employees e, hr.departments d, hr.jobs j\n",
    "WHERE  e.department_id = d.department_id\n",
    "AND    e.job_id = j.job_id\n",
    "AND    e.last_name LIKE 'A%';\n",
    "Execution Plan\n",
    "Plan hash value: 975837011\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "| Id| Operation                     | Name        |Rows|Bytes|Cost(%CPU)|Time  |\n",
    "| 0 | SELECT STATEMENT              |             |  3 | 189 | 7(15)| 00:00:01 |\n",
    "|*1 |  HASH JOIN                    |             |  3 | 189 | 7(15)| 00:00:01 |\n",
    "|*2 |   HASH JOIN                   |             |  3 | 141 | 5(20)| 00:00:01 |\n",
    "| 3 |    TABLE ACCESS BY INDEX ROWID| EMPLOYEES   |  3 |  60 | 2 (0)| 00:00:01 |\n",
    "|*4 |     INDEX RANGE SCAN          | EMP_NAME_IX |  3 |     | 1 (0)| 00:00:01 |\n",
    "| 5 |    TABLE ACCESS FULL          | JOBS        | 19 | 513 | 2 (0)| 00:00:01 |\n",
    "| 6 |   TABLE ACCESS FULL           | DEPARTMENTS | 27 | 432 | 2 (0)| 00:00:01 |\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Predicate Information (identified by operation id):\n",
    " \n",
    "   1 - access(\"E\".\"DEPARTMENT_ID\"=\"D\".\"DEPARTMENT_ID\")\n",
    "   2 - access(\"E\".\"JOB_ID\"=\"J\".\"JOB_ID\")\n",
    "   4 - access(\"E\".\"LAST_NAME\" LIKE 'A%')\n",
    "       filter(\"E\".\"LAST_NAME\" LIKE 'A%')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:80%; text-align:center;\"><img src='Images/Row_source_tree.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"padding-bottom:0.5em;\" /><b>Access Plan Row Source Tree Representation</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 0.23.4\n",
      "numpy: 1.15.2\n",
      "sklearn: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# sklearn\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "print('sklearn: %s' % sk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS10' # Schema upon which to operate test\n",
    "test_split=.2\n",
    "y_labels = ['COST',\n",
    "            'CARDINALITY',\n",
    "            'BYTES',\n",
    "            'CPU_COST',\n",
    "            'IO_COST',\n",
    "            'TEMP_SPACE',\n",
    "            'TIME']\n",
    "nrows = 1000000\n",
    "#\n",
    "# Random Forest Config\n",
    "parallel_degree = 4\n",
    "n_estimators = 500\n",
    "max_depth = 7\n",
    "criterion='gini'\n",
    "#\n",
    "# Isolation Forest Config\n",
    "contamination = .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (6,18,19,20,21,22,25,26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ('DBID',)    ('SQL_ID',)  ('PLAN_HASH_VALUE',)  ('ID',)    ('OPERATION',)  \\\n",
      "0  2634225673  9jrzzvpgammqz            1147771029        0  SELECT STATEMENT   \n",
      "1  2634225673  9jrzzvpgammqz            1147771029        1              SORT   \n",
      "2  2634225673  9jrzzvpgammqz            1147771029        2    PX COORDINATOR   \n",
      "3  2634225673  9jrzzvpgammqz            1147771029        3           PX SEND   \n",
      "4  2634225673  9jrzzvpgammqz            1147771029        4              SORT   \n",
      "\n",
      "  ('OPTIONS',) ('OBJECT_NODE',)  ('OBJECT#',) ('OBJECT_OWNER',)  \\\n",
      "0          NaN              NaN           NaN               NaN   \n",
      "1    AGGREGATE              NaN           NaN               NaN   \n",
      "2          NaN              NaN           NaN               NaN   \n",
      "3  QC (RANDOM)           :Q1000           NaN               SYS   \n",
      "4    AGGREGATE           :Q1000           NaN               NaN   \n",
      "\n",
      "  ('OBJECT_NAME',)     ...     ('ACCESS_PREDICATES',) ('FILTER_PREDICATES',)  \\\n",
      "0              NaN     ...                        NaN                    NaN   \n",
      "1              NaN     ...                        NaN                    NaN   \n",
      "2              NaN     ...                        NaN                    NaN   \n",
      "3         :TQ10000     ...                        NaN                    NaN   \n",
      "4              NaN     ...                        NaN                    NaN   \n",
      "\n",
      "  ('PROJECTION',)  ('TIME',)  ('QBLOCK_NAME',)  ('REMARKS',)  \\\n",
      "0             NaN        NaN               NaN           NaN   \n",
      "1             NaN        NaN             SEL$1           NaN   \n",
      "2             NaN        NaN               NaN           NaN   \n",
      "3             NaN        NaN               NaN           NaN   \n",
      "4             NaN        NaN               NaN           NaN   \n",
      "\n",
      "        ('TIMESTAMP',)                                     ('OTHER_XML',)  \\\n",
      "0  2018-11-30 17:31:11                                                NaN   \n",
      "1  2018-11-30 17:31:11  <other_xml><info type=\"derived_cpu_dop\" id=\"1\"...   \n",
      "2  2018-11-30 17:31:11                                                NaN   \n",
      "3  2018-11-30 17:31:11                                                NaN   \n",
      "4  2018-11-30 17:31:11                                                NaN   \n",
      "\n",
      "  ('CON_DBID',) ('CON_ID',)  \n",
      "0    2634225673           0  \n",
      "1    2634225673           0  \n",
      "2    2634225673           0  \n",
      "3    2634225673           0  \n",
      "4    2634225673           0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "------------------------------------------\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "rep_vsql_plan_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/rep_vsql_plan.csv'\n",
    "#rep_vsql_plan_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/rep_vsql_plan.csv'\n",
    "#\n",
    "dtype={'COST':'int64',\n",
    "       'CARDINALITY':'int64',\n",
    "       'BYTES':'int64',\n",
    "       'CPU_COST':'int64',\n",
    "       'IO_COST':'int64',\n",
    "       'TEMP_SPACE':'int64',\n",
    "       'TIME':'int64'}\n",
    "rep_vsql_plan_df = pd.read_csv(rep_vsql_plan_path,nrows=nrows,dtype=dtype)\n",
    "print(rep_vsql_plan_df.head())\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_vsql_plan_df.columns = prettify_header(rep_vsql_plan_df.columns.values)\n",
    "print('------------------------------------------')\n",
    "print(rep_vsql_plan_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read outlier data from file into pandas dataframes and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1467, 35)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12400  11/20/2018 09:11:22     NaN  SELECT STATEMENT              NaN   \n",
      "1   12400  11/20/2018 09:11:22     NaN             COUNT          STOPKEY   \n",
      "2   12400  11/20/2018 09:11:22     NaN              VIEW              NaN   \n",
      "3   12400  11/20/2018 09:11:22     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12400  11/20/2018 09:11:22     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN      TPCDS10         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN      TPCDS10         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE     ...      \\\n",
      "0             NaN     ...       \n",
      "1             NaN     ...       \n",
      "2              18     ...       \n",
      "3             NaN     ...       \n",
      "4              19     ...       \n",
      "\n",
      "                                           OTHER_XML DISTRIBUTION  \\\n",
      "0                                                NaN          NaN   \n",
      "1  <other_xml><info type=\"db_version\">12.1.0.2</i...          NaN   \n",
      "2                                                NaN          NaN   \n",
      "3                                                NaN          NaN   \n",
      "4                                                NaN          NaN   \n",
      "\n",
      "      CPU_COST IO_COST TEMP_SPACE ACCESS_PREDICATES FILTER_PREDICATES  \\\n",
      "0  14569318465  124674        NaN               NaN               NaN   \n",
      "1          NaN     NaN        NaN               NaN       ROWNUM<=100   \n",
      "2  14569318465  124674        NaN               NaN               NaN   \n",
      "3  14569318465  124674        NaN               NaN               NaN   \n",
      "4  14534050219  124674        NaN               NaN               NaN   \n",
      "\n",
      "                                          PROJECTION TIME QBLOCK_NAME  \n",
      "0                                                NaN    5         NaN  \n",
      "1  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN      SEL$11  \n",
      "2  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    5      SEL$12  \n",
      "3  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    5      SEL$12  \n",
      "4  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    5       SET$4  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "------------------------------------------\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'ID',\n",
      "       'PARENT_ID', 'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES',\n",
      "       'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID',\n",
      "       'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST',\n",
      "       'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION',\n",
      "       'TIME', 'QBLOCK_NAME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# CSV Outlier Paths\n",
    "outlier_hints_q5_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_5.csv'\n",
    "outlier_hints_q10_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_10.csv'\n",
    "outlier_hints_q14_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_14.csv'\n",
    "outlier_hints_q18_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_18.csv'\n",
    "outlier_hints_q22_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_22.csv'\n",
    "outlier_hints_q27_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_27.csv'\n",
    "outlier_hints_q35_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_35.csv'\n",
    "outlier_hints_q36_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_36.csv'\n",
    "outlier_hints_q51_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_51.csv'\n",
    "outlier_hints_q67_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_67.csv'\n",
    "outlier_hints_q70_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_70.csv'\n",
    "outlier_hints_q77_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_77.csv'\n",
    "outlier_hints_q80_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_80.csv'\n",
    "outlier_hints_q86_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/hints/output/query_86.csv'\n",
    "#\n",
    "outlier_predicates_q5_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_5.csv'\n",
    "outlier_predicates_q10_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_10.csv'\n",
    "outlier_predicates_q14_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_14.csv'\n",
    "outlier_predicates_q18_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_18.csv'\n",
    "outlier_predicates_q22_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_22.csv'\n",
    "outlier_predicates_q27_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_27.csv'\n",
    "outlier_predicates_q35_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_35.csv'\n",
    "outlier_predicates_q36_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_36.csv'\n",
    "outlier_predicates_q51_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_51.csv'\n",
    "outlier_predicates_q67_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_67.csv'\n",
    "outlier_predicates_q70_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_70.csv'\n",
    "outlier_predicates_q77_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_77.csv'\n",
    "outlier_predicates_q80_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_80.csv'\n",
    "outlier_predicates_q86_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/predicates/output/query_86.csv'\n",
    "#\n",
    "outlier_rownum_q5_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_5.csv'\n",
    "outlier_rownum_q10_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_10.csv'\n",
    "outlier_rownum_q14_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_14.csv'\n",
    "outlier_rownum_q18_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_18.csv'\n",
    "outlier_rownum_q22_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_22.csv'\n",
    "outlier_rownum_q27_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_27.csv'\n",
    "outlier_rownum_q35_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_35.csv'\n",
    "outlier_rownum_q36_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_36.csv'\n",
    "outlier_rownum_q51_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_51.csv'\n",
    "outlier_rownum_q67_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_67.csv'\n",
    "outlier_rownum_q70_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_70.csv'\n",
    "outlier_rownum_q77_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_77.csv'\n",
    "outlier_rownum_q80_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_80.csv'\n",
    "outlier_rownum_q86_path = 'C:/Users/gabriel.sammut/University/ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/rownum/output/query_86.csv'\n",
    "#\n",
    "# Read CSV Paths\n",
    "outlier_hints_q5_df = pd.read_csv(outlier_hints_q5_path,dtype=str)\n",
    "outlier_hints_q10_df = pd.read_csv(outlier_hints_q10_path,dtype=str)\n",
    "outlier_hints_q14_df = pd.read_csv(outlier_hints_q14_path,dtype=str)\n",
    "outlier_hints_q18_df = pd.read_csv(outlier_hints_q18_path,dtype=str)\n",
    "outlier_hints_q22_df = pd.read_csv(outlier_hints_q22_path,dtype=str)\n",
    "outlier_hints_q27_df = pd.read_csv(outlier_hints_q27_path,dtype=str)\n",
    "outlier_hints_q35_df = pd.read_csv(outlier_hints_q35_path,dtype=str)\n",
    "outlier_hints_q36_df = pd.read_csv(outlier_hints_q36_path,dtype=str)\n",
    "outlier_hints_q51_df = pd.read_csv(outlier_hints_q51_path,dtype=str)\n",
    "outlier_hints_q67_df = pd.read_csv(outlier_hints_q67_path,dtype=str)\n",
    "outlier_hints_q70_df = pd.read_csv(outlier_hints_q70_path,dtype=str)\n",
    "outlier_hints_q77_df = pd.read_csv(outlier_hints_q77_path,dtype=str)\n",
    "outlier_hints_q80_df = pd.read_csv(outlier_hints_q80_path,dtype=str)\n",
    "outlier_hints_q86_df = pd.read_csv(outlier_hints_q86_path,dtype=str)\n",
    "#\n",
    "outlier_predicates_q5_df = pd.read_csv(outlier_predicates_q5_path,dtype=str)\n",
    "outlier_predicates_q10_df = pd.read_csv(outlier_predicates_q10_path,dtype=str)\n",
    "outlier_predicates_q14_df = pd.read_csv(outlier_predicates_q14_path,dtype=str)\n",
    "outlier_predicates_q18_df = pd.read_csv(outlier_predicates_q18_path,dtype=str)\n",
    "outlier_predicates_q22_df = pd.read_csv(outlier_predicates_q22_path,dtype=str)\n",
    "outlier_predicates_q27_df = pd.read_csv(outlier_predicates_q27_path,dtype=str)\n",
    "outlier_predicates_q35_df = pd.read_csv(outlier_predicates_q35_path,dtype=str)\n",
    "outlier_predicates_q36_df = pd.read_csv(outlier_predicates_q36_path,dtype=str)\n",
    "outlier_predicates_q51_df = pd.read_csv(outlier_predicates_q51_path,dtype=str)\n",
    "outlier_predicates_q67_df = pd.read_csv(outlier_predicates_q67_path,dtype=str)\n",
    "outlier_predicates_q70_df = pd.read_csv(outlier_predicates_q70_path,dtype=str)\n",
    "outlier_predicates_q77_df = pd.read_csv(outlier_predicates_q77_path,dtype=str)\n",
    "outlier_predicates_q80_df = pd.read_csv(outlier_predicates_q80_path,dtype=str)\n",
    "outlier_predicates_q86_df = pd.read_csv(outlier_predicates_q86_path,dtype=str)\n",
    "#\n",
    "outlier_rownum_q5_df = pd.read_csv(outlier_rownum_q5_path,dtype=str)\n",
    "outlier_rownum_q10_df = pd.read_csv(outlier_rownum_q10_path,dtype=str)\n",
    "outlier_rownum_q14_df = pd.read_csv(outlier_rownum_q14_path,dtype=str)\n",
    "outlier_rownum_q18_df = pd.read_csv(outlier_rownum_q18_path,dtype=str)\n",
    "outlier_rownum_q22_df = pd.read_csv(outlier_rownum_q22_path,dtype=str)\n",
    "outlier_rownum_q27_df = pd.read_csv(outlier_rownum_q27_path,dtype=str)\n",
    "outlier_rownum_q35_df = pd.read_csv(outlier_rownum_q35_path,dtype=str)\n",
    "outlier_rownum_q36_df = pd.read_csv(outlier_rownum_q36_path,dtype=str)\n",
    "outlier_rownum_q51_df = pd.read_csv(outlier_rownum_q51_path,dtype=str)\n",
    "outlier_rownum_q67_df = pd.read_csv(outlier_rownum_q67_path,dtype=str)\n",
    "outlier_rownum_q70_df = pd.read_csv(outlier_rownum_q70_path,dtype=str)\n",
    "outlier_rownum_q77_df = pd.read_csv(outlier_rownum_q77_path,dtype=str)\n",
    "outlier_rownum_q80_df = pd.read_csv(outlier_rownum_q80_path,dtype=str)\n",
    "outlier_rownum_q86_df = pd.read_csv(outlier_rownum_q86_path,dtype=str)\n",
    "#\n",
    "# Merge dataframes into a single pandas matrix\n",
    "df_outliers = pd.concat([outlier_hints_q5_df, outlier_hints_q10_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q14_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q18_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q22_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q27_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q35_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q36_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q51_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q67_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q70_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q77_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q80_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q86_df], sort=False)\n",
    "#\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q5_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q10_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q14_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q18_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q22_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q27_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q35_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q36_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q51_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q67_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q70_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q77_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q80_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q86_df], sort=False)\n",
    "#\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q5_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q10_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q14_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q18_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q22_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q27_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q35_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q36_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q51_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q67_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q70_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q77_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q80_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q86_df], sort=False)   \n",
    "#\n",
    "print(df_outliers.shape)\n",
    "print(df_outliers.head())\n",
    "print('------------------------------------------')\n",
    "print(df_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A Columns\n",
      "\n",
      "\n",
      "REP_VSQL_PLAN Features 39: ['OPTIONS', 'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME', 'REMARKS', 'OTHER_XML']\n",
      "\n",
      "\n",
      "DF_OUTLIERS Features 35: ['REMARKS', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "#\n",
    "print('N/A Columns\\n')\n",
    "print('\\nREP_VSQL_PLAN Features ' + str(len(rep_vsql_plan_df.columns)) + ': ' + str(get_na_columns(df=rep_vsql_plan_df,headers=rep_vsql_plan_df.columns)) + \"\\n\")\n",
    "print('\\nDF_OUTLIERS Features ' + str(len(df_outliers.columns)) + ': ' + str(get_na_columns(df=df_outliers,headers=df_outliers.columns)) + \"\\n\")\n",
    "#\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "#\n",
    "# Populating NaN values with amount '0'\n",
    "df = fill_na(df=rep_vsql_plan_df)\n",
    "df_outliers = fill_na(df=df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape before changes: [(1000000, 39)]\n",
      "Shape after changes: [(1000000, 31)]\n",
      "Dropped a total [8]\n",
      "\n",
      "Shape before changes: [(1467, 35)]\n",
      "Shape after changes: [(1467, 27)]\n",
      "Dropped a total [8]\n",
      "\n",
      "After flatline column drop:\n",
      "(1000000, 31)\n",
      "Index(['SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'DISTRIBUTION',\n",
      "       'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME', 'QBLOCK_NAME', 'TIMESTAMP',\n",
      "       'OTHER_XML'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier flatline column drop:\n",
      "(1467, 27)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'OPERATION', 'OPTIONS', 'OBJECT_OWNER',\n",
      "       'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE',\n",
      "       'OPTIMIZER', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'COST', 'CARDINALITY', 'BYTES', 'OTHER_XML', 'CPU_COST', 'IO_COST',\n",
      "       'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION',\n",
      "       'TIME', 'QBLOCK_NAME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "df = drop_flatline_columns(df=df)\n",
    "df_outliers = drop_flatline_columns(df=df_outliers)\n",
    "#\n",
    "print('\\nAfter flatline column drop:')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier flatline column drop:')\n",
    "print(df_outliers.shape)\n",
    "print(df_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Converting labels/features into numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels:\n",
      "['OPERATION', 'OPTIONS', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'QBLOCK_NAME']\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "          SQL_ID  PLAN_HASH_VALUE  ID  OPERATION  OPTIONS OBJECT_NODE  \\\n",
      "0  9jrzzvpgammqz       1147771029   0         31        0           0   \n",
      "1  9jrzzvpgammqz       1147771029   1         33        1           0   \n",
      "2  9jrzzvpgammqz       1147771029   2         27        0           0   \n",
      "3  9jrzzvpgammqz       1147771029   3         29       31      :Q1000   \n",
      "4  9jrzzvpgammqz       1147771029   4         33        1      :Q1000   \n",
      "\n",
      "   OBJECT#  OBJECT_OWNER  OBJECT_NAME  OBJECT_ALIAS  \\\n",
      "0      0.0             0            0             0   \n",
      "1      0.0             0            0             0   \n",
      "2      0.0             0            0             0   \n",
      "3      0.0             1          213             0   \n",
      "4      0.0             0            0             0   \n",
      "\n",
      "                         ...                          PARTITION_STOP  \\\n",
      "0                        ...                                       0   \n",
      "1                        ...                                       0   \n",
      "2                        ...                                       0   \n",
      "3                        ...                                       0   \n",
      "4                        ...                                       0   \n",
      "\n",
      "   PARTITION_ID  DISTRIBUTION  CPU_COST  IO_COST  TEMP_SPACE  TIME  \\\n",
      "0           0.0             0                0.0               0.0   \n",
      "1           0.0             0                0.0               0.0   \n",
      "2           0.0             0                0.0               0.0   \n",
      "3           0.0   QC (RANDOM)                0.0               0.0   \n",
      "4           0.0             0                0.0               0.0   \n",
      "\n",
      "  QBLOCK_NAME            TIMESTAMP  \\\n",
      "0           0  2018-11-30 17:31:11   \n",
      "1          14  2018-11-30 17:31:11   \n",
      "2           0  2018-11-30 17:31:11   \n",
      "3           0  2018-11-30 17:31:11   \n",
      "4           0  2018-11-30 17:31:11   \n",
      "\n",
      "                                           OTHER_XML  \n",
      "0                                                  0  \n",
      "1  <other_xml><info type=\"derived_cpu_dop\" id=\"1\"...  \n",
      "2                                                  0  \n",
      "3                                                  0  \n",
      "4                                                  0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Encoded labels:\n",
      "['OPERATION', 'OPTIONS', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'QBLOCK_NAME']\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "  PLAN_ID            TIMESTAMP  OPERATION  OPTIONS  OBJECT_OWNER  OBJECT_NAME  \\\n",
      "0   12400  11/20/2018 09:11:22         10        0             0            0   \n",
      "1   12400  11/20/2018 09:11:22          1       21             0            0   \n",
      "2   12400  11/20/2018 09:11:22         16        0             2            0   \n",
      "3   12400  11/20/2018 09:11:22         11       10             0            0   \n",
      "4   12400  11/20/2018 09:11:22         16        0             2            0   \n",
      "\n",
      "   OBJECT_ALIAS OBJECT_INSTANCE  OBJECT_TYPE  OPTIMIZER     ...       BYTES  \\\n",
      "0             0               0            0          1     ...        6400   \n",
      "1             0               0            0          0     ...           0   \n",
      "2           114              18            0          0     ...      363200   \n",
      "3             0               0            0          0     ...      363200   \n",
      "4           104              19            0          0     ...      363200   \n",
      "\n",
      "                                           OTHER_XML     CPU_COST IO_COST  \\\n",
      "0                                                  0  14569318465  124674   \n",
      "1  <other_xml><info type=\"db_version\">12.1.0.2</i...            0       0   \n",
      "2                                                  0  14569318465  124674   \n",
      "3                                                  0  14569318465  124674   \n",
      "4                                                  0  14534050219  124674   \n",
      "\n",
      "  TEMP_SPACE ACCESS_PREDICATES FILTER_PREDICATES  \\\n",
      "0          0                 0                 0   \n",
      "1          0                 0       ROWNUM<=100   \n",
      "2          0                 0                 0   \n",
      "3          0                 0                 0   \n",
      "4          0                 0                 0   \n",
      "\n",
      "                                          PROJECTION TIME QBLOCK_NAME  \n",
      "0                                                  0    5           0  \n",
      "1  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    0           4  \n",
      "2  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    5           5  \n",
      "3  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    5           5  \n",
      "4  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    5          47  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "def encode(df, encoded_labels):\n",
    "    for col in df.columns:\n",
    "        if col in encoded_labels:\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "#\n",
    "# Determine labels used for encoding\n",
    "encoded_labels = ['OPERATION','OPTIONS','OBJECT_OWNER','OBJECT_NAME','OBJECT_ALIAS','OBJECT_TYPE','OPTIMIZER','QBLOCK_NAME']\n",
    "#\n",
    "df = encode(df=df, encoded_labels=encoded_labels)\n",
    "print('Encoded labels:\\n' + str(encoded_labels) + \"\\n\\n----------------------------------------------\\n\\n\")\n",
    "print(df.head())\n",
    "#\n",
    "df_outliers = encode(df=df_outliers, encoded_labels=encoded_labels)\n",
    "print('Encoded labels:\\n' + str(encoded_labels) + \"\\n\\n----------------------------------------------\\n\\n\")\n",
    "print(df_outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer conversion\n",
    "\n",
    "Each column is converted into a column of type values which are integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inliers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f43656208969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Inliers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_labels\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[0;32m   4999\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5000\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[1;32m-> 5001\u001b[1;33m                                          **kwargs)\n\u001b[0m\u001b[0;32m   5002\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m   3712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3713\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3714\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'astype'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3716\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3581\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[1;32m--> 575\u001b[1;33m                             **kwargs)\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_astype\u001b[1;34m(self, dtype, copy, errors, values, klass, mgr, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                 \u001b[1;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[1;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\util.pxd\u001b[0m in \u001b[0;36mutil.set_value_at_unsafe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "print('Inliers')\n",
    "df[y_labels] = df[y_labels].astype('int64')\n",
    "print(type(df))\n",
    "print(df.shape)\n",
    "#\n",
    "print('Outliers')\n",
    "df_outliers[y_labels] = df_outliers[y_labels].astype('int64')\n",
    "print(type(df_outliers))\n",
    "print(df_outliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Plan Resource Aggregation (Per Access Plan Type)\n",
    "\n",
    "This method attempts to tackle the problem of access plan anomolies by aggregating resources per explain plan. Notable resources which are being considered are as follows:\n",
    "\n",
    "* COST\n",
    "* CARDINALITY\n",
    "* BYTES\n",
    "* PARTITION_DELTA (Partition End - Partition Start)\n",
    "* CPU_COST\n",
    "* IO_COST\n",
    "* TEMP_SPACE\n",
    "* TIME\n",
    "\n",
    "The reasoning behind these fields in particular is mainly because these columns can be aggregated together. Aggregation is carried on per access plan type. Aggregation at this phase ensures that each SQL ID is represented as a mean vector, which represents the PLAN_HASH_VALUE as a vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before')\n",
    "print(df.shape)\n",
    "df_agg = df.groupby(['SQL_ID','PLAN_HASH_VALUE']).mean()\n",
    "df_agg.reset_index(inplace=True)\n",
    "print(df_agg.columns)\n",
    "print(df_agg.shape)\n",
    "#\n",
    "print('-------------------\\nAfter')\n",
    "print(df_outliers.shape)\n",
    "df_outliers_agg = df_outliers.groupby(['PLAN_ID']).mean()\n",
    "df_outliers_agg.reset_index(inplace=True)\n",
    "print(df_outliers_agg.columns)\n",
    "print(df_outliers_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Reduction\n",
    "\n",
    "Strips further columns unneccessary to the experiment, so as to have the same columns for both training data set and outlier set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_outliers_agg.columns:\n",
    "    if col not in df_agg.columns:\n",
    "        df_outliers_agg.drop(columns=[col], inplace=True)\n",
    "for col in df_agg.columns:\n",
    "    if col not in df_outliers_agg.columns:\n",
    "        df_agg.drop(columns=[col], inplace=True)\n",
    "#\n",
    "print(df_agg.columns)\n",
    "print(df_agg.shape)\n",
    "print('------------------------------------------')\n",
    "print(df_outliers_agg.columns)\n",
    "print(df_outliers_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training (Random Forest - Per Access Plan Type)\n",
    "\n",
    "The following section oversees the supervised training of inlier/outlier explain plans. This section first splits the training dataset into two: train + test sets, by mixing half the outliers with the inlier training set. Validation is then performed on a 50/50 mix of inlier / outlier vectors. The ability to classify explain plan vectors as inliers / outliers will be gauged.\n",
    "\n",
    "Labels are denoted as follows:\n",
    "* Inliers  - 0\n",
    "* Outliers - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random Forest\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest Class (Regression + Classification)\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, n_estimators, max_depth=None, criterion='gini', parallel_degree=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.parallel_degree=parallel_degree\n",
    "        self.criterion = criterion\n",
    "        self.model = RandomForestClassifier(max_depth=self.max_depth,\n",
    "                                            n_estimators=self.n_estimators,\n",
    "                                            criterion=self.criterion,\n",
    "                                            n_jobs=self.parallel_degree)\n",
    "    #\n",
    "    def fit_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits training data to target labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X,y)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, plot=False):\n",
    "        \"\"\"\n",
    "        Runs test data through previously trained model, and evaluate differently depending if a regression of classification model\n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        acc = accuracy_score(y, yhat, normalize=True)\n",
    "        precision = precision_score(y, yhat, average='binary')\n",
    "        recall = recall_score(y, yhat, average='binary')\n",
    "        f1 = f1_score(y, yhat, average='binary')\n",
    "        print('Test Accuracy: ' +  str(acc))\n",
    "        print('Test Precision: ' +  str(precision))\n",
    "        print('Test Recall: ' +  str(recall))\n",
    "        print('Test FScore: ' +  str(f1) + \"\\n\")\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, estimator, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'estimator', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'estimator': estimator,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(0)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "rfc = RandomForest(n_estimators=n_estimators,\n",
    "                   max_depth=max_depth,\n",
    "                   criterion=criterion,\n",
    "                   parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "rfc.fit_model(X=X_df_train,\n",
    "              y=y_df_train)\n",
    "# Evaluation\n",
    "rfc.predict_and_evaluate(X=X_df_validate, \n",
    "                         y=y_df_validate,\n",
    "                         plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning (Isolation Forest - Per Access Plan Type)\n",
    "\n",
    "The following section attempts to train a generalized version of input access plans using Isolation Forests. These trained models are then used to classify access plan outliers from inlier (normal) access plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationForestWrapper:\n",
    "    #\n",
    "    def __init__(self, contamination=.1, parallel_degree=1):\n",
    "        \"\"\"\n",
    "        Constructor Method\n",
    "        :param X - Numpy Array\n",
    "        :param contamination - Real value\n",
    "        :param parallel_degree - Parellization parameter \n",
    "        \"\"\"\n",
    "        self.model = IsolationForest(n_estimators=100, max_samples=256, contamination=contamination, random_state=0, n_jobs=parallel_degree)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def fit_model(self, X):\n",
    "        \"\"\"\n",
    "        Fits Isolation Model and plots scorings\n",
    "        \"\"\"\n",
    "        self.model.fit(X)        \n",
    "    #\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X) \n",
    "    #\n",
    "    def evaluate_model(self, X, y, plot=False):\n",
    "        yhat = self.predict(X)\n",
    "        acc = accuracy_score(y, yhat, normalize=True)\n",
    "        precision = precision_score(y, yhat, average='binary')\n",
    "        recall = recall_score(y, yhat, average='binary')\n",
    "        f1 = f1_score(y, yhat, average='binary')\n",
    "        print('Test Accuracy: ' +  str(acc))\n",
    "        print('Test Precision: ' +  str(precision))\n",
    "        print('Test Recall: ' +  str(recall))\n",
    "        print('Test FScore: ' +  str(f1) + \"\\n\")\n",
    "        if plot is True:\n",
    "            scores = self.model.decision_function(X)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.hist(scores, bins=50);\n",
    "            plt.title('Isolation Forest Scorings')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(1)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(-1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "ifw = IsolationForestWrapper(contamination=contamination,\n",
    "                             parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "ifw.fit_model(X=X_df_train)\n",
    "#\n",
    "# Evaluation\n",
    "ifw.evaluate_model(X=X_df_validate, \n",
    "                   y=y_df_validate,\n",
    "                   plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Plan Resource Aggregation (Per Access Plan Instance)\n",
    "\n",
    "This method attempts to tackle the problem of access plan anomolies by aggregating resources per explain plan. Notable resources which are being considered are as follows:\n",
    "\n",
    "* COST\n",
    "* CARDINALITY\n",
    "* BYTES\n",
    "* PARTITION_DELTA (Partition End - Partition Start)\n",
    "* CPU_COST\n",
    "* IO_COST\n",
    "* TEMP_SPACE\n",
    "* TIME\n",
    "\n",
    "The reasoning behind these fields in particular is mainly because these columns can be aggregated together. Aggregation is carried on per access plan instance. Therefore batches are summed together with every explain plan, per timestamp. Contraty to the previous resource aggregation, whereas before plan aggregation occurred per SQL_ID class, now aggregation is carried out with every SQL_ID occurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Adds a columns per SQL_ID, PLAN_HASH_VALUE grouping, which can be used to group instances together\n",
    "def add_grouping_column(df, column_identifier):\n",
    "    \"\"\"\n",
    "    Receives a pandas dataframe, and adds a new column which allows dataframe to be aggregated per \n",
    "    SQL_ID, PLAN_HASH_VALUE combination.\n",
    "    \n",
    "    :param: df                - Pandas Dataframe\n",
    "    :param: column_identifier - String denoting matrix column to group by\n",
    "    \n",
    "    :return: Pandas Dataframe, with added column    \n",
    "    \"\"\"\n",
    "    print('Shape before transformation: ' + str(df.shape))\n",
    "    new_grouping_col = []\n",
    "    counter = 0\n",
    "    last_sql_id = df[column_identifier].iloc(0) # Starts with first SQL_ID\n",
    "    for index, row in df.iterrows():\n",
    "        if column_identifier == 'SQL_ID':\n",
    "            if last_sql_id != row.SQL_ID:\n",
    "                last_sql_id = row.SQL_ID\n",
    "                counter += 1\n",
    "        elif column_identifier == 'PLAN_ID':\n",
    "            if last_sql_id != row.PLAN_ID:\n",
    "                last_sql_id = row.PLAN_ID\n",
    "                counter += 1\n",
    "        else:\n",
    "            raise ValueError('Column does not exist!')\n",
    "        new_grouping_col.append(counter)\n",
    "    #\n",
    "    # Append list as new column\n",
    "    new_col = pd.Series(new_grouping_col)\n",
    "    df['PLAN_INSTANCE'] = new_col.values\n",
    "    print('Shape after transformation: ' + str(df.shape))\n",
    "    return df\n",
    "#\n",
    "df = add_grouping_column(df=df,column_identifier='SQL_ID')\n",
    "df_outliers = add_grouping_column(df=df_outliers,column_identifier='PLAN_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.groupby(['SQL_ID','PLAN_HASH_VALUE','PLAN_INSTANCE']).mean()\n",
    "df_agg.reset_index(inplace=True)\n",
    "#\n",
    "df_outliers_agg = df_outliers.groupby(['PLAN_ID','PLAN_INSTANCE']).mean()\n",
    "df_outliers_agg.reset_index(inplace=True)\n",
    "#\n",
    "for col in df_outliers_agg.columns:\n",
    "    if col not in df_agg.columns:\n",
    "        df_outliers_agg.drop(columns=[col], inplace=True)\n",
    "for col in df_agg.columns:\n",
    "    if col not in df_outliers_agg.columns:\n",
    "        df_agg.drop(columns=[col], inplace=True)\n",
    "#\n",
    "df_agg.drop(columns=['PLAN_INSTANCE'], inplace=True)\n",
    "df_outliers_agg.drop(columns=['PLAN_INSTANCE'], inplace=True)\n",
    "#\n",
    "print(df_agg.columns)\n",
    "print(df_agg.shape)\n",
    "print('------------------------------------------')\n",
    "print(df_outliers_agg.columns)\n",
    "print(df_outliers_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training (Random Forest - Per Access Plan Instance)\n",
    "\n",
    "The following section oversees the supervised training of inlier/outlier explain plans. This section first splits the training dataset into two: train + test sets, by mixing half the outliers with the inlier training set. Validation is then performed on a 50/50 mix of inlier / outlier vectors. The ability to classify explain plan vectors as inliers / outliers will be gauged.\n",
    "\n",
    "Labels are denoted as follows:\n",
    "* Inliers  - 0\n",
    "* Outliers - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(0)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "rfc = RandomForest(n_estimators=n_estimators,\n",
    "                   max_depth=max_depth,\n",
    "                   criterion=criterion,\n",
    "                   parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "rfc.fit_model(X=X_df_train,\n",
    "              y=y_df_train)\n",
    "# Evaluation\n",
    "rfc.predict_and_evaluate(X=X_df_validate, \n",
    "                         y=y_df_validate,\n",
    "                         plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning (Isolation Forest - Per Access Plan Instance)\n",
    "\n",
    "The following section attempts to train a generalized version of input access plans using Isolation Forests. These trained models are then used to classify access plan outliers from inlier (normal) access plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(1)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(-1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "ifw = IsolationForestWrapper(contamination=contamination,\n",
    "                             parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "ifw.fit_model(X=X_df_train)\n",
    "#\n",
    "# Evaluation\n",
    "ifw.evaluate_model(X=X_df_validate, \n",
    "                   y=y_df_validate,\n",
    "                   plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
