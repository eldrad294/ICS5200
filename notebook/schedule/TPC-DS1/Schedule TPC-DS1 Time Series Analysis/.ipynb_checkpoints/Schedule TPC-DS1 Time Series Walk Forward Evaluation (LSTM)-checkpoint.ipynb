{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Foward Evaluation\n",
    "\n",
    "This section mimics a real-life like approach, where a model is trained on a week worth of data, and then used to predict 7 days worth of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Installation and Importing Libraries\n",
    "\n",
    "* https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/\n",
    "* https://vertexai-plaidml.readthedocs-hosted.com/en/latest/installing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.4\n",
      "pandas: 0.23.4\n",
      "statsmodels: 0.9.0\n",
      "sklearn: 0.20.2\n",
      "theano: 1.0.3\n",
      "tensorflow: 1.11.0\n",
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas.plotting import lag_plot\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# statsmodels\n",
    "import statsmodels\n",
    "print('statsmodels: %s' % statsmodels.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# theano\n",
    "import theano\n",
    "print('theano: %s' % theano.__version__)\n",
    "# tensorflow\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "# plaidml keras\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "# keras\n",
    "import keras as ke\n",
    "print('keras: %s' % ke.__version__)\n",
    "# math\n",
    "import math\n",
    "import csv\n",
    "import os.path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=13 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "nrows=10000000\n",
    "test_split=.5 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "sub_sample_start=350 # Denotes frist 0..n samples (Used for plotting purposes)\n",
    "y_label = ['CPU_TIME_DELTA', 'DISK_READS_DELTA']# Denotes which label to use for time series experiments\n",
    "\n",
    "# Feature Selection\n",
    "parallel_degree = -1\n",
    "n_estimators=100\n",
    "\n",
    "# LSTM Network Structure\n",
    "epochs=150\n",
    "batch=64\n",
    "activation='tanh'\n",
    "initializer='normal'\n",
    "dropout=0\n",
    "layer=2\n",
    "state=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n",
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'BEGIN_TIME' 'END_TIME' 'INTSIZE'\n",
      " 'GROUP_ID' 'METRIC_ID' 'METRIC_NAME' 'METRIC_UNIT' 'NUM_INTERVAL'\n",
      " 'MINVAL' 'MAXVAL' 'AVERAGE' 'STANDARD_DEVIATION' 'SUM_SQUARES' 'CON_DBID'\n",
      " 'CON_ID' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME' 'END_INTERVAL_TIME'\n",
      " 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT' 'SNAP_FLAG' 'SNAP_TIMEZONE']\n",
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'STAT_ID' 'STAT_NAME' 'VALUE'\n",
      " 'CON_DBID' 'CON_ID' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    }
   ],
   "source": [
    "# Root path\n",
    "#root_dir = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds\n",
    "root_dir = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds\n",
    "\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = root_dir + '/rep_hist_snapshot.csv'\n",
    "rep_hist_sysmetric_summary_path = root_dir + '/rep_hist_sysmetric_summary.csv'\n",
    "rep_hist_sysstat_path = root_dir + '/rep_hist_sysstat.csv'\n",
    "\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path, nrows=nrows)\n",
    "rep_hist_sysmetric_summary_df = pd.read_csv(rep_hist_sysmetric_summary_path, nrows=nrows)\n",
    "rep_hist_sysstat_df = pd.read_csv(rep_hist_sysstat_path, nrows=nrows)\n",
    "\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "rep_hist_sysmetric_summary_df.columns = prettify_header(rep_hist_sysmetric_summary_df.columns.values)\n",
    "rep_hist_sysstat_df.columns = prettify_header(rep_hist_sysstat_df.columns.values)\n",
    "\n",
    "print(rep_hist_snapshot_df.columns.values)\n",
    "print(rep_hist_sysmetric_summary_df.columns.values)\n",
    "print(rep_hist_sysstat_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting Tables and Changing Matrix Shapes\n",
    "\n",
    "Changes all dataframe shapes to be similar to each other, where in a number of snap_id timestamps are cojoined with instance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header Lengths [Before Pivot]\n",
      "REP_HIST_SNAPSHOT: 90\n",
      "REP_HIST_SYSMETRIC_SUMMARY: 26\n",
      "REP_HIST_SYSSTAT: 16\n",
      "\n",
      "Header Lengths [After Pivot]\n",
      "REP_HIST_SNAPSHOT: 77\n",
      "REP_HIST_SYSMETRIC_SUMMARY: 163\n",
      "REP_HIST_SYSSTAT: 1180\n",
      "\n",
      "Dataframe shapes:\n",
      "Table [REP_HIST_SNAPSHOT] - (19831, 77)\n",
      "Table [REP_HIST_SYSMETRIC_SUMMARY] - (19831, 163)\n",
      "Table [REP_HIST_SYSSTAT] - (8489, 1180)\n",
      "\n",
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'PLAN_HASH_VALUE' 'OPTIMIZER_COST'\n",
      " 'OPTIMIZER_ENV_HASH_VALUE' 'SHARABLE_MEM' 'LOADED_VERSIONS'\n",
      " 'VERSION_COUNT' 'SQL_PROFILE' 'PARSING_SCHEMA_ID' 'PARSING_USER_ID'\n",
      " 'FETCHES_TOTAL' 'FETCHES_DELTA' 'END_OF_FETCH_COUNT_TOTAL'\n",
      " 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL' 'SORTS_DELTA' 'EXECUTIONS_TOTAL'\n",
      " 'EXECUTIONS_DELTA' 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA'\n",
      " 'LOADS_TOTAL' 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'FLAG' 'CON_DBID' 'CON_ID' 'COMMAND_TYPE'\n",
      " 'SNAP_LEVEL' 'ERROR_COUNT' 'SNAP_FLAG']\n",
      "['INDEX' 'SNAP_ID' 'ACTIVE PARALLEL SESSIONS' 'ACTIVE SERIAL SESSIONS'\n",
      " 'AVERAGE ACTIVE SESSIONS' 'AVERAGE SYNCHRONOUS SINGLE-BLOCK READ LATENCY'\n",
      " 'BACKGROUND CPU USAGE PER SEC' 'BACKGROUND CHECKPOINTS PER SEC'\n",
      " 'BACKGROUND TIME PER SEC' 'BRANCH NODE SPLITS PER SEC'\n",
      " 'BRANCH NODE SPLITS PER TXN' 'BUFFER CACHE HIT RATIO' 'CPU USAGE PER SEC'\n",
      " 'CPU USAGE PER TXN' 'CR BLOCKS CREATED PER SEC'\n",
      " 'CR BLOCKS CREATED PER TXN' 'CR UNDO RECORDS APPLIED PER SEC'\n",
      " 'CR UNDO RECORDS APPLIED PER TXN' 'CAPTURED USER CALLS'\n",
      " 'CELL PHYSICAL IO INTERCONNECT BYTES' 'CONSISTENT READ CHANGES PER SEC'\n",
      " 'CONSISTENT READ CHANGES PER TXN' 'CONSISTENT READ GETS PER SEC'\n",
      " 'CONSISTENT READ GETS PER TXN' 'CURRENT LOGONS COUNT' 'CURRENT OS LOAD'\n",
      " 'CURRENT OPEN CURSORS COUNT' 'CURSOR CACHE HIT RATIO'\n",
      " 'DB BLOCK CHANGES PER SEC' 'DB BLOCK CHANGES PER TXN'\n",
      " 'DB BLOCK CHANGES PER USER CALL' 'DB BLOCK GETS PER SEC'\n",
      " 'DB BLOCK GETS PER TXN' 'DB BLOCK GETS PER USER CALL'\n",
      " 'DBWR CHECKPOINTS PER SEC' 'DDL STATEMENTS PARALLELIZED PER SEC'\n",
      " 'DML STATEMENTS PARALLELIZED PER SEC' 'DATABASE CPU TIME RATIO'\n",
      " 'DATABASE TIME PER SEC' 'DATABASE WAIT TIME RATIO' 'DISK SORT PER SEC'\n",
      " 'DISK SORT PER TXN' 'ENQUEUE DEADLOCKS PER SEC'\n",
      " 'ENQUEUE DEADLOCKS PER TXN' 'ENQUEUE REQUESTS PER SEC'\n",
      " 'ENQUEUE REQUESTS PER TXN' 'ENQUEUE TIMEOUTS PER SEC'\n",
      " 'ENQUEUE TIMEOUTS PER TXN' 'ENQUEUE WAITS PER SEC'\n",
      " 'ENQUEUE WAITS PER TXN' 'EXECUTE WITHOUT PARSE RATIO'\n",
      " 'EXECUTIONS PER SEC' 'EXECUTIONS PER TXN' 'EXECUTIONS PER USER CALL'\n",
      " 'FULL INDEX SCANS PER SEC' 'FULL INDEX SCANS PER TXN'\n",
      " 'GC CR BLOCK RECEIVED PER SECOND' 'GC CR BLOCK RECEIVED PER TXN'\n",
      " 'GC CURRENT BLOCK RECEIVED PER SECOND'\n",
      " 'GC CURRENT BLOCK RECEIVED PER TXN' 'GLOBAL CACHE AVERAGE CR GET TIME'\n",
      " 'GLOBAL CACHE AVERAGE CURRENT GET TIME' 'GLOBAL CACHE BLOCKS CORRUPTED'\n",
      " 'GLOBAL CACHE BLOCKS LOST' 'HARD PARSE COUNT PER SEC'\n",
      " 'HARD PARSE COUNT PER TXN' 'HOST CPU USAGE PER SEC'\n",
      " 'HOST CPU UTILIZATION (%)' 'I/O MEGABYTES PER SECOND'\n",
      " 'I/O REQUESTS PER SECOND' 'LEAF NODE SPLITS PER SEC'\n",
      " 'LEAF NODE SPLITS PER TXN' 'LIBRARY CACHE HIT RATIO'\n",
      " 'LIBRARY CACHE MISS RATIO' 'LOGICAL READS PER SEC'\n",
      " 'LOGICAL READS PER TXN' 'LOGICAL READS PER USER CALL' 'LOGONS PER SEC'\n",
      " 'LOGONS PER TXN' 'LONG TABLE SCANS PER SEC' 'LONG TABLE SCANS PER TXN'\n",
      " 'MEMORY SORTS RATIO' 'NETWORK TRAFFIC VOLUME PER SEC'\n",
      " 'OPEN CURSORS PER SEC' 'OPEN CURSORS PER TXN' 'PGA CACHE HIT %'\n",
      " 'PQ QC SESSION COUNT' 'PQ SLAVE SESSION COUNT'\n",
      " 'PX DOWNGRADED 1 TO 25% PER SEC' 'PX DOWNGRADED 25 TO 50% PER SEC'\n",
      " 'PX DOWNGRADED 50 TO 75% PER SEC' 'PX DOWNGRADED 75 TO 99% PER SEC'\n",
      " 'PX DOWNGRADED TO SERIAL PER SEC' 'PX OPERATIONS NOT DOWNGRADED PER SEC'\n",
      " 'PARSE FAILURE COUNT PER SEC' 'PARSE FAILURE COUNT PER TXN'\n",
      " 'PHYSICAL READ BYTES PER SEC' 'PHYSICAL READ IO REQUESTS PER SEC'\n",
      " 'PHYSICAL READ TOTAL BYTES PER SEC'\n",
      " 'PHYSICAL READ TOTAL IO REQUESTS PER SEC'\n",
      " 'PHYSICAL READS DIRECT LOBS PER SEC' 'PHYSICAL READS DIRECT LOBS PER TXN'\n",
      " 'PHYSICAL READS DIRECT PER SEC' 'PHYSICAL READS DIRECT PER TXN'\n",
      " 'PHYSICAL READS PER SEC' 'PHYSICAL READS PER TXN'\n",
      " 'PHYSICAL WRITE BYTES PER SEC' 'PHYSICAL WRITE IO REQUESTS PER SEC'\n",
      " 'PHYSICAL WRITE TOTAL BYTES PER SEC'\n",
      " 'PHYSICAL WRITE TOTAL IO REQUESTS PER SEC'\n",
      " 'PHYSICAL WRITES DIRECT LOBS  PER TXN'\n",
      " 'PHYSICAL WRITES DIRECT LOBS PER SEC' 'PHYSICAL WRITES DIRECT PER SEC'\n",
      " 'PHYSICAL WRITES DIRECT PER TXN' 'PHYSICAL WRITES PER SEC'\n",
      " 'PHYSICAL WRITES PER TXN' 'PROCESS LIMIT %'\n",
      " 'QUERIES PARALLELIZED PER SEC' 'RECURSIVE CALLS PER SEC'\n",
      " 'RECURSIVE CALLS PER TXN' 'REDO ALLOCATION HIT RATIO'\n",
      " 'REDO GENERATED PER SEC' 'REDO GENERATED PER TXN' 'REDO WRITES PER SEC'\n",
      " 'REDO WRITES PER TXN' 'REPLAYED USER CALLS' 'RESPONSE TIME PER TXN'\n",
      " 'ROW CACHE HIT RATIO' 'ROW CACHE MISS RATIO' 'ROWS PER SORT'\n",
      " 'RUN QUEUE PER SEC' 'SQL SERVICE RESPONSE TIME' 'SESSION COUNT'\n",
      " 'SESSION LIMIT %' 'SHARED POOL FREE %' 'SOFT PARSE RATIO'\n",
      " 'STREAMS POOL USAGE PERCENTAGE' 'TEMP SPACE USED'\n",
      " 'TOTAL INDEX SCANS PER SEC' 'TOTAL INDEX SCANS PER TXN'\n",
      " 'TOTAL PGA ALLOCATED' 'TOTAL PGA USED BY SQL WORKAREAS'\n",
      " 'TOTAL PARSE COUNT PER SEC' 'TOTAL PARSE COUNT PER TXN'\n",
      " 'TOTAL SORTS PER USER CALL' 'TOTAL TABLE SCANS PER SEC'\n",
      " 'TOTAL TABLE SCANS PER TXN' 'TOTAL TABLE SCANS PER USER CALL'\n",
      " 'TXNS PER LOGON' 'USER CALLS PER SEC' 'USER CALLS PER TXN'\n",
      " 'USER CALLS RATIO' 'USER COMMITS PER SEC' 'USER COMMITS PERCENTAGE'\n",
      " 'USER LIMIT %' 'USER ROLLBACK UNDO RECORDS APPLIED PER TXN'\n",
      " 'USER ROLLBACK UNDOREC APPLIED PER SEC' 'USER ROLLBACKS PER SEC'\n",
      " 'USER ROLLBACKS PERCENTAGE' 'USER TRANSACTION PER SEC'\n",
      " 'VM IN BYTES PER SEC' 'VM OUT BYTES PER SEC'\n",
      " 'WORKLOAD CAPTURE AND REPLAY STATUS']\n",
      "['INDEX' 'SNAP_ID' 'ADG GLOBAL FLUSH' ...\n",
      " 'WRITE CLONES CREATED FOR RECOVERY' 'WRITE CLONES CREATED IN BACKGROUND'\n",
      " 'WRITE CLONES CREATED IN FOREGROUND']\n",
      "(19831, 77)\n",
      "(19831, 163)\n",
      "(8489, 1180)\n"
     ]
    }
   ],
   "source": [
    "print('Header Lengths [Before Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "\n",
    "# Table REP_HIST_SYSMETRIC_SUMMARY\n",
    "rep_hist_sysmetric_summary_df = rep_hist_sysmetric_summary_df.pivot(index='SNAP_ID', columns='METRIC_NAME', values='AVERAGE')\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "rep_hist_sysmetric_summary_df[['SNAP_ID']] = rep_hist_sysmetric_summary_df[['SNAP_ID']].astype(int)\n",
    "#rep_hist_sysmetric_summary_df = rep_hist_sysstat_df.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "rep_hist_sysmetric_summary_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "\n",
    "# Table REP_HIST_SYSSTAT\n",
    "rep_hist_sysstat_df = rep_hist_sysstat_df.pivot(index='SNAP_ID', columns='STAT_NAME', values='VALUE')\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "rep_hist_sysstat_df[['SNAP_ID']] = rep_hist_sysstat_df[['SNAP_ID']].astype(int)\n",
    "#rep_hist_sysstat_df = rep_hist_sysstat_df.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "rep_hist_sysstat_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "\n",
    "rep_hist_sysmetric_summary_df.rename(str.upper, inplace=True, axis='columns')\n",
    "rep_hist_sysstat_df.rename(str.upper, inplace=True, axis='columns')\n",
    "\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT)\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID','DBID','INSTANCE_NUMBER']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "\n",
    "print('\\nHeader Lengths [After Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "\n",
    "# DF Shape\n",
    "print('\\nDataframe shapes:\\nTable [REP_HIST_SNAPSHOT] - ' + str(rep_hist_snapshot_df.shape))\n",
    "print('Table [REP_HIST_SYSMETRIC_SUMMARY] - ' + str(rep_hist_sysmetric_summary_df.shape))\n",
    "print('Table [REP_HIST_SYSSTAT] - ' + str(rep_hist_sysstat_df.shape) + '\\n')\n",
    "\n",
    "print(rep_hist_snapshot_df.columns.values)\n",
    "print(rep_hist_sysmetric_summary_df.columns.values)\n",
    "print(rep_hist_sysstat_df.columns.values)\n",
    "print(rep_hist_snapshot_df.shape)\n",
    "print(rep_hist_sysmetric_summary_df.shape)\n",
    "print(rep_hist_sysstat_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Empty Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A Columns\n",
      "\n",
      "\n",
      " REP_HIST_SNAPSHOT Features 77: []\n",
      "\n",
      "REP_HIST_SYSMETRIC_SUMMARY Features 163: ['ACTIVE PARALLEL SESSIONS', 'ACTIVE SERIAL SESSIONS', 'AVERAGE ACTIVE SESSIONS', 'AVERAGE SYNCHRONOUS SINGLE-BLOCK READ LATENCY', 'BACKGROUND CPU USAGE PER SEC', 'BACKGROUND TIME PER SEC', 'CAPTURED USER CALLS', 'CELL PHYSICAL IO INTERCONNECT BYTES', 'HOST CPU USAGE PER SEC', 'I/O MEGABYTES PER SECOND', 'I/O REQUESTS PER SECOND', 'PX OPERATIONS NOT DOWNGRADED PER SEC', 'REPLAYED USER CALLS', 'RUN QUEUE PER SEC', 'SESSION COUNT', 'TEMP SPACE USED', 'TOTAL PGA ALLOCATED', 'TOTAL PGA USED BY SQL WORKAREAS', 'VM IN BYTES PER SEC', 'VM OUT BYTES PER SEC', 'WORKLOAD CAPTURE AND REPLAY STATUS']\n",
      "\n",
      "REP_HIST_SYSSTAT Features 1180: ['BA SPARE STATISTIC 12', 'BA SPARE STATISTIC 9', 'CLI FLSTASK CREATE', 'CLUSTERWIDE GLOBAL TRANSACTIONS', 'DDL STATEMENTS PARALLELIZED', 'EHCC COMPRESSED LENGTH COMPRESSED', 'EFFECTIVE IO TIME', 'FORWARDED 2PC COMMANDS ACROSS RAC NODES', 'IM POPULATE (FASTSTART) CUS READ ATTEMPTS', 'IM POPULATE CUS MEMCOMPRESS FOR CAPACITY LOW', 'IM REPOPULATE (TRICKLE) CUS MEMCOMPRESS FOR QUERY LOW', 'IM REPOPULATE (TRICKLE) CUS REQUESTED', 'IM REPOPULATE CUS NO MEMCOMPRESS', 'IM SCAN ROWS', 'IM SCAN ROWS JOURNAL', 'IM SCAN ROWS PROJECTED', 'IM SPACE SMU BYTES ALLOCATED', 'IM SPACE SMU CREATIONS COMMITTED', 'IM SPACE PRIVATE JOURNAL EXTENTS ALLOCATED', 'IPC CPU USED BY THIS SESSION', 'PARALLEL OPERATIONS DOWNGRADED TO SERIAL', 'WORKLOAD REPLAY: THINK TIME', 'BACKUP DATA COMPRESSED REMOTELY', 'CALLS TO KCMGRS', 'CELL CUS SENT UNCOMPRESSED', 'CONSISTENT GETS', 'DATA WAREHOUSING SCANNED BLOCKS', 'FLASH CACHE INSERT SKIP: MODIFICATION', 'GC IM SHRINKS', 'GC CLEANOUT SAVED', 'KA LOCAL MESSAGE WAITS', 'KA MESSAGES SENT', 'LOB WRITES UNALIGNED', 'PHYSICAL READ SNAP IO REQUESTS BASE', 'PHYSICAL READS CACHE', 'REDO SYNCH TIME', 'REDO WRITE SIZE COUNT (   4KB)', 'REDO WRITE SIZE COUNT (INF)', 'SECUREFILE NUMBER OF NON-TRANSFORMED FLUSHES', 'SESSION PGA MEMORY', 'TRACKED TRANSACTIONS', 'WRITE CLONES CREATED IN FOREGROUND']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "\n",
    "print('N/A Columns\\n')\n",
    "print('\\n REP_HIST_SNAPSHOT Features ' + str(len(rep_hist_snapshot_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_snapshot_df,headers=rep_hist_snapshot_df.columns)) + \"\\n\")\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY Features ' + str(len(rep_hist_sysmetric_summary_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_sysmetric_summary_df,headers=rep_hist_sysmetric_summary_df.columns)) + \"\\n\")\n",
    "print('REP_HIST_SYSSTAT Features ' + str(len(rep_hist_sysstat_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_sysstat_df,headers=rep_hist_sysstat_df.columns)) + \"\\n\")\n",
    "\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Populating NaN values with amount '0'\n",
    "rep_hist_snapshot_df = fill_na(df=rep_hist_snapshot_df)\n",
    "rep_hist_sysmetric_summary_df = fill_na(df=rep_hist_sysmetric_summary_df)\n",
    "rep_hist_sysstat_df = fill_na(df=rep_hist_sysstat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Frames\n",
    "\n",
    "This part merges the following pandas data frame into a single frame:\n",
    "* REP_HIST_SNAPSHOT\n",
    "* REP_HIST_SYSMETRIC_SUMMARY\n",
    "* REP_HIST_SYSSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8489, 1418)\n",
      "----------------------------------\n",
      "['SNAP_ID', 'DBID', 'INSTANCE_NUMBER', 'PLAN_HASH_VALUE', 'OPTIMIZER_COST', 'OPTIMIZER_ENV_HASH_VALUE', 'SHARABLE_MEM', 'LOADED_VERSIONS', 'VERSION_COUNT', 'SQL_PROFILE', 'PARSING_SCHEMA_ID', 'PARSING_USER_ID', 'FETCHES_TOTAL', 'FETCHES_DELTA', 'END_OF_FETCH_COUNT_TOTAL', 'END_OF_FETCH_COUNT_DELTA', 'SORTS_TOTAL', 'SORTS_DELTA', 'EXECUTIONS_TOTAL', 'EXECUTIONS_DELTA', 'PX_SERVERS_EXECS_TOTAL', 'PX_SERVERS_EXECS_DELTA', 'LOADS_TOTAL', 'LOADS_DELTA', 'INVALIDATIONS_TOTAL', 'INVALIDATIONS_DELTA', 'PARSE_CALLS_TOTAL', 'PARSE_CALLS_DELTA', 'DISK_READS_TOTAL', 'DISK_READS_DELTA', 'BUFFER_GETS_TOTAL', 'BUFFER_GETS_DELTA', 'ROWS_PROCESSED_TOTAL', 'ROWS_PROCESSED_DELTA', 'CPU_TIME_TOTAL', 'CPU_TIME_DELTA', 'ELAPSED_TIME_TOTAL', 'ELAPSED_TIME_DELTA', 'IOWAIT_TOTAL', 'IOWAIT_DELTA', 'CLWAIT_TOTAL', 'CLWAIT_DELTA', 'APWAIT_TOTAL', 'APWAIT_DELTA', 'CCWAIT_TOTAL', 'CCWAIT_DELTA', 'DIRECT_WRITES_TOTAL', 'DIRECT_WRITES_DELTA', 'PLSEXEC_TIME_TOTAL', 'PLSEXEC_TIME_DELTA', 'JAVEXEC_TIME_TOTAL', 'JAVEXEC_TIME_DELTA', 'IO_OFFLOAD_ELIG_BYTES_TOTAL', 'IO_OFFLOAD_ELIG_BYTES_DELTA', 'IO_INTERCONNECT_BYTES_TOTAL', 'IO_INTERCONNECT_BYTES_DELTA', 'PHYSICAL_READ_REQUESTS_TOTAL', 'PHYSICAL_READ_REQUESTS_DELTA', 'PHYSICAL_READ_BYTES_TOTAL', 'PHYSICAL_READ_BYTES_DELTA', 'PHYSICAL_WRITE_REQUESTS_TOTAL', 'PHYSICAL_WRITE_REQUESTS_DELTA', 'PHYSICAL_WRITE_BYTES_TOTAL', 'PHYSICAL_WRITE_BYTES_DELTA', 'OPTIMIZED_PHYSICAL_READS_TOTAL', 'OPTIMIZED_PHYSICAL_READS_DELTA', 'CELL_UNCOMPRESSED_BYTES_TOTAL', 'CELL_UNCOMPRESSED_BYTES_DELTA', 'IO_OFFLOAD_RETURN_BYTES_TOTAL', 'IO_OFFLOAD_RETURN_BYTES_DELTA', 'FLAG', 'CON_DBID', 'CON_ID', 'COMMAND_TYPE', 'SNAP_LEVEL', 'ERROR_COUNT', 'SNAP_FLAG', 'INDEX_x', 'ACTIVE PARALLEL SESSIONS', 'ACTIVE SERIAL SESSIONS', 'AVERAGE ACTIVE SESSIONS', 'AVERAGE SYNCHRONOUS SINGLE-BLOCK READ LATENCY', 'BACKGROUND CPU USAGE PER SEC', 'BACKGROUND CHECKPOINTS PER SEC', 'BACKGROUND TIME PER SEC', 'BRANCH NODE SPLITS PER SEC', 'BRANCH NODE SPLITS PER TXN', 'BUFFER CACHE HIT RATIO', 'CPU USAGE PER SEC', 'CPU USAGE PER TXN', 'CR BLOCKS CREATED PER SEC', 'CR BLOCKS CREATED PER TXN', 'CR UNDO RECORDS APPLIED PER SEC', 'CR UNDO RECORDS APPLIED PER TXN', 'CAPTURED USER CALLS', 'CELL PHYSICAL IO INTERCONNECT BYTES_x', 'CONSISTENT READ CHANGES PER SEC', 'CONSISTENT READ CHANGES PER TXN', 'CONSISTENT READ GETS PER SEC', 'CONSISTENT READ GETS PER TXN', 'CURRENT LOGONS COUNT', 'CURRENT OS LOAD', 'CURRENT OPEN CURSORS COUNT', 'CURSOR CACHE HIT RATIO', 'DB BLOCK CHANGES PER SEC', 'DB BLOCK CHANGES PER TXN', 'DB BLOCK CHANGES PER USER CALL', 'DB BLOCK GETS PER SEC', 'DB BLOCK GETS PER TXN', 'DB BLOCK GETS PER USER CALL', 'DBWR CHECKPOINTS PER SEC', 'DDL STATEMENTS PARALLELIZED PER SEC', 'DML STATEMENTS PARALLELIZED PER SEC', 'DATABASE CPU TIME RATIO', 'DATABASE TIME PER SEC', 'DATABASE WAIT TIME RATIO', 'DISK SORT PER SEC', 'DISK SORT PER TXN', 'ENQUEUE DEADLOCKS PER SEC', 'ENQUEUE DEADLOCKS PER TXN', 'ENQUEUE REQUESTS PER SEC', 'ENQUEUE REQUESTS PER TXN', 'ENQUEUE TIMEOUTS PER SEC', 'ENQUEUE TIMEOUTS PER TXN', 'ENQUEUE WAITS PER SEC', 'ENQUEUE WAITS PER TXN', 'EXECUTE WITHOUT PARSE RATIO', 'EXECUTIONS PER SEC', 'EXECUTIONS PER TXN', 'EXECUTIONS PER USER CALL', 'FULL INDEX SCANS PER SEC', 'FULL INDEX SCANS PER TXN', 'GC CR BLOCK RECEIVED PER SECOND', 'GC CR BLOCK RECEIVED PER TXN', 'GC CURRENT BLOCK RECEIVED PER SECOND', 'GC CURRENT BLOCK RECEIVED PER TXN', 'GLOBAL CACHE AVERAGE CR GET TIME', 'GLOBAL CACHE AVERAGE CURRENT GET TIME', 'GLOBAL CACHE BLOCKS CORRUPTED', 'GLOBAL CACHE BLOCKS LOST', 'HARD PARSE COUNT PER SEC', 'HARD PARSE COUNT PER TXN', 'HOST CPU USAGE PER SEC', 'HOST CPU UTILIZATION (%)', 'I/O MEGABYTES PER SECOND', 'I/O REQUESTS PER SECOND', 'LEAF NODE SPLITS PER SEC', 'LEAF NODE SPLITS PER TXN', 'LIBRARY CACHE HIT RATIO', 'LIBRARY CACHE MISS RATIO', 'LOGICAL READS PER SEC', 'LOGICAL READS PER TXN', 'LOGICAL READS PER USER CALL', 'LOGONS PER SEC', 'LOGONS PER TXN', 'LONG TABLE SCANS PER SEC', 'LONG TABLE SCANS PER TXN', 'MEMORY SORTS RATIO', 'NETWORK TRAFFIC VOLUME PER SEC', 'OPEN CURSORS PER SEC', 'OPEN CURSORS PER TXN', 'PGA CACHE HIT %', 'PQ QC SESSION COUNT', 'PQ SLAVE SESSION COUNT', 'PX DOWNGRADED 1 TO 25% PER SEC', 'PX DOWNGRADED 25 TO 50% PER SEC', 'PX DOWNGRADED 50 TO 75% PER SEC', 'PX DOWNGRADED 75 TO 99% PER SEC', 'PX DOWNGRADED TO SERIAL PER SEC', 'PX OPERATIONS NOT DOWNGRADED PER SEC', 'PARSE FAILURE COUNT PER SEC', 'PARSE FAILURE COUNT PER TXN', 'PHYSICAL READ BYTES PER SEC', 'PHYSICAL READ IO REQUESTS PER SEC', 'PHYSICAL READ TOTAL BYTES PER SEC', 'PHYSICAL READ TOTAL IO REQUESTS PER SEC', 'PHYSICAL READS DIRECT LOBS PER SEC', 'PHYSICAL READS DIRECT LOBS PER TXN', 'PHYSICAL READS DIRECT PER SEC', 'PHYSICAL READS DIRECT PER TXN', 'PHYSICAL READS PER SEC', 'PHYSICAL READS PER TXN', 'PHYSICAL WRITE BYTES PER SEC', 'PHYSICAL WRITE IO REQUESTS PER SEC', 'PHYSICAL WRITE TOTAL BYTES PER SEC', 'PHYSICAL WRITE TOTAL IO REQUESTS PER SEC', 'PHYSICAL WRITES DIRECT LOBS  PER TXN', 'PHYSICAL WRITES DIRECT LOBS PER SEC', 'PHYSICAL WRITES DIRECT PER SEC', 'PHYSICAL WRITES DIRECT PER TXN', 'PHYSICAL WRITES PER SEC', 'PHYSICAL WRITES PER TXN', 'PROCESS LIMIT %', 'QUERIES PARALLELIZED PER SEC', 'RECURSIVE CALLS PER SEC', 'RECURSIVE CALLS PER TXN', 'REDO ALLOCATION HIT RATIO', 'REDO GENERATED PER SEC', 'REDO GENERATED PER TXN', 'REDO WRITES PER SEC', 'REDO WRITES PER TXN', 'REPLAYED USER CALLS', 'RESPONSE TIME PER TXN', 'ROW CACHE HIT RATIO', 'ROW CACHE MISS RATIO', 'ROWS PER SORT', 'RUN QUEUE PER SEC', 'SQL SERVICE RESPONSE TIME', 'SESSION COUNT', 'SESSION LIMIT %', 'SHARED POOL FREE %', 'SOFT PARSE RATIO', 'STREAMS POOL USAGE PERCENTAGE', 'TEMP SPACE USED', 'TOTAL INDEX SCANS PER SEC', 'TOTAL INDEX SCANS PER TXN', 'TOTAL PGA ALLOCATED', 'TOTAL PGA USED BY SQL WORKAREAS', 'TOTAL PARSE COUNT PER SEC', 'TOTAL PARSE COUNT PER TXN', 'TOTAL SORTS PER USER CALL', 'TOTAL TABLE SCANS PER SEC', 'TOTAL TABLE SCANS PER TXN', 'TOTAL TABLE SCANS PER USER CALL', 'TXNS PER LOGON', 'USER CALLS PER SEC', 'USER CALLS PER TXN', 'USER CALLS RATIO', 'USER COMMITS PER SEC', 'USER COMMITS PERCENTAGE', 'USER LIMIT %', 'USER ROLLBACK UNDO RECORDS APPLIED PER TXN', 'USER ROLLBACK UNDOREC APPLIED PER SEC', 'USER ROLLBACKS PER SEC', 'USER ROLLBACKS PERCENTAGE', 'USER TRANSACTION PER SEC', 'VM IN BYTES PER SEC', 'VM OUT BYTES PER SEC', 'WORKLOAD CAPTURE AND REPLAY STATUS', 'INDEX_y', 'ADG GLOBAL FLUSH', 'ADG PARSELOCK X GET ATTEMPTS', 'ADG PARSELOCK X GET SUCCESSES', 'BA AU BYTES ALLOCATED', 'BA BYTES FOR FILE MAPS', 'BA BYTES READ FROM DISK', 'BA BYTES READ FROM FLASH', 'BA COUNT - BORROWED FROM OTHER NODE', 'BA COUNT - CACHE DENIED', 'BA COUNT - CACHE MISS', 'BA COUNT - SEARCHED IN PB', 'BA COUNT - SHORT CIRCUIT IN IDN', 'BA COUNT - TOTAL ALLOCATION REQUESTS', 'BA COUNT - UNABLE TO SHORT CIRCUIT IN IDN', 'BA COUNT WHEN 10% OF BUCKETS IN PB', 'BA COUNT WHEN 25% OF BUCKETS IN PB', 'BA COUNT WHEN 50% OF BUCKETS IN PB', 'BA COUNT WHEN 75% OF BUCKETS IN PB', 'BA COUNT WHEN 90% OF BUCKETS IN PB', 'BA FILE BYTES ALLOCATED', 'BA FILE BYTES DELETED', 'BA FILES CREATED COUNT', 'BA FILES DELETED COUNT', 'BA FLASH BYTES REQUESTED', 'BA NON-FLASH BYTES REQUESTED', 'BA SPARE STATISTIC 1', 'BA SPARE STATISTIC 10', 'BA SPARE STATISTIC 11', 'BA SPARE STATISTIC 12', 'BA SPARE STATISTIC 2', 'BA SPARE STATISTIC 3', 'BA SPARE STATISTIC 4', 'BA SPARE STATISTIC 5', 'BA SPARE STATISTIC 6', 'BA SPARE STATISTIC 7', 'BA SPARE STATISTIC 8', 'BA SPARE STATISTIC 9', 'BATCHED IO (BOUND) VECTOR COUNT', 'BATCHED IO (FULL) VECTOR COUNT', 'BATCHED IO (SPACE) VECTOR COUNT', 'BATCHED IO BLOCK MISS COUNT', 'BATCHED IO BUFFER DEFRAG COUNT', 'BATCHED IO DOUBLE MISS COUNT', 'BATCHED IO SAME UNIT COUNT', 'BATCHED IO SINGLE BLOCK COUNT', 'BATCHED IO SLOW JUMP COUNT', 'BATCHED IO VECTOR BLOCK COUNT', 'BATCHED IO VECTOR READ COUNT', 'BATCHED IO ZERO BLOCK COUNT', 'BLOCK CLEANOUT OPTIM REFERENCED', 'CCURSOR + SQL AREA EVICTED', 'CLI BG ENQ', 'CLI BG FLS DONE', 'CLI BG ATTEMPT FLUSH', 'CLI BUF WRT', 'CLI CLIENT FLUSH', 'CLI FLSTASK CREATE', 'CLI FLUSH', 'CLI IMM WRT', 'CLI PRVTZ LOB ', 'CLI SGA ALLOC', 'CLI THRU WRT', 'CLI BYTES FLS TO EXT', 'CLI BYTES FLS TO TABLE', 'CPU USED BY THIS SESSION', 'CPU USED WHEN CALL STARTED', 'CR BLOCKS CREATED', 'CACHED COMMIT SCN REFERENCED', 'CLUSTERWIDE GLOBAL TRANSACTIONS', 'CLUSTERWIDE GLOBAL TRANSACTIONS SPANNING RAC NODES', 'COMMIT SCN CACHED', 'DB TIME', 'DBWR CHECKPOINT BUFFERS WRITTEN', 'DBWR CHECKPOINTS', 'DBWR FUSION WRITES', 'DBWR LRU SCANS', 'DBWR OBJECT DROP BUFFERS WRITTEN', 'DBWR PARALLEL QUERY CHECKPOINT BUFFERS WRITTEN', 'DBWR REVISITED BEING-WRITTEN BUFFER', 'DBWR TABLESPACE CHECKPOINT BUFFERS WRITTEN', 'DBWR THREAD CHECKPOINT BUFFERS WRITTEN', 'DBWR TRANSACTION TABLE WRITES', 'DBWR UNDO BLOCK WRITES', 'DDL STATEMENTS PARALLELIZED', 'DFO TREES PARALLELIZED', 'DML STATEMENTS PARALLELIZED', 'DX/BB ENQUEUE LOCK BACKGROUND GET TIME', 'DX/BB ENQUEUE LOCK BACKGROUND GETS', 'DX/BB ENQUEUE LOCK FOREGROUND REQUESTS', 'DX/BB ENQUEUE LOCK FOREGROUND WAIT TIME', 'EHCC ANALYZE CUS DECOMPRESSED', 'EHCC ANALYZER CALLS', 'EHCC ARCHIVE CUS COMPRESSED', 'EHCC ARCHIVE CUS DECOMPRESSED', 'EHCC ATTEMPTED BLOCK COMPRESSIONS', 'EHCC BLOCK COMPRESSIONS', 'EHCC CU ROW PIECES COMPRESSED', 'EHCC CUS COMPRESSED', 'EHCC CUS DECOMPRESSED', 'EHCC CUS ALL ROWS PASS MINMAX', 'EHCC CUS NO ROWS PASS MINMAX', 'EHCC CUS SOME ROWS PASS MINMAX', 'EHCC CHECK CUS DECOMPRESSED', 'EHCC COLUMNS DECOMPRESSED', 'EHCC COMPRESSED LENGTH COMPRESSED', 'EHCC COMPRESSED LENGTH DECOMPRESSED', 'EHCC CONVENTIONAL DMLS', 'EHCC DML CUS DECOMPRESSED', 'EHCC DECOMPRESSED LENGTH COMPRESSED', 'EHCC DECOMPRESSED LENGTH DECOMPRESSED', 'EHCC DUMP CUS DECOMPRESSED', 'EHCC NORMAL SCAN CUS DECOMPRESSED', 'EHCC PIECES BUFFERED FOR DECOMPRESSION', 'EHCC PREDS ALL ROWS PASS MINMAX', 'EHCC PREDS NO ROWS PASS MINMAX', 'EHCC PREDS SOME ROWS PASS MINMAX', 'EHCC QUERY HIGH CUS COMPRESSED', 'EHCC QUERY HIGH CUS DECOMPRESSED', 'EHCC QUERY LOW CUS COMPRESSED', 'EHCC QUERY LOW CUS DECOMPRESSED', 'EHCC ROWID CUS DECOMPRESSED', 'EHCC ROWS COMPRESSED', 'EHCC ROWS NOT COMPRESSED', 'EHCC TOTAL COLUMNS FOR DECOMPRESSION', 'EHCC TOTAL PIECES FOR DECOMPRESSION', 'EHCC TOTAL ROWS FOR DECOMPRESSION', 'EHCC TURBO SCAN CUS DECOMPRESSED', 'EHCC USED ON PILLAR TABLESPACE', 'EHCC USED ON ZFS TABLESPACE', 'EFFECTIVE IO TIME', 'FORWARDED 2PC COMMANDS ACROSS RAC NODES', 'GTX PROCESSES SPAWNED BY AUTOTUNE', 'GTX PROCESSES STOPPED BY AUTOTUNE', 'HSC COMPRESSED SEGMENT BLOCK CHANGES', 'HSC HEAP SEGMENT BLOCK CHANGES', 'HSC IDL COMPRESSED BLOCKS', 'HSC OLTP COMPRESSED BLOCKS', 'HSC OLTP COMPRESSION SKIPPED ROWS', 'HSC OLTP COMPRESSION WIDE COMPRESSED ROW PIECES', 'HSC OLTP DROP COLUMN', 'HSC OLTP NON COMPRESSIBLE BLOCKS', 'HSC OLTP SPACE SAVING', 'HSC OLTP COMPRESSION BLOCK CHECKED', 'HSC OLTP INLINE COMPRESSION', 'HSC OLTP NEGATIVE COMPRESSION', 'HSC OLTP POSITIVE COMPRESSION', 'HSC OLTP RECURSIVE COMPRESSION', 'HEAP SEGMENT ARRAY INSERTS', 'HEAP SEGMENT ARRAY UPDATES', 'HEATMAP BLKLEVEL FLUSH TASK CREATE', 'HEATMAP BLKLEVEL FLUSHED', 'HEATMAP BLKLEVEL FLUSHED TO BF', 'HEATMAP BLKLEVEL FLUSHED TO SYSAUX', 'HEATMAP BLKLEVEL NOT TRACKED - MEMORY', 'HEATMAP BLKLEVEL NOT UPDATED - REPEAT', 'HEATMAP BLKLEVEL RANGES FLUSHED', 'HEATMAP BLKLEVEL RANGES SKIPPED', 'HEATMAP BLKLEVEL TRACKED', 'HEATMAP BLKLEVEL FLUSH TASK COUNT', 'HEATMAP SEGLEVEL - FLUSH', 'HEATMAP SEGLEVEL - FULL TABLE SCAN', 'HEATMAP SEGLEVEL - INDEXLOOKUP', 'HEATMAP SEGLEVEL - SEGMENTS FLUSHED', 'HEATMAP SEGLEVEL - TABLELOOKUP', 'HEATMAP SEGLEVEL - WRITE', 'IM FETCHES BY ROWID FROM IMCU', 'IM FETCHES BY ROWID FROM DISK', 'IM FETCHES BY ROWID FROM FETCH LIST', 'IM FETCHES BY ROWID FROM JOURNAL', 'IM FETCHES BY ROWID ROW INVALID IN IMCU', 'IM POPULATE (FASTSTART) CUS ACCUMULATED WRITE TIME (MS)', 'IM POPULATE (FASTSTART) CUS BYTES READ', 'IM POPULATE (FASTSTART) CUS BYTES WRITTEN', 'IM POPULATE (FASTSTART) CUS READ', 'IM POPULATE (FASTSTART) CUS READ ATTEMPTS', 'IM POPULATE (FASTSTART) CUS VERIFIED', 'IM POPULATE (FASTSTART) CUS WALL CLOCK WRITE TIME (MS)', 'IM POPULATE (FASTSTART) CUS WRITE REQUESTS', 'IM POPULATE (FASTSTART) CUS WRITES', 'IM POPULATE (FASTSTART) ACCUMULATED TIME (MS)', 'IM POPULATE (FASTSTART) NUMBER OF INCOMPATIBLE SAVEPOINTS', 'IM POPULATE (FASTSTART) NUMBER OF INVALID SAVEPOINTS', 'IM POPULATE (FASTSTART) NUMBER OF SAVEPOINTS UPDATED', 'IM POPULATE CUS', 'IM POPULATE CUS CHAIN PIECES', 'IM POPULATE CUS COLUMNS', 'IM POPULATE CUS EMPTY', 'IM POPULATE CUS MEMCOMPRESS FOR CAPACITY HIGH', 'IM POPULATE CUS MEMCOMPRESS FOR CAPACITY LOW', 'IM POPULATE CUS MEMCOMPRESS FOR DML', 'IM POPULATE CUS MEMCOMPRESS FOR QUERY HIGH', 'IM POPULATE CUS MEMCOMPRESS FOR QUERY LOW', 'IM POPULATE CUS NO MEMCOMPRESS', 'IM POPULATE CUS REQUESTED', 'IM POPULATE CUS RESUBMITTED', 'IM POPULATE ACCUMULATED TIME (MS)', 'IM POPULATE BLOCKS INVALID', 'IM POPULATE BYTES FROM STORAGE', 'IM POPULATE BYTES IN-MEMORY DATA', 'IM POPULATE BYTES UNCOMPRESSED DATA', 'IM POPULATE ROWS', 'IM POPULATE SEGMENTS', 'IM POPULATE SEGMENTS REQUESTED', 'IM POPULATE TRANSACTIONS ACTIVE', 'IM POPULATE TRANSACTIONS CHECK', 'IM POPULATE UNDO RECORDS APPLIED', 'IM POPULATE UNDO SEGHEADER ROLLBACK', 'IM PREPOPULATE CUS', 'IM PREPOPULATE CUS CHAIN PIECES', 'IM PREPOPULATE CUS COLUMNS', 'IM PREPOPULATE CUS EMPTY', 'IM PREPOPULATE CUS MEMCOMPRESS FOR CAPACITY HIGH', 'IM PREPOPULATE CUS MEMCOMPRESS FOR CAPACITY LOW', 'IM PREPOPULATE CUS MEMCOMPRESS FOR DML', 'IM PREPOPULATE CUS MEMCOMPRESS FOR QUERY HIGH', 'IM PREPOPULATE CUS MEMCOMPRESS FOR QUERY LOW', 'IM PREPOPULATE CUS NO MEMCOMPRESS', 'IM PREPOPULATE CUS REQUESTED', 'IM PREPOPULATE CUS RESUBMITTED', 'IM PREPOPULATE ACCUMULATED TIME (MS)', 'IM PREPOPULATE BYTES FROM STORAGE', 'IM PREPOPULATE BYTES IN-MEMORY DATA', 'IM PREPOPULATE BYTES UNCOMPRESSED DATA', 'IM PREPOPULATE ROWS', 'IM PREPOPULATE SEGMENTS', 'IM PREPOPULATE SEGMENTS REQUESTED', 'IM RAC CUS INVALID', 'IM RAC BLOCKS INVALID', 'IM REPOPULATE (SCAN) CUS', 'IM REPOPULATE (SCAN) CUS REQUESTED', 'IM REPOPULATE (SCAN) CUS RESUBMITTED', 'IM REPOPULATE (TRICKLE) CUS', 'IM REPOPULATE (TRICKLE) CUS CHAIN PIECES', 'IM REPOPULATE (TRICKLE) CUS COLUMNS', 'IM REPOPULATE (TRICKLE) CUS EMPTY', 'IM REPOPULATE (TRICKLE) CUS MEMCOMPRESS FOR CAPACITY HIGH', 'IM REPOPULATE (TRICKLE) CUS MEMCOMPRESS FOR CAPACITY LOW', 'IM REPOPULATE (TRICKLE) CUS MEMCOMPRESS FOR DML', 'IM REPOPULATE (TRICKLE) CUS MEMCOMPRESS FOR QUERY HIGH', 'IM REPOPULATE (TRICKLE) CUS MEMCOMPRESS FOR QUERY LOW', 'IM REPOPULATE (TRICKLE) CUS NO MEMCOMPRESS', 'IM REPOPULATE (TRICKLE) CUS REQUESTED', 'IM REPOPULATE (TRICKLE) CUS RESUBMITTED', 'IM REPOPULATE (TRICKLE) ACCUMULATED TIME (MS)', 'IM REPOPULATE (TRICKLE) BYTES FROM STORAGE', 'IM REPOPULATE (TRICKLE) BYTES IN-MEMORY DATA', 'IM REPOPULATE (TRICKLE) BYTES UNCOMPRESSED DATA', 'IM REPOPULATE (TRICKLE) ROWS', 'IM REPOPULATE CUS', 'IM REPOPULATE CUS CHAIN PIECES', 'IM REPOPULATE CUS COLUMNS', 'IM REPOPULATE CUS EMPTY', 'IM REPOPULATE CUS MEMCOMPRESS FOR CAPACITY HIGH', 'IM REPOPULATE CUS MEMCOMPRESS FOR CAPACITY LOW', 'IM REPOPULATE CUS MEMCOMPRESS FOR DML', 'IM REPOPULATE CUS MEMCOMPRESS FOR QUERY HIGH', 'IM REPOPULATE CUS MEMCOMPRESS FOR QUERY LOW', 'IM REPOPULATE CUS NO MEMCOMPRESS', 'IM REPOPULATE CUS REQUESTED', 'IM REPOPULATE CUS RESUBMITTED', 'IM REPOPULATE ACCUMULATED TIME (MS)', 'IM REPOPULATE BLOCKS INVALID', 'IM REPOPULATE BYTES FROM STORAGE', 'IM REPOPULATE BYTES IN-MEMORY DATA', 'IM REPOPULATE BYTES UNCOMPRESSED DATA', 'IM REPOPULATE ROWS', 'IM REPOPULATE SEGMENTS', 'IM REPOPULATE SEGMENTS REQUESTED', 'IM REPOPULATE TRANSACTIONS ACTIVE', 'IM REPOPULATE TRANSACTIONS CHECK', 'IM REPOPULATE UNDO RECORDS APPLIED', 'IM REPOPULATE UNDO SEGHEADER ROLLBACK', 'IM SCAN CUS CLEANOUT', 'IM SCAN CUS COLUMN NOT IN MEMORY', 'IM SCAN CUS COLUMNS ACCESSED', 'IM SCAN CUS COLUMNS DECOMPRESSED', 'IM SCAN CUS COLUMNS THEORETICAL MAX', 'IM SCAN CUS FAILED TO REGET PIN', 'IM SCAN CUS INVALID', 'IM SCAN CUS INVALID (ALL ROWS ARE INVALID)', 'IM SCAN CUS INVALID OR MISSING REVERT TO ON DISK EXTENT', 'IM SCAN CUS MEMCOMPRESS FOR CAPACITY HIGH', 'IM SCAN CUS MEMCOMPRESS FOR CAPACITY LOW', 'IM SCAN CUS MEMCOMPRESS FOR DML', 'IM SCAN CUS MEMCOMPRESS FOR QUERY HIGH', 'IM SCAN CUS MEMCOMPRESS FOR QUERY LOW', 'IM SCAN CUS NO CLEANOUT', 'IM SCAN CUS NO MEMCOMPRESS', 'IM SCAN CUS NO ROLLBACK', 'IM SCAN CUS OPTIMIZED READ', 'IM SCAN CUS PREDICATES APPLIED', 'IM SCAN CUS PREDICATES OPTIMIZED', 'IM SCAN CUS PREDICATES RECEIVED', 'IM SCAN CUS PRUNED', 'IM SCAN CUS ROLLBACK', 'IM SCAN CUS SPLIT PIECES', 'IM SCAN CUS UNDO RECORDS APPLIED', 'IM SCAN BLOCKS CACHE', 'IM SCAN BYTES IN-MEMORY', 'IM SCAN BYTES UNCOMPRESSED', 'IM SCAN FETCHES JOURNAL', 'IM SCAN FOUND INVALID SMU', 'IM SCAN INVALID ALL BLOCKS', 'IM SCAN JOURNAL', 'IM SCAN JOURNAL CLEANOUT', 'IM SCAN JOURNAL NO CLEANOUT', 'IM SCAN ROWS', 'IM SCAN ROWS CACHE', 'IM SCAN ROWS DISCONTINUOUS', 'IM SCAN ROWS EXCLUDED', 'IM SCAN ROWS JOURNAL', 'IM SCAN ROWS JOURNAL TOTAL', 'IM SCAN ROWS OPTIMIZED', 'IM SCAN ROWS PROJECTED', 'IM SCAN ROWS RANGE EXCLUDED', 'IM SCAN ROWS VALID', 'IM SCAN SEGMENTS DISK', 'IM SCAN SEGMENTS MINMAX ELIGIBLE', 'IM SPACE CU BYTES ALLOCATED', 'IM SPACE CU BYTES FREED', 'IM SPACE CU CREATIONS COMMITTED', 'IM SPACE CU CREATIONS INITIATED', 'IM SPACE CU EXTENTS ALLOCATED', 'IM SPACE CU EXTENTS FREED', 'IM SPACE SMU BYTES ALLOCATED', 'IM SPACE SMU BYTES FREED', 'IM SPACE SMU CREATIONS COMMITTED', 'IM SPACE SMU CREATIONS INITIATED', 'IM SPACE SMU EXTENTS ALLOCATED', 'IM SPACE SMU EXTENTS FREED', 'IM SPACE PRIVATE JOURNAL BYTES ALLOCATED', 'IM SPACE PRIVATE JOURNAL BYTES FREED', 'IM SPACE PRIVATE JOURNAL EXTENTS ALLOCATED', 'IM SPACE PRIVATE JOURNAL EXTENTS FREED', 'IM SPACE PRIVATE JOURNAL SEGMENTS ALLOCATED', 'IM SPACE PRIVATE JOURNAL SEGMENTS FREED', 'IM SPACE SEGMENTS ALLOCATED', 'IM SPACE SEGMENTS FREED', 'IM SPACE SHARED JOURNAL BYTES ALLOCATED', 'IM SPACE SHARED JOURNAL BYTES FREED', 'IM SPACE SHARED JOURNAL EXTENTS ALLOCATED', 'IM SPACE SHARED JOURNAL EXTENTS FREED', 'IM SPACE SHARED JOURNAL SEGMENTS ALLOCATED', 'IM SPACE SHARED JOURNAL SEGMENTS FREED', 'IM TRANSACTIONS', 'IM TRANSACTIONS CU CLEANOUT', 'IM TRANSACTIONS CUS INVALID', 'IM TRANSACTIONS BLOCKS INVALIDATED', 'IM TRANSACTIONS DOWNGRADE MODE', 'IM TRANSACTIONS FOUND INVALID CU', 'IM TRANSACTIONS JOURNAL CLEANOUT', 'IM TRANSACTIONS ROWS INVALIDATED', 'IM TRANSACTIONS ROWS JOURNALED', 'IM ZZZZ SPARE1', 'IM ZZZZ SPARE10', 'IM ZZZZ SPARE2', 'IM ZZZZ SPARE3', 'IM ZZZZ SPARE4', 'IM ZZZZ SPARE5', 'IM ZZZZ SPARE6', 'IM ZZZZ SPARE7', 'IM ZZZZ SPARE8', 'IM ZZZZ SPARE9', 'IMU CR ROLLBACKS', 'IMU FLUSHES', 'IMU REDO ALLOCATION SIZE', 'IMU BIND FLUSHES', 'IMU COMMITS', 'IMU CONTENTION', 'IMU KTICHG FLUSH', 'IMU MBU FLUSH', 'IMU POOL NOT ALLOCATED', 'IMU RECURSIVE-TRANSACTION FLUSH', 'IMU UNDO ALLOCATION SIZE', 'IMU UNDO RETENTION FLUSH', 'IMU- FAILED TO GET A PRIVATE STRAND', 'IPC CPU USED BY THIS SESSION', 'KTFB ALLOC MYINST', 'KTFB ALLOC REQ', 'KTFB ALLOC SEARCH FFB', 'KTFB ALLOC SPACE (BLOCK)', 'KTFB ALLOC STEAL', 'KTFB ALLOC TIME (MS)', 'KTFB APPLY REQ', 'KTFB APPLY TIME (MS)', 'KTFB COMMIT REQ', 'KTFB COMMIT TIME (MS)', 'KTFB FREE REQ', 'KTFB FREE SPACE (BLOCK)', 'KTFB FREE TIME (MS)', 'LOB TABLE ID LOOKUP CACHE MISSES', 'MISSES FOR WRITING MAPPING', 'NO. OF DECRYPT OPS', 'NO. OF ENCRYPT OPS', 'NO. OF NAMESPACES CREATED', 'NO. OF PRINCIPAL CACHE MISSES', 'NO. OF PRINCIPAL INVALIDATIONS', 'NO. OF ROLES ENABLED OR DISABLED', 'NO. OF USER CALLBACKS EXECUTED', 'NO. OF XS SESSIONS ATTACHED', 'NO. OF XS SESSIONS CREATED', 'NUMBER OF FORMAT_PRESERVING REDACTIONS', 'NUMBER OF FULL REDACTIONS', 'NUMBER OF NONE REDACTIONS', 'NUMBER OF PARTIAL REDACTIONS', 'NUMBER OF RANDOM REDACTIONS', 'NUMBER OF REGEXP REDACTIONS', 'NUMBER OF READ IOS ISSUED', 'OLAP AGGREGATE FUNCTION CALC', 'OLAP AGGREGATE FUNCTION LOGICAL NA', 'OLAP AGGREGATE FUNCTION PRECOMPUTE', 'OLAP CUSTOM MEMBER LIMIT', 'OLAP ENGINE CALLS', 'OLAP FAST LIMIT', 'OLAP FULL LIMIT', 'OLAP GID LIMIT', 'OLAP INHIER LIMIT', 'OLAP IMPORT ROWS LOADED', 'OLAP IMPORT ROWS PUSHED', 'OLAP LIMIT TIME', 'OLAP PAGING MANAGER CACHE CHANGED PAGE', 'OLAP PAGING MANAGER CACHE HIT', 'OLAP PAGING MANAGER CACHE MISS', 'OLAP PAGING MANAGER CACHE WRITE', 'OLAP PAGING MANAGER NEW PAGE', 'OLAP PAGING MANAGER POOL SIZE', 'OLAP PERM LOB READ', 'OLAP ROW ID LIMIT', 'OLAP ROW LOAD TIME', 'OLAP ROW SOURCE ROWS PROCESSED', 'OLAP SESSION CACHE HIT', 'OLAP SESSION CACHE MISS', 'OLAP TEMP SEGMENT READ', 'OLAP TEMP SEGMENTS', 'OLAP UNIQUE KEY ATTRIBUTE LIMIT', 'OS BLOCK INPUT OPERATIONS', 'OS BLOCK OUTPUT OPERATIONS', 'OS CPU QT WAIT TIME', 'OS INTEGRAL SHARED TEXT SIZE', 'OS INTEGRAL UNSHARED DATA SIZE', 'OS INTEGRAL UNSHARED STACK SIZE', 'OS INVOLUNTARY CONTEXT SWITCHES', 'OS MAXIMUM RESIDENT SET SIZE', 'OS PAGE FAULTS', 'OS PAGE RECLAIMS', 'OS SIGNALS RECEIVED', 'OS SOCKET MESSAGES RECEIVED', 'OS SOCKET MESSAGES SENT', 'OS SWAPS', 'OS SYSTEM TIME USED', 'OS USER TIME USED', 'OS VOLUNTARY CONTEXT SWITCHES', 'OTC COMMIT OPTIMIZATION ATTEMPTS', 'OTC COMMIT OPTIMIZATION FAILURE - SETUP', 'OTC COMMIT OPTIMIZATION HITS', \"PX LOCAL MESSAGES RECV'D\", 'PX LOCAL MESSAGES SENT', \"PX REMOTE MESSAGES RECV'D\", 'PX REMOTE MESSAGES SENT', 'PARALLEL OPERATIONS DOWNGRADED 1 TO 25 PCT', 'PARALLEL OPERATIONS DOWNGRADED 25 TO 50 PCT', 'PARALLEL OPERATIONS DOWNGRADED 50 TO 75 PCT', 'PARALLEL OPERATIONS DOWNGRADED 75 TO 99 PCT', 'PARALLEL OPERATIONS DOWNGRADED TO SERIAL', 'PARALLEL OPERATIONS NOT DOWNGRADED', 'REQUESTS TO/FROM CLIENT', 'ROWCR - RESUME', 'ROWCR - ROW CONTENTION', 'ROWCR ATTEMPTS', 'ROWCR HITS', 'SCN INCREMENTS DUE TO ANOTHER DATABASE', 'SMON POSTED FOR DROPPING TEMP SEGMENT', 'SMON POSTED FOR INSTANCE RECOVERY', 'SMON POSTED FOR TXN RECOVERY FOR OTHER INSTANCES', 'SMON POSTED FOR UNDO SEGMENT RECOVERY', 'SMON POSTED FOR UNDO SEGMENT SHRINK', 'SQL*NET ROUNDTRIPS TO/FROM CLIENT', 'SQL*NET ROUNDTRIPS TO/FROM DBLINK', 'SECUREFILES COPY FROM DBFS LINK', 'SECUREFILES DBFS LINK OPERATIONS', 'SECUREFILES DBFS LINK OVERWRITES', 'SECUREFILES DBFS LINK STREAMING READS', 'SECUREFILES GET DBFS LINK REFERENCE', 'SECUREFILES IMPLICIT COPY FROM DBFS LINK', 'SECUREFILES MOVE TO DBFS LINK', 'SECUREFILES PUT DBFS LINK REFERENCE', 'STREAMING NO-STALL REAP', 'STREAMING STALL REAP', 'TBS EXTENSION: BYTES EXTENDED', 'TBS EXTENSION: FILES EXTENDED', 'TBS EXTENSION: TASKS CREATED', 'TBS EXTENSION: TASKS EXECUTED', 'WORKLOAD CAPTURE: DBTIME', 'WORKLOAD CAPTURE: ERRORS', 'WORKLOAD CAPTURE: SIZE (IN BYTES) OF RECORDING', 'WORKLOAD CAPTURE: UNREPLAYABLE USER CALLS', 'WORKLOAD CAPTURE: UNSUPPORTED USER CALLS', 'WORKLOAD CAPTURE: USER CALLS', 'WORKLOAD CAPTURE: USER CALLS FLUSHED', 'WORKLOAD CAPTURE: USER LOGINS', 'WORKLOAD CAPTURE: USER TXNS', 'WORKLOAD REPLAY: DBTIME', 'WORKLOAD REPLAY: DEADLOCKS RESOLVED', 'WORKLOAD REPLAY: NETWORK TIME', 'WORKLOAD REPLAY: THINK TIME', 'WORKLOAD REPLAY: TIME GAIN', 'WORKLOAD REPLAY: TIME LOSS', 'WORKLOAD REPLAY: USER CALLS', 'ACTIVE TXN COUNT DURING CLEANOUT', 'APPLICATION WAIT TIME', 'AUTO EXTENDS ON UNDO TABLESPACE', 'BACKGROUND CHECKPOINTS COMPLETED', 'BACKGROUND CHECKPOINTS STARTED', 'BACKGROUND TIMEOUTS', 'BACKUP COMPRESSED DATA WRITTEN LOCALLY', 'BACKUP COMPRESSED DATA WRITTEN REMOTELY', 'BACKUP DATA COMPRESSED LOCALLY', 'BACKUP DATA COMPRESSED REMOTELY', 'BACKUP PIECE LOCAL PROCESSING TIME', 'BACKUP PIECE REMOTE PROCESSING TIME', 'BACKUP PIECES COMPRESSED LOCALLY', 'BACKUP PIECES COMPRESSED REMOTELY', 'BLOCKS DECRYPTED', 'BLOCKS ENCRYPTED', 'BRANCH NODE SPLITS', 'BUFFER IS NOT PINNED COUNT', 'BUFFER IS PINNED COUNT', 'BYTES RECEIVED VIA SQL*NET FROM CLIENT', 'BYTES RECEIVED VIA SQL*NET FROM DBLINK', 'BYTES SENT VIA SQL*NET TO CLIENT', 'BYTES SENT VIA SQL*NET TO DBLINK', 'BYTES VIA SQL*NET VECTOR FROM CLIENT', 'BYTES VIA SQL*NET VECTOR FROM DBLINK', 'BYTES VIA SQL*NET VECTOR TO CLIENT', 'BYTES VIA SQL*NET VECTOR TO DBLINK', 'CALLS TO GET SNAPSHOT SCN: KCMGSS', 'CALLS TO KCMGAS', 'CALLS TO KCMGCS', 'CALLS TO KCMGRS', 'CELL CUS PROCESSED FOR COMPRESSED', 'CELL CUS PROCESSED FOR UNCOMPRESSED', 'CELL CUS SENT COMPRESSED', 'CELL CUS SENT HEAD PIECE', 'CELL CUS SENT UNCOMPRESSED', 'CELL IO UNCOMPRESSED BYTES', 'CELL XT GRANULE BYTES REQUESTED FOR PREDICATE OFFLOAD', 'CELL XT GRANULE PREDICATE OFFLOAD RETRIES', 'CELL XT GRANULES REQUESTED FOR PREDICATE OFFLOAD', 'CELL BLOCKS HELPED BY COMMIT CACHE', 'CELL BLOCKS HELPED BY MINSCN OPTIMIZATION', 'CELL BLOCKS PROCESSED BY CACHE LAYER', 'CELL BLOCKS PROCESSED BY DATA LAYER', 'CELL BLOCKS PROCESSED BY INDEX LAYER', 'CELL BLOCKS PROCESSED BY TXN LAYER', 'CELL COMMIT CACHE QUERIES', 'CELL FLASH CACHE READ HITS', 'CELL INDEX SCANS', 'CELL INTERCONNECT BYTES RETURNED BY XT SMART SCAN', 'CELL LOGICAL WRITE IO REQUESTS', 'CELL LOGICAL WRITE IO REQUESTS ELIGIBLE FOR OFFLOAD', 'CELL NUM BLOCK IOS DUE TO A FILE INSTANT RESTORE IN PROGRESS', 'CELL NUM BYTES IN BLOCK IO DURING PREDICATE OFFLOAD', 'CELL NUM BYTES IN PASSTHRU DURING PREDICATE OFFLOAD', 'CELL NUM BYTES OF IO REISSUED DUE TO RELOCATION', 'CELL NUM FAST RESPONSE SESSIONS', 'CELL NUM FAST RESPONSE SESSIONS CONTINUING TO SMART SCAN', 'CELL NUM SMART IO SESSIONS IN RDBMS BLOCK IO DUE TO BIG PAYLOAD', 'CELL NUM SMART IO SESSIONS IN RDBMS BLOCK IO DUE TO NO CELL MEM', 'CELL NUM SMART IO SESSIONS IN RDBMS BLOCK IO DUE TO OPEN FAIL', 'CELL NUM SMART IO SESSIONS IN RDBMS BLOCK IO DUE TO USER', 'CELL NUM SMART IO SESSIONS USING PASSTHRU MODE DUE TO CELLSRV', 'CELL NUM SMART IO SESSIONS USING PASSTHRU MODE DUE TO TIMEZONE', 'CELL NUM SMART IO SESSIONS USING PASSTHRU MODE DUE TO USER', 'CELL NUM SMART FILE CREATION SESSIONS USING RDBMS BLOCK IO MODE', 'CELL NUM SMARTIO AUTOMEM BUFFER ALLOCATION ATTEMPTS', 'CELL NUM SMARTIO AUTOMEM BUFFER ALLOCATION FAILURES', 'CELL NUM SMARTIO PERMANENT CELL FAILURES', 'CELL NUM SMARTIO TRANSIENT CELL FAILURES', 'CELL OVERWRITES IN FLASH CACHE', 'CELL PARTIAL WRITES IN FLASH CACHE', 'CELL PHYSICAL IO BYTES ELIGIBLE FOR PREDICATE OFFLOAD', 'CELL PHYSICAL IO BYTES SAVED BY COLUMNAR CACHE', 'CELL PHYSICAL IO BYTES SAVED BY STORAGE INDEX', 'CELL PHYSICAL IO BYTES SAVED DURING OPTIMIZED RMAN FILE RESTORE', 'CELL PHYSICAL IO BYTES SAVED DURING OPTIMIZED FILE CREATION', 'CELL PHYSICAL IO BYTES SENT DIRECTLY TO DB NODE TO BALANCE CPU ', 'CELL PHYSICAL IO INTERCONNECT BYTES_y', 'CELL PHYSICAL IO INTERCONNECT BYTES RETURNED BY SMART SCAN', 'CELL PHYSICAL WRITE IO BYTES ELIGIBLE FOR OFFLOAD', 'CELL PHYSICAL WRITE IO HOST NETWORK BYTES WRITTEN DURING OFFLOA', 'CELL PHYSICAL WRITE BYTES SAVED BY SMART FILE INITIALIZATION', 'CELL SCANS', 'CELL SIMULATED PHYSICAL IO BYTES ELIGIBLE FOR PREDICATE OFFLOAD', 'CELL SIMULATED PHYSICAL IO BYTES RETURNED BY PREDICATE OFFLOAD', 'CELL SMART IO SESSION CACHE HARD MISSES', 'CELL SMART IO SESSION CACHE HITS', 'CELL SMART IO SESSION CACHE HWM', 'CELL SMART IO SESSION CACHE LOOKUPS', 'CELL SMART IO SESSION CACHE SOFT MISSES', 'CELL STATISTICS SPARE1', 'CELL STATISTICS SPARE2', 'CELL STATISTICS SPARE3', 'CELL STATISTICS SPARE4', 'CELL STATISTICS SPARE5', 'CELL STATISTICS SPARE6', 'CELL TRANSACTIONS FOUND IN COMMIT CACHE', 'CELL WRITES TO FLASH CACHE', 'CHAINED ROWS PROCESSED BY CELL', 'CHAINED ROWS REJECTED BY CELL', 'CHAINED ROWS SKIPPED BY CELL', 'CHANGE WRITE TIME', 'CHECKPOINT CLONES CREATED FOR ADG RECOVERY', 'CLEANOUT - NUMBER OF KTUGCT CALLS', 'CLEANOUTS AND ROLLBACKS - CONSISTENT READ GETS', 'CLEANOUTS ONLY - CONSISTENT READ GETS', 'CLUSTER KEY SCAN BLOCK GETS', 'CLUSTER KEY SCANS', 'CLUSTER WAIT TIME', 'COLD RECYCLE READS', 'COMMIT BATCH PERFORMED', 'COMMIT BATCH REQUESTED', 'COMMIT BATCH/IMMEDIATE PERFORMED', 'COMMIT BATCH/IMMEDIATE REQUESTED', 'COMMIT CLEANOUT FAILURES: BLOCK LOST', 'COMMIT CLEANOUT FAILURES: BUFFER BEING WRITTEN', 'COMMIT CLEANOUT FAILURES: CALLBACK FAILURE ', 'COMMIT CLEANOUT FAILURES: CANNOT PIN', 'COMMIT CLEANOUT FAILURES: HOT BACKUP IN PROGRESS', 'COMMIT CLEANOUT FAILURES: WRITE DISABLED', 'COMMIT CLEANOUTS', 'COMMIT CLEANOUTS SUCCESSFULLY COMPLETED', 'COMMIT IMMEDIATE PERFORMED', 'COMMIT IMMEDIATE REQUESTED', 'COMMIT NOWAIT PERFORMED', 'COMMIT NOWAIT REQUESTED', 'COMMIT TXN COUNT DURING CLEANOUT', 'COMMIT WAIT PERFORMED', 'COMMIT WAIT REQUESTED', 'COMMIT WAIT/NOWAIT PERFORMED', 'COMMIT WAIT/NOWAIT REQUESTED', 'CONCURRENCY WAIT TIME', 'CONSISTENT CHANGES', 'CONSISTENT GETS', 'CONSISTENT GETS DIRECT', 'CONSISTENT GETS EXAMINATION', 'CONSISTENT GETS EXAMINATION (FASTPATH)', 'CONSISTENT GETS FROM CACHE', 'CONSISTENT GETS PIN', 'CONSISTENT GETS PIN (FASTPATH)', 'CURRENT BLOCKS CONVERTED FOR CR', 'CURSOR AUTHENTICATIONS', 'CURSOR RELOAD FAILURES', 'CVMAP UNAVAILABLE', 'DATA BLOCKS CONSISTENT READS - UNDO RECORDS APPLIED', 'DATA WAREHOUSING COOLING ACTION', 'DATA WAREHOUSING EVICTED OBJECTS', 'DATA WAREHOUSING EVICTED OBJECTS - COOLING', 'DATA WAREHOUSING EVICTED OBJECTS - REPLACE', 'DATA WAREHOUSING SCANNED BLOCKS', 'DATA WAREHOUSING SCANNED BLOCKS - DISK', 'DATA WAREHOUSING SCANNED BLOCKS - FLASH', 'DATA WAREHOUSING SCANNED BLOCKS - MEMORY', 'DATA WAREHOUSING SCANNED BLOCKS - OFFLOAD', 'DATA WAREHOUSING SCANNED OBJECTS', 'DB BLOCK CHANGES', 'DB BLOCK GETS', 'DB BLOCK GETS DIRECT', 'DB BLOCK GETS FROM CACHE', 'DB BLOCK GETS FROM CACHE (FASTPATH)', 'DB CORRUPT BLOCKS DETECTED', 'DB CORRUPT BLOCKS RECOVERED', 'DEFERRED (CURRENT) BLOCK CLEANOUT APPLICATIONS', 'DEFERRED CUR CLEANOUTS (INDEX BLOCKS)', 'DIRTY BUFFERS INSPECTED', 'DOUBLING UP WITH IMU SEGMENT', 'DROP SEGMENT CALLS IN SPACE PRESSURE', 'ENQUEUE CONVERSIONS', 'ENQUEUE DEADLOCKS', 'ENQUEUE RELEASES', 'ENQUEUE REQUESTS', 'ENQUEUE TIMEOUTS', 'ENQUEUE WAITS', 'ERROR COUNT CLEARED BY CELL', 'EXCHANGE DEADLOCKS', 'EXECUTE COUNT', 'FAILED PROBES ON INDEX BLOCK RECLAMATION', 'FASTPATH CONSISTENT GET QUOTA LIMIT', 'FBDA WOKEN UP', 'FILE IO SERVICE TIME', 'FILE IO WAIT TIME', 'FILTERED BLOCKS FAILED BLOCK CHECK', 'FLASH CACHE EVICTION: AGED OUT', 'FLASH CACHE EVICTION: BUFFER PINNED', 'FLASH CACHE EVICTION: INVALIDATED', 'FLASH CACHE INSERT SKIP: DBWR OVERLOADED', 'FLASH CACHE INSERT SKIP: CORRUPT', 'FLASH CACHE INSERT SKIP: EXISTS', 'FLASH CACHE INSERT SKIP: MODIFICATION', 'FLASH CACHE INSERT SKIP: NOT CURRENT', 'FLASH CACHE INSERT SKIP: NOT USEFUL', 'FLASH CACHE INSERTS', 'FLASHBACK CACHE READ OPTIMIZATIONS FOR BLOCK NEW', 'FLASHBACK DIRECT READ OPTIMIZATIONS FOR BLOCK NEW', 'FLASHBACK LOG WRITE BYTES', 'FLASHBACK LOG WRITES', 'FLASHBACK SECUREFILE CACHE READ OPTIMIZATIONS FOR BLOCK NEW', 'FLASHBACK SECUREFILE DIRECT READ OPTIMIZATIONS FOR BLOCK NEW', 'FOREGROUND PROPAGATED TRACKED TRANSACTIONS', 'FRAME SIGNATURE MISMATCH', 'FREE BUFFER INSPECTED', 'FREE BUFFER REQUESTED', 'GC CPU USED BY THIS SESSION', 'GC IM BLOCKS INVALIDATED', 'GC IM EXPANDS', 'GC IM GRANTS', 'GC IM SHRINKS', 'GC BLOCKS COMPRESSED', 'GC BLOCKS CORRUPT', 'GC BLOCKS LOST', 'GC CLAIM BLOCKS LOST', 'GC CLEANOUT APPLIED', 'GC CLEANOUT NO SPACE', 'GC CLEANOUT SAVED', 'GC CLUSTER FLASH CACHE READS FAILURE', 'GC CLUSTER FLASH CACHE READS RECEIVED', 'GC CLUSTER FLASH CACHE READS SERVED', 'GC CLUSTER FLASH CACHE RECEIVED READ TIME', 'GC CR BLOCK BUILD TIME', 'GC CR BLOCK FLUSH TIME', 'GC CR BLOCK RECEIVE TIME', 'GC CR BLOCK SEND TIME', 'GC CR BLOCKS FLUSHED', 'GC CR BLOCKS RECEIVED', 'GC CR BLOCKS RECEIVED WITH BPS', 'GC CR BLOCKS SERVED', 'GC CR BLOCKS SERVED WITH BPS', 'GC CURRENT BLOCK FLUSH TIME', 'GC CURRENT BLOCK PIN TIME', 'GC CURRENT BLOCK RECEIVE TIME', 'GC CURRENT BLOCK SEND TIME', 'GC CURRENT BLOCKS FLUSHED', 'GC CURRENT BLOCKS PINNED', 'GC CURRENT BLOCKS RECEIVED', 'GC CURRENT BLOCKS RECEIVED WITH BPS', 'GC CURRENT BLOCKS SERVED', 'GC CURRENT BLOCKS SERVED WITH BPS', 'GC FLASH CACHE READS SERVED', 'GC FLASH CACHE SERVED READ TIME', 'GC FORCE CR READ CR', 'GC FORCE CR READ CURRENT', 'GC KA GRANT RECEIVE TIME', 'GC KA GRANTS RECEIVED', 'GC KBYTES SAVED', 'GC KBYTES SENT', 'GC LOCAL GRANTS', 'GC READ TIME WAITED', 'GC READ WAIT FAILURES', 'GC READ WAIT TIMEOUTS', 'GC READ WAITS', 'GC READER BYPASS GRANTS', 'GC READER BYPASS WAITS', 'GC REMOTE GRANTS', 'GCS MESSAGES SENT', 'GES MESSAGES SENT', 'GLOBAL ENQUEUE CPU USED BY THIS SESSION', 'GLOBAL ENQUEUE GET TIME', 'GLOBAL ENQUEUE GETS ASYNC', 'GLOBAL ENQUEUE GETS SYNC', 'GLOBAL ENQUEUE RELEASES', 'GLOBAL UNDO SEGMENT HINTS HELPED', 'GLOBAL UNDO SEGMENT HINTS WERE STALE', 'HEAP BLOCK COMPRESS', 'HOT BUFFERS MOVED TO HEAD OF LRU', 'IMMEDIATE (CR) BLOCK CLEANOUT APPLICATIONS', 'IMMEDIATE (CURRENT) BLOCK CLEANOUT APPLICATIONS', 'IMMEDIATE CR CLEANOUTS (INDEX BLOCKS)', 'IN CALL IDLE WAIT TIME', 'INDEX CMPH GENCU, UNCOMP SENTINALS', 'INDEX CMPH LD, CU FIT', 'INDEX CMPH LD, CU FIT, ADD ROWS', 'INDEX CMPH LD, CU NEGATIVE COMP', 'INDEX CMPH LD, CU OVER-EST', 'INDEX CMPH LD, CU UNDER-EST', 'INDEX CMPH LD, LF BLKS FLUSHED', 'INDEX CMPH LD, LF BLKS W/ UND CU', 'INDEX CMPH LD, LF BLKS W/O CU', 'INDEX CMPH LD, LF BLKS W/O UNC R', 'INDEX CMPH LD, RETRY IN OVER-EST', 'INDEX CMPH LD, ROWS COMPRESSED', 'INDEX CMPH LD, ROWS UNCOMPRESSED', 'INDEX CMPH SP, LEAF BLOCK 90_10 SPLITS FAILED', 'INDEX CMPH SP, LEAF BLOCK SPLITS AVOIDED', 'INDEX CMPL CO, PREFIX MISMATCH', 'INDEX CMPL RO, BLOCKS NOT COMPRESSED', 'INDEX CMPL RO, PREFIX CHANGE AT BLOCK', 'INDEX CMPL RO, PREFIX NO CHANGE AT BLOCK', 'INDEX CMPL RO, REORG AVOID LOAD NEW BLOCK', 'INDEX CMPL RO, REORG AVOID SPLIT', 'INDEX CRX UPGRADE (FOUND)', 'INDEX CRX UPGRADE (POSITIONED)', 'INDEX CRX UPGRADE (PREFETCH)', 'INDEX FAST FULL SCANS (DIRECT READ)', 'INDEX FAST FULL SCANS (FULL)', 'INDEX FAST FULL SCANS (ROWID RANGES)', 'INDEX FETCH BY KEY', 'INDEX RECLAMATION/EXTENSION SWITCH', 'INDEX SCANS KDIIXS1', 'JAVA CALL HEAP COLLECTED BYTES', 'JAVA CALL HEAP COLLECTED COUNT', 'JAVA CALL HEAP GC COUNT', 'JAVA CALL HEAP LIVE OBJECT COUNT', 'JAVA CALL HEAP LIVE OBJECT COUNT MAX', 'JAVA CALL HEAP LIVE SIZE', 'JAVA CALL HEAP LIVE SIZE MAX', 'JAVA CALL HEAP OBJECT COUNT', 'JAVA CALL HEAP OBJECT COUNT MAX', 'JAVA CALL HEAP TOTAL SIZE', 'JAVA CALL HEAP TOTAL SIZE MAX', 'JAVA CALL HEAP USED SIZE', 'JAVA CALL HEAP USED SIZE MAX', 'JAVA SESSION HEAP COLLECTED BYTES', 'JAVA SESSION HEAP COLLECTED COUNT', 'JAVA SESSION HEAP GC COUNT', 'JAVA SESSION HEAP LIVE OBJECT COUNT', 'JAVA SESSION HEAP LIVE OBJECT COUNT MAX', 'JAVA SESSION HEAP LIVE SIZE', 'JAVA SESSION HEAP LIVE SIZE MAX', 'JAVA SESSION HEAP OBJECT COUNT', 'JAVA SESSION HEAP OBJECT COUNT MAX', 'JAVA SESSION HEAP USED SIZE', 'JAVA SESSION HEAP USED SIZE MAX', 'KA GRANTS RECEIVED', 'KA LOCAL MESSAGE WAITS', 'KA LOCAL MESSAGES RECEIVED', 'KA MESSAGES SENT', 'KA WAIT CALLS ATTEMPTED', 'KA WAIT CALLS FOR INVALID KGA', 'KA WAIT CALLS OTHER', 'KA WAIT CALLS SUPPRESSED', 'KA WAIT DUE TO TIMEOUT', 'KA WAIT DUE TO TRIGGER', 'LARGE TRACKED TRANSACTIONS', 'LEAF NODE 90-10 SPLITS', 'LEAF NODE SPLITS', 'LOB READS', 'LOB WRITES', 'LOB WRITES UNALIGNED', 'LOCAL UNDO SEGMENT HINTS HELPED', 'LOCAL UNDO SEGMENT HINTS WERE STALE', 'LOGICAL READ BYTES FROM CACHE', 'LOGONS CUMULATIVE', 'LOGONS CURRENT', 'MAX CF ENQ HOLD TIME', 'MESSAGES RECEIVED', 'MESSAGES SENT', 'MIN ACTIVE SCN OPTIMIZATION APPLIED ON CR', 'NO BUFFER TO KEEP PINNED COUNT', 'NO WORK - CONSISTENT READ GETS', 'NON-IDLE WAIT COUNT', 'NON-IDLE WAIT TIME', 'NUMBER OF MAP MISSES', 'NUMBER OF MAP OPERATIONS', 'OPENED CURSORS CUMULATIVE', 'OPENED CURSORS CURRENT', 'PARSE COUNT (DESCRIBE)', 'PARSE COUNT (FAILURES)', 'PARSE COUNT (HARD)', 'PARSE COUNT (TOTAL)', 'PARSE TIME CPU', 'PARSE TIME ELAPSED', 'PHYSICAL READ IO REQUESTS', 'PHYSICAL READ BYTES', 'PHYSICAL READ FLASH CACHE HITS', 'PHYSICAL READ PARTIAL REQUESTS', 'PHYSICAL READ REQUESTS OPTIMIZED', 'PHYSICAL READ SNAP IO REQUESTS BASE', 'PHYSICAL READ SNAP IO REQUESTS COPY', 'PHYSICAL READ SNAP IO REQUESTS NO DATA', 'PHYSICAL READ SNAP BYTES BASE', 'PHYSICAL READ SNAP BYTES COPY', 'PHYSICAL READ TOTAL IO REQUESTS', 'PHYSICAL READ TOTAL BYTES', 'PHYSICAL READ TOTAL BYTES OPTIMIZED', 'PHYSICAL READ TOTAL MULTI BLOCK REQUESTS', 'PHYSICAL READS', 'PHYSICAL READS CACHE', 'PHYSICAL READS CACHE FOR SECUREFILE FLASHBACK BLOCK NEW', 'PHYSICAL READS CACHE PREFETCH', 'PHYSICAL READS DIRECT', 'PHYSICAL READS DIRECT (LOB)', 'PHYSICAL READS DIRECT FOR SECUREFILE FLASHBACK BLOCK NEW', 'PHYSICAL READS DIRECT TEMPORARY TABLESPACE', 'PHYSICAL READS FOR FLASHBACK NEW', 'PHYSICAL READS PREFETCH WARMUP', 'PHYSICAL READS RETRY CORRUPT', 'PHYSICAL WRITE IO REQUESTS', 'PHYSICAL WRITE BYTES', 'PHYSICAL WRITE REQUESTS OPTIMIZED', 'PHYSICAL WRITE SNAP IO REQUESTS NEW ALLOCATIONS', 'PHYSICAL WRITE TOTAL IO REQUESTS', 'PHYSICAL WRITE TOTAL BYTES', 'PHYSICAL WRITE TOTAL BYTES OPTIMIZED', 'PHYSICAL WRITE TOTAL MULTI BLOCK REQUESTS', 'PHYSICAL WRITES', 'PHYSICAL WRITES DIRECT', 'PHYSICAL WRITES DIRECT (LOB)', 'PHYSICAL WRITES DIRECT TEMPORARY TABLESPACE', 'PHYSICAL WRITES FROM CACHE', 'PHYSICAL WRITES NON CHECKPOINT', 'PINNED BUFFERS INSPECTED', 'PINNED CURSORS CURRENT', 'PREFETCH CLIENTS - 16K', 'PREFETCH CLIENTS - 2K', 'PREFETCH CLIENTS - 32K', 'PREFETCH CLIENTS - 4K', 'PREFETCH CLIENTS - 8K', 'PREFETCH CLIENTS - DEFAULT', 'PREFETCH CLIENTS - KEEP', 'PREFETCH CLIENTS - RECYCLE', 'PREFETCH WARMUP BLOCKS AGED OUT BEFORE USE', 'PREFETCH WARMUP BLOCKS FLUSHED OUT BEFORE USE', 'PREFETCHED BLOCKS AGED OUT BEFORE USE', 'PROCESS LAST NON-IDLE TIME', 'QUERIES PARALLELIZED', 'QUEUE FLUSH', 'QUEUE OCP PAGES', 'QUEUE POSITION UPDATE', 'QUEUE QNO PAGES', 'QUEUE SINGLE ROW', 'QUEUE SPLITS', 'QUEUE UPDATE WITHOUT CP UPDATE', 'READ-ONLY VIOLATION COUNT', 'RECIEVE BUFFER UNAVAILABLE', 'RECOVERY ARRAY READ TIME', 'RECOVERY ARRAY READS', 'RECOVERY BLOCK GETS FROM CACHE', 'RECOVERY BLOCKS READ', 'RECOVERY BLOCKS READ FOR LOST WRITE DETECTION', 'RECOVERY BLOCKS SKIPPED LOST WRITE CHECKS', 'RECOVERY MARKER', 'RECURSIVE ABORTS ON INDEX BLOCK RECLAMATION', 'RECURSIVE CALLS', 'RECURSIVE CPU USAGE', 'RECURSIVE SYSTEM API INVOCATIONS', 'REDO KB READ', 'REDO KB READ (MEMORY)', 'REDO KB READ (MEMORY) FOR TRANSPORT', 'REDO KB READ FOR TRANSPORT', 'REDO BLOCKS CHECKSUMMED BY FG (EXCLUSIVE)', 'REDO BLOCKS CHECKSUMMED BY LGWR', 'REDO BLOCKS READ FOR RECOVERY', 'REDO BLOCKS WRITTEN', 'REDO BLOCKS WRITTEN (GROUP 0)', 'REDO BLOCKS WRITTEN (GROUP 1)', 'REDO BLOCKS WRITTEN (GROUP 2)', 'REDO BLOCKS WRITTEN (GROUP 3)', 'REDO BLOCKS WRITTEN (GROUP 4)', 'REDO BLOCKS WRITTEN (GROUP 5)', 'REDO BLOCKS WRITTEN (GROUP 6)', 'REDO BLOCKS WRITTEN (GROUP 7)', 'REDO BUFFER ALLOCATION RETRIES', 'REDO ENTRIES', 'REDO ENTRIES FOR LOST WRITE DETECTION', 'REDO K-BYTES READ FOR RECOVERY', 'REDO K-BYTES READ FOR TERMINAL RECOVERY', 'REDO LOG SPACE REQUESTS', 'REDO LOG SPACE WAIT TIME', 'REDO NON-DURABLE RECORDS SKIPPED', 'REDO ORDERING MARKS', 'REDO SIZE', 'REDO SIZE FOR DIRECT WRITES', 'REDO SIZE FOR LOST WRITE DETECTION', 'REDO SUBSCN MAX COUNTS', 'REDO SYNCH LONG WAITS', 'REDO SYNCH POLL WRITES', 'REDO SYNCH POLLS', 'REDO SYNCH TIME', 'REDO SYNCH TIME (USEC)', 'REDO SYNCH TIME OVERHEAD (USEC)', 'REDO SYNCH TIME OVERHEAD COUNT (  2MS)', 'REDO SYNCH TIME OVERHEAD COUNT (  8MS)', 'REDO SYNCH TIME OVERHEAD COUNT ( 32MS)', 'REDO SYNCH TIME OVERHEAD COUNT (128MS)', 'REDO SYNCH TIME OVERHEAD COUNT (INF)', 'REDO SYNCH WRITES', 'REDO WASTAGE', 'REDO WRITE BROADCAST ACK COUNT', 'REDO WRITE BROADCAST ACK TIME', 'REDO WRITE BROADCAST LGWR POST COUNT', 'REDO WRITE FINISH TIME', 'REDO WRITE GATHER TIME', 'REDO WRITE INFO FIND', 'REDO WRITE INFO FIND FAIL', 'REDO WRITE ISSUE TIME', 'REDO WRITE SCHEDULE TIME', 'REDO WRITE SIZE COUNT (   4KB)', 'REDO WRITE SIZE COUNT (   8KB)', 'REDO WRITE SIZE COUNT (  16KB)', 'REDO WRITE SIZE COUNT (  32KB)', 'REDO WRITE SIZE COUNT (  64KB)', 'REDO WRITE SIZE COUNT ( 128KB)', 'REDO WRITE SIZE COUNT ( 256KB)', 'REDO WRITE SIZE COUNT ( 512KB)', 'REDO WRITE SIZE COUNT (1024KB)', 'REDO WRITE SIZE COUNT (INF)', 'REDO WRITE TIME', 'REDO WRITE TIME (USEC)', 'REDO WRITE TOTAL TIME', 'REDO WRITE WORKER DELAY (USEC)', 'REDO WRITES', 'REDO WRITES (GROUP 0)', 'REDO WRITES (GROUP 1)', 'REDO WRITES (GROUP 2)', 'REDO WRITES (GROUP 3)', 'REDO WRITES (GROUP 4)', 'REDO WRITES (GROUP 5)', 'REDO WRITES (GROUP 6)', 'REDO WRITES (GROUP 7)', 'REDO WRITES ADAPTIVE ALL', 'REDO WRITES ADAPTIVE WORKER', 'REMOTE ORADEBUG REQUESTS', 'ROLLBACK CHANGES - UNDO RECORDS APPLIED', 'ROLLBACKS ONLY - CONSISTENT READ GETS', 'ROOT NODE SPLITS', 'ROWS FETCHED VIA CALLBACK', 'SAGE SEND BLOCK BY CELL', 'SCHEDULER WAIT TIME', 'SECUREFILE ADD DEDUPD LOB TO SET', 'SECUREFILE ALLOCATION BYTES', 'SECUREFILE ALLOCATION CHUNKS', 'SECUREFILE BYTES CLEARTEXT', 'SECUREFILE BYTES DEDUPLICATED', 'SECUREFILE BYTES ENCRYPTED', 'SECUREFILE BYTES NON-TRANSFORMED', 'SECUREFILE COMPRESSED BYTES', 'SECUREFILE CREATE DEDUP SET', 'SECUREFILE DEDUP CALLBACK OPER FINAL', 'SECUREFILE DEDUP FITS INLINE', 'SECUREFILE DEDUP FLUSH TOO LOW', 'SECUREFILE DEDUP HASH COLLISION', 'SECUREFILE DEDUP PREFIX HASH MATCH', 'SECUREFILE DEDUP WAPP CACHE MISS', 'SECUREFILE DESTROY DEDUP SET', 'SECUREFILE DIRECT READ BYTES', 'SECUREFILE DIRECT READ OPS', 'SECUREFILE DIRECT WRITE BYTES', 'SECUREFILE DIRECT WRITE OPS', 'SECUREFILE INODE IOREAP TIME', 'SECUREFILE INODE READ TIME', 'SECUREFILE INODE WRITE TIME', 'SECUREFILE NUMBER OF FLUSHES', 'SECUREFILE NUMBER OF NON-TRANSFORMED FLUSHES', 'SECUREFILE REJECT DEDUPLICATION', 'SECUREFILE RMV FROM DEDUP SET', 'SECUREFILE UNCOMPRESSED BYTES', 'SEGMENT CFS ALLOCATIONS', 'SEGMENT CHUNKS ALLOCATION FROM DISEPNSER', 'SEGMENT DISPENSER ALLOCATIONS', 'SEGMENT DISPENSER LOAD EMPTY', 'SEGMENT DISPENSER LOAD TASKS', 'SEGMENT PREALLOC BYTES', 'SEGMENT PREALLOC OPS', 'SEGMENT PREALLOC TASKS', 'SEGMENT PREALLOC TIME (MS)', 'SEGMENT PREALLOC UFS2CFS BYTES', 'SEGMENT TOTAL CHUNK ALLOCATION', 'SERIALIZABLE ABORTS', 'SESSION CONNECT TIME', 'SESSION CURSOR CACHE COUNT', 'SESSION CURSOR CACHE HITS', 'SESSION LOGICAL READS', 'SESSION LOGICAL READS - IM', 'SESSION LOGICAL READS IN LOCAL NUMA GROUP', 'SESSION LOGICAL READS IN REMOTE NUMA GROUP', 'SESSION PGA MEMORY', 'SESSION PGA MEMORY MAX', 'SESSION STORED PROCEDURE SPACE', 'SESSION UGA MEMORY', 'SESSION UGA MEMORY MAX', 'SHARED HASH LATCH UPGRADES - NO WAIT', 'SHARED HASH LATCH UPGRADES - WAIT', 'SHARED IO POOL BUFFER GET FAILURE', 'SHARED IO POOL BUFFER GET SUCCESS', 'SLAVE PROPAGATED TRACKED TRANSACTIONS', 'SORTS (DISK)', 'SORTS (MEMORY)', 'SORTS (ROWS)', 'SPACE WAS FOUND BY TUNE DOWN', 'SPACE WAS NOT FOUND BY TUNE DOWN', 'SPARE STATISTIC 1', 'SPARE STATISTIC 10', 'SPARE STATISTIC 11', 'SPARE STATISTIC 12', 'SPARE STATISTIC 13', 'SPARE STATISTIC 14', 'SPARE STATISTIC 15', 'SPARE STATISTIC 16', 'SPARE STATISTIC 17', 'SPARE STATISTIC 18', 'SPARE STATISTIC 19', 'SPARE STATISTIC 2', 'SPARE STATISTIC 20', 'SPARE STATISTIC 21', 'SPARE STATISTIC 22', 'SPARE STATISTIC 23', 'SPARE STATISTIC 24', 'SPARE STATISTIC 25', 'SPARE STATISTIC 26', 'SPARE STATISTIC 27', 'SPARE STATISTIC 28', 'SPARE STATISTIC 29', 'SPARE STATISTIC 3', 'SPARE STATISTIC 30', 'SPARE STATISTIC 31', 'SPARE STATISTIC 32', 'SPARE STATISTIC 33', 'SPARE STATISTIC 34', 'SPARE STATISTIC 35', 'SPARE STATISTIC 36', 'SPARE STATISTIC 37', 'SPARE STATISTIC 38', 'SPARE STATISTIC 39', 'SPARE STATISTIC 4', 'SPARE STATISTIC 40', 'SPARE STATISTIC 5', 'SPARE STATISTIC 6', 'SPARE STATISTIC 7', 'SPARE STATISTIC 8', 'SPARE STATISTIC 9', 'SQL AREA EVICTED', 'SQL AREA PURGED', 'STEPS OF TUNE DOWN RET. IN SPACE PRESSURE', 'SUMMED DIRTY QUEUE LENGTH', 'SWITCH CURRENT TO NEW BUFFER', 'TABLE FETCH BY ROWID', 'TABLE FETCH CONTINUED ROW', 'TABLE SCAN BLOCKS GOTTEN', 'TABLE SCAN DISK IMC FALLBACK', 'TABLE SCAN DISK NON-IMC ROWS GOTTEN', 'TABLE SCAN ROWS GOTTEN', 'TABLE SCANS (IM)', 'TABLE SCANS (CACHE PARTITIONS)', 'TABLE SCANS (DIRECT READ)', 'TABLE SCANS (LONG TABLES)', 'TABLE SCANS (ROWID RANGES)', 'TABLE SCANS (SHORT TABLES)', 'TEMP SPACE ALLOCATED (BYTES)', 'TOTAL CF ENQ HOLD TIME', 'TOTAL NUMBER OF CF ENQ HOLDERS', 'TOTAL NUMBER OF SLOTS', 'TOTAL NUMBER OF TIMES SMON POSTED', 'TOTAL NUMBER OF UNDO SEGMENTS DROPPED', 'TRACKED ROWS', 'TRACKED TRANSACTIONS', 'TRANSACTION LOCK BACKGROUND GET TIME', 'TRANSACTION LOCK BACKGROUND GETS', 'TRANSACTION LOCK FOREGROUND REQUESTS', 'TRANSACTION LOCK FOREGROUND WAIT TIME', 'TRANSACTION ROLLBACKS', 'TRANSACTION TABLES CONSISTENT READ ROLLBACKS', 'TRANSACTION TABLES CONSISTENT READS - UNDO RECORDS APPLIED', 'TUNE DOWN RETENTIONS IN SPACE PRESSURE', 'UNDO CHANGE VECTOR SIZE', 'UNDO SEGMENT HEADER WAS PINNED', 'USER I/O WAIT TIME', 'USER CALLS', 'USER COMMITS', 'USER LOGONS CUMULATIVE', 'USER LOGOUTS CUMULATIVE', 'USER ROLLBACKS', 'VERY LARGE TRACKED TRANSACTIONS', 'WORKAREA EXECUTIONS - MULTIPASS', 'WORKAREA EXECUTIONS - ONEPASS', 'WORKAREA EXECUTIONS - OPTIMAL', 'WORKAREA MEMORY ALLOCATED', 'WRITE CLONES CREATED FOR RECOVERY', 'WRITE CLONES CREATED IN BACKGROUND', 'WRITE CLONES CREATED IN FOREGROUND']\n"
     ]
    }
   ],
   "source": [
    "print\n",
    "df = pd.merge(rep_hist_snapshot_df, rep_hist_sysmetric_summary_df,how='inner',on ='SNAP_ID')\n",
    "df = pd.merge(df, rep_hist_sysstat_df,how='inner',on ='SNAP_ID')\n",
    "print(df.shape)\n",
    "print('----------------------------------')\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8489, 1418)\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=['SNAP_ID'], ascending=True, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point precision conversion\n",
    "\n",
    "Each column is converted into a column of type values which are floating point for higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8489, 1418)\n"
     ]
    }
   ],
   "source": [
    "df.astype('float32', inplace=True)\n",
    "df = np.round(df, 3) # rounds to 3 dp\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundant Feature Removal\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before column drop:\n",
      "(8489, 1418)\n",
      "\n",
      "Shape before changes: [(8489, 1418)]\n",
      "Shape after changes: [(8489, 520)]\n",
      "Dropped a total [898]\n",
      "\n",
      "After flatline column drop:\n",
      "(8489, 520)\n",
      "\n",
      "After additional column drop:\n",
      "(8489, 510)\n"
     ]
    }
   ],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "\n",
    "print('Before column drop:')\n",
    "print(df.shape)\n",
    "df = drop_flatline_columns(df=df)\n",
    "print('\\nAfter flatline column drop:')\n",
    "print(df.shape)\n",
    "dropped_columns_df = [ 'PLAN_HASH_VALUE',\n",
    "                       'OPTIMIZER_ENV_HASH_VALUE',\n",
    "                       'LOADED_VERSIONS',\n",
    "                       'VERSION_COUNT',\n",
    "                       'PARSING_SCHEMA_ID',\n",
    "                       'PARSING_USER_ID',\n",
    "                       'CON_DBID',\n",
    "                       'SNAP_LEVEL',\n",
    "                       'SNAP_FLAG',\n",
    "                       'COMMAND_TYPE']\n",
    "df.drop(columns=dropped_columns_df, inplace=True)\n",
    "print('\\nAfter additional column drop:')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection - Standard Deviation Method\n",
    "\n",
    "Detection and transformation of outliers, categorized as more than 3 standard deviations away.\n",
    "\n",
    "If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers.\n",
    "\n",
    "The Gaussian distribution has the property that the standard deviation from the mean can be used to reliably summarize the percentage of values in the sample.\n",
    "\n",
    "For example, within one standard deviation of the mean will cover 68% of the data.\n",
    "\n",
    "So, if the mean is 50 and the standard deviation is 5, as in the test dataset above, then all data in the sample between 45 and 55 will account for about 68% of the data sample. We can cover more of the data sample if we expand the range as follows:\n",
    "\n",
    "* 1 Standard Deviation from the Mean: 68%\n",
    "* 2 Standard Deviations from the Mean: 95%\n",
    "* 3 Standard Deviations from the Mean: 99.7%\n",
    "\n",
    "A value that falls outside of 3 standard deviations is part of the distribution, but it is an unlikely or rare event at approximately 1 in 370 samples.\n",
    "\n",
    "Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution. For smaller samples of data, perhaps a value of 2 standard deviations (95%) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9%) can be used.\n",
    "\n",
    "More infor here: https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU_TIME_DELTA - [0] outliers\n",
      "DISK_READS_DELTA - [226] outliers\n",
      "Total Outliers: [226]\n",
      "\n",
      "Label[CPU_TIME_DELTA] - Min[5833869] - Max[1648357491] - Mean[496909591.6899517] - Std[454768007.5116687]\n",
      "Label[DISK_READS_DELTA] - Min[33.0] - Max[20950067.0] - Mean[1851843.031688067] - Std[2785177.331241688]\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "Header [DISK_READS_DELTA] - Location [7] - Value [14673149.0]\n",
      "Header [DISK_READS_DELTA] - Location [8] - Value [20155595.0]\n",
      "Header [DISK_READS_DELTA] - Location [9] - Value [12084700.0]\n",
      "Header [DISK_READS_DELTA] - Location [20] - Value [19053001.0]\n",
      "Header [DISK_READS_DELTA] - Location [21] - Value [14991051.0]\n",
      "Header [DISK_READS_DELTA] - Location [32] - Value [18892537.0]\n",
      "Header [DISK_READS_DELTA] - Location [33] - Value [16191142.0]\n",
      "Header [DISK_READS_DELTA] - Location [44] - Value [15902005.0]\n",
      "Header [DISK_READS_DELTA] - Location [45] - Value [17596414.0]\n",
      "Header [DISK_READS_DELTA] - Location [56] - Value [12002480.0]\n",
      "Header [DISK_READS_DELTA] - Location [57] - Value [20467715.0]\n",
      "Header [DISK_READS_DELTA] - Location [58] - Value [15544324.0]\n",
      "Header [DISK_READS_DELTA] - Location [70] - Value [19944407.0]\n",
      "Header [DISK_READS_DELTA] - Location [71] - Value [15754396.0]\n",
      "Header [DISK_READS_DELTA] - Location [82] - Value [19068706.0]\n",
      "Header [DISK_READS_DELTA] - Location [83] - Value [16189658.0]\n",
      "Header [DISK_READS_DELTA] - Location [94] - Value [16777727.0]\n",
      "Header [DISK_READS_DELTA] - Location [95] - Value [19479223.0]\n",
      "Header [DISK_READS_DELTA] - Location [96] - Value [11253747.0]\n",
      "Header [DISK_READS_DELTA] - Location [107] - Value [20536137.0]\n",
      "Header [DISK_READS_DELTA] - Location [108] - Value [15328345.0]\n",
      "Header [DISK_READS_DELTA] - Location [119] - Value [19845628.0]\n",
      "Header [DISK_READS_DELTA] - Location [120] - Value [16418773.0]\n",
      "Header [DISK_READS_DELTA] - Location [131] - Value [19316491.0]\n",
      "Header [DISK_READS_DELTA] - Location [132] - Value [17174721.0]\n",
      "Header [DISK_READS_DELTA] - Location [143] - Value [13397937.0]\n",
      "Header [DISK_READS_DELTA] - Location [144] - Value [20654293.0]\n",
      "Header [DISK_READS_DELTA] - Location [145] - Value [14703686.0]\n",
      "Header [DISK_READS_DELTA] - Location [156] - Value [17616328.0]\n",
      "Header [DISK_READS_DELTA] - Location [157] - Value [15850219.0]\n",
      "Header [DISK_READS_DELTA] - Location [168] - Value [16955649.0]\n",
      "Header [DISK_READS_DELTA] - Location [169] - Value [15342970.0]\n",
      "Header [DISK_READS_DELTA] - Location [180] - Value [15044503.0]\n",
      "Header [DISK_READS_DELTA] - Location [181] - Value [17507157.0]\n",
      "Header [DISK_READS_DELTA] - Location [193] - Value [18483430.0]\n",
      "Header [DISK_READS_DELTA] - Location [194] - Value [13614603.0]\n",
      "Header [DISK_READS_DELTA] - Location [205] - Value [16305804.0]\n",
      "Header [DISK_READS_DELTA] - Location [206] - Value [12436562.0]\n",
      "Header [DISK_READS_DELTA] - Location [217] - Value [13834845.0]\n",
      "Header [DISK_READS_DELTA] - Location [218] - Value [13913026.0]\n",
      "Header [DISK_READS_DELTA] - Location [229] - Value [13737358.0]\n",
      "Header [DISK_READS_DELTA] - Location [230] - Value [20950067.0]\n",
      "Header [DISK_READS_DELTA] - Location [231] - Value [16294502.0]\n",
      "Header [DISK_READS_DELTA] - Location [242] - Value [17255327.0]\n",
      "Header [DISK_READS_DELTA] - Location [243] - Value [13795891.0]\n",
      "Header [DISK_READS_DELTA] - Location [254] - Value [14880364.0]\n",
      "Header [DISK_READS_DELTA] - Location [255] - Value [13348296.0]\n",
      "Header [DISK_READS_DELTA] - Location [263] - Value [11007509.0]\n",
      "Header [DISK_READS_DELTA] - Location [267] - Value [10289071.0]\n",
      "Header [DISK_READS_DELTA] - Location [292] - Value [13571257.0]\n",
      "Header [DISK_READS_DELTA] - Location [293] - Value [11231847.0]\n",
      "Header [DISK_READS_DELTA] - Location [305] - Value [10874188.0]\n",
      "Header [DISK_READS_DELTA] - Location [317] - Value [12783644.0]\n",
      "Header [DISK_READS_DELTA] - Location [329] - Value [14376028.0]\n",
      "Header [DISK_READS_DELTA] - Location [342] - Value [12253758.0]\n",
      "Header [DISK_READS_DELTA] - Location [343] - Value [11275558.0]\n",
      "Header [DISK_READS_DELTA] - Location [354] - Value [10647121.0]\n",
      "Header [DISK_READS_DELTA] - Location [355] - Value [17194419.0]\n",
      "Header [DISK_READS_DELTA] - Location [379] - Value [16868955.0]\n",
      "Header [DISK_READS_DELTA] - Location [380] - Value [15609340.0]\n",
      "Header [DISK_READS_DELTA] - Location [392] - Value [14959561.0]\n",
      "Header [DISK_READS_DELTA] - Location [405] - Value [12351766.0]\n",
      "Header [DISK_READS_DELTA] - Location [417] - Value [17357597.0]\n",
      "Header [DISK_READS_DELTA] - Location [418] - Value [13942123.0]\n",
      "Header [DISK_READS_DELTA] - Location [430] - Value [14209769.0]\n",
      "Header [DISK_READS_DELTA] - Location [431] - Value [12891463.0]\n",
      "Header [DISK_READS_DELTA] - Location [442] - Value [11040487.0]\n",
      "Header [DISK_READS_DELTA] - Location [443] - Value [11945605.0]\n",
      "Header [DISK_READS_DELTA] - Location [455] - Value [17984963.0]\n",
      "Header [DISK_READS_DELTA] - Location [456] - Value [12513888.0]\n",
      "Header [DISK_READS_DELTA] - Location [467] - Value [14304189.0]\n",
      "Header [DISK_READS_DELTA] - Location [468] - Value [10868122.0]\n",
      "Header [DISK_READS_DELTA] - Location [479] - Value [13634279.0]\n",
      "Header [DISK_READS_DELTA] - Location [480] - Value [11246883.0]\n",
      "Header [DISK_READS_DELTA] - Location [491] - Value [12136324.0]\n",
      "Header [DISK_READS_DELTA] - Location [492] - Value [17719351.0]\n",
      "Header [DISK_READS_DELTA] - Location [493] - Value [10509603.0]\n",
      "Header [DISK_READS_DELTA] - Location [504] - Value [16415638.0]\n",
      "Header [DISK_READS_DELTA] - Location [505] - Value [11139858.0]\n",
      "Header [DISK_READS_DELTA] - Location [516] - Value [10767076.0]\n",
      "Header [DISK_READS_DELTA] - Location [529] - Value [15475663.0]\n",
      "Header [DISK_READS_DELTA] - Location [530] - Value [16571933.0]\n",
      "Header [DISK_READS_DELTA] - Location [542] - Value [12592050.0]\n",
      "Header [DISK_READS_DELTA] - Location [554] - Value [16530907.0]\n",
      "Header [DISK_READS_DELTA] - Location [555] - Value [11517933.0]\n",
      "Header [DISK_READS_DELTA] - Location [578] - Value [14274095.0]\n",
      "Header [DISK_READS_DELTA] - Location [579] - Value [12428769.0]\n",
      "Header [DISK_READS_DELTA] - Location [602] - Value [12706339.0]\n",
      "Header [DISK_READS_DELTA] - Location [603] - Value [13604158.0]\n",
      "Header [DISK_READS_DELTA] - Location [615] - Value [16512600.0]\n",
      "Header [DISK_READS_DELTA] - Location [627] - Value [16380367.0]\n",
      "Header [DISK_READS_DELTA] - Location [628] - Value [11096796.0]\n",
      "Header [DISK_READS_DELTA] - Location [639] - Value [13860723.0]\n",
      "Header [DISK_READS_DELTA] - Location [640] - Value [11474228.0]\n",
      "Header [DISK_READS_DELTA] - Location [651] - Value [13039136.0]\n",
      "Header [DISK_READS_DELTA] - Location [652] - Value [11394674.0]\n",
      "Header [DISK_READS_DELTA] - Location [663] - Value [10691585.0]\n",
      "Header [DISK_READS_DELTA] - Location [664] - Value [11983862.0]\n",
      "Header [DISK_READS_DELTA] - Location [676] - Value [13697103.0]\n",
      "Header [DISK_READS_DELTA] - Location [689] - Value [13471057.0]\n",
      "Header [DISK_READS_DELTA] - Location [690] - Value [11278836.0]\n",
      "Header [DISK_READS_DELTA] - Location [707] - Value [11308656.0]\n",
      "Header [DISK_READS_DELTA] - Location [750] - Value [12843415.0]\n",
      "Header [DISK_READS_DELTA] - Location [793] - Value [16902690.0]\n",
      "Header [DISK_READS_DELTA] - Location [794] - Value [11681097.0]\n",
      "Header [DISK_READS_DELTA] - Location [830] - Value [15544066.0]\n",
      "Header [DISK_READS_DELTA] - Location [831] - Value [14361448.0]\n",
      "Header [DISK_READS_DELTA] - Location [843] - Value [12817525.0]\n",
      "Header [DISK_READS_DELTA] - Location [855] - Value [16432675.0]\n",
      "Header [DISK_READS_DELTA] - Location [856] - Value [11828275.0]\n",
      "Header [DISK_READS_DELTA] - Location [879] - Value [11378175.0]\n",
      "Header [DISK_READS_DELTA] - Location [903] - Value [13266662.0]\n",
      "Header [DISK_READS_DELTA] - Location [915] - Value [10575152.0]\n",
      "Header [DISK_READS_DELTA] - Location [941] - Value [11962886.0]\n",
      "Header [DISK_READS_DELTA] - Location [965] - Value [10894847.0]\n",
      "Header [DISK_READS_DELTA] - Location [977] - Value [10947280.0]\n",
      "Header [DISK_READS_DELTA] - Location [990] - Value [13122552.0]\n",
      "Header [DISK_READS_DELTA] - Location [1101] - Value [10889035.0]\n",
      "Header [DISK_READS_DELTA] - Location [1162] - Value [16487614.0]\n",
      "Header [DISK_READS_DELTA] - Location [1163] - Value [14502012.0]\n",
      "Header [DISK_READS_DELTA] - Location [1224] - Value [10946771.0]\n",
      "Header [DISK_READS_DELTA] - Location [1309] - Value [11547399.0]\n",
      "Header [DISK_READS_DELTA] - Location [1333] - Value [10825078.0]\n",
      "Header [DISK_READS_DELTA] - Location [1334] - Value [16868008.0]\n",
      "Header [DISK_READS_DELTA] - Location [1408] - Value [15693459.0]\n",
      "Header [DISK_READS_DELTA] - Location [1409] - Value [14117822.0]\n",
      "Header [DISK_READS_DELTA] - Location [1433] - Value [17064027.0]\n",
      "Header [DISK_READS_DELTA] - Location [1434] - Value [12586308.0]\n",
      "Header [DISK_READS_DELTA] - Location [1457] - Value [19203575.0]\n",
      "Header [DISK_READS_DELTA] - Location [1458] - Value [18565757.0]\n",
      "Header [DISK_READS_DELTA] - Location [1495] - Value [11821197.0]\n",
      "Header [DISK_READS_DELTA] - Location [1569] - Value [16524539.0]\n",
      "Header [DISK_READS_DELTA] - Location [1826] - Value [10627948.0]\n",
      "Header [DISK_READS_DELTA] - Location [2153] - Value [12274539.0]\n",
      "Header [DISK_READS_DELTA] - Location [2675] - Value [12779302.0]\n",
      "Header [DISK_READS_DELTA] - Location [4544] - Value [12104182.0]\n",
      "Header [DISK_READS_DELTA] - Location [6936] - Value [13117554.0]\n",
      "Header [DISK_READS_DELTA] - Location [6984] - Value [11549883.0]\n",
      "Header [DISK_READS_DELTA] - Location [6996] - Value [11214730.0]\n",
      "Header [DISK_READS_DELTA] - Location [7032] - Value [13821153.0]\n",
      "Header [DISK_READS_DELTA] - Location [7044] - Value [12864114.0]\n",
      "Header [DISK_READS_DELTA] - Location [7057] - Value [13420023.0]\n",
      "Header [DISK_READS_DELTA] - Location [7069] - Value [13197962.0]\n",
      "Header [DISK_READS_DELTA] - Location [7093] - Value [11878438.0]\n",
      "Header [DISK_READS_DELTA] - Location [7104] - Value [10634892.0]\n",
      "Header [DISK_READS_DELTA] - Location [7105] - Value [12082813.0]\n",
      "Header [DISK_READS_DELTA] - Location [7140] - Value [15022428.0]\n",
      "Header [DISK_READS_DELTA] - Location [7141] - Value [14072692.0]\n",
      "Header [DISK_READS_DELTA] - Location [7152] - Value [12075638.0]\n",
      "Header [DISK_READS_DELTA] - Location [7153] - Value [10348197.0]\n",
      "Header [DISK_READS_DELTA] - Location [7176] - Value [16261414.0]\n",
      "Header [DISK_READS_DELTA] - Location [7247] - Value [12573970.0]\n",
      "Header [DISK_READS_DELTA] - Location [7321] - Value [11899429.0]\n",
      "Header [DISK_READS_DELTA] - Location [7322] - Value [10720933.0]\n",
      "Header [DISK_READS_DELTA] - Location [7345] - Value [14063899.0]\n",
      "Header [DISK_READS_DELTA] - Location [7346] - Value [11946425.0]\n",
      "Header [DISK_READS_DELTA] - Location [7357] - Value [19577858.0]\n",
      "Header [DISK_READS_DELTA] - Location [7358] - Value [16678992.0]\n",
      "Header [DISK_READS_DELTA] - Location [7359] - Value [10284127.0]\n",
      "Header [DISK_READS_DELTA] - Location [7671] - Value [14457319.0]\n",
      "Header [DISK_READS_DELTA] - Location [7672] - Value [11036659.0]\n",
      "Header [DISK_READS_DELTA] - Location [7779] - Value [13118818.0]\n",
      "Header [DISK_READS_DELTA] - Location [7838] - Value [10446294.0]\n",
      "Header [DISK_READS_DELTA] - Location [7850] - Value [15393309.0]\n",
      "Header [DISK_READS_DELTA] - Location [7851] - Value [13738181.0]\n",
      "Header [DISK_READS_DELTA] - Location [7898] - Value [12699475.0]\n",
      "Header [DISK_READS_DELTA] - Location [7969] - Value [15315440.0]\n",
      "Header [DISK_READS_DELTA] - Location [7970] - Value [14392915.0]\n",
      "Header [DISK_READS_DELTA] - Location [7993] - Value [15814647.0]\n",
      "Header [DISK_READS_DELTA] - Location [7994] - Value [13315548.0]\n",
      "Header [DISK_READS_DELTA] - Location [8006] - Value [11355418.0]\n",
      "Header [DISK_READS_DELTA] - Location [8007] - Value [17171464.0]\n",
      "Header [DISK_READS_DELTA] - Location [8018] - Value [12654334.0]\n",
      "Header [DISK_READS_DELTA] - Location [8019] - Value [19744533.0]\n",
      "Header [DISK_READS_DELTA] - Location [8020] - Value [13133659.0]\n",
      "Header [DISK_READS_DELTA] - Location [8031] - Value [13486586.0]\n",
      "Header [DISK_READS_DELTA] - Location [8043] - Value [17792033.0]\n",
      "Header [DISK_READS_DELTA] - Location [8055] - Value [13148305.0]\n",
      "Header [DISK_READS_DELTA] - Location [8067] - Value [16504214.0]\n",
      "Header [DISK_READS_DELTA] - Location [8078] - Value [11337422.0]\n",
      "Header [DISK_READS_DELTA] - Location [8079] - Value [19711090.0]\n",
      "Header [DISK_READS_DELTA] - Location [8080] - Value [13759981.0]\n",
      "Header [DISK_READS_DELTA] - Location [8091] - Value [16701798.0]\n",
      "Header [DISK_READS_DELTA] - Location [8103] - Value [18877102.0]\n",
      "Header [DISK_READS_DELTA] - Location [8104] - Value [14251987.0]\n",
      "Header [DISK_READS_DELTA] - Location [8127] - Value [12677681.0]\n",
      "Header [DISK_READS_DELTA] - Location [8139] - Value [14888841.0]\n",
      "Header [DISK_READS_DELTA] - Location [8152] - Value [16162313.0]\n",
      "Header [DISK_READS_DELTA] - Location [8153] - Value [12487669.0]\n",
      "Header [DISK_READS_DELTA] - Location [8189] - Value [15590889.0]\n",
      "Header [DISK_READS_DELTA] - Location [8190] - Value [12905805.0]\n",
      "Header [DISK_READS_DELTA] - Location [8201] - Value [16149511.0]\n",
      "Header [DISK_READS_DELTA] - Location [8202] - Value [16056332.0]\n",
      "Header [DISK_READS_DELTA] - Location [8213] - Value [14764635.0]\n",
      "Header [DISK_READS_DELTA] - Location [8214] - Value [12795467.0]\n",
      "Header [DISK_READS_DELTA] - Location [8225] - Value [18922357.0]\n",
      "Header [DISK_READS_DELTA] - Location [8226] - Value [16563933.0]\n",
      "Header [DISK_READS_DELTA] - Location [8237] - Value [11589098.0]\n",
      "Header [DISK_READS_DELTA] - Location [8249] - Value [14254770.0]\n",
      "Header [DISK_READS_DELTA] - Location [8250] - Value [12158566.0]\n",
      "Header [DISK_READS_DELTA] - Location [8261] - Value [17850680.0]\n",
      "Header [DISK_READS_DELTA] - Location [8262] - Value [15411597.0]\n",
      "Header [DISK_READS_DELTA] - Location [8274] - Value [13522703.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header [DISK_READS_DELTA] - Location [8286] - Value [10473765.0]\n",
      "Header [DISK_READS_DELTA] - Location [8298] - Value [11339663.0]\n",
      "Header [DISK_READS_DELTA] - Location [8299] - Value [10567382.0]\n",
      "Header [DISK_READS_DELTA] - Location [8310] - Value [15629304.0]\n",
      "Header [DISK_READS_DELTA] - Location [8311] - Value [14419606.0]\n",
      "Header [DISK_READS_DELTA] - Location [8322] - Value [15570563.0]\n",
      "Header [DISK_READS_DELTA] - Location [8323] - Value [15068273.0]\n",
      "Header [DISK_READS_DELTA] - Location [8346] - Value [11855392.0]\n",
      "Header [DISK_READS_DELTA] - Location [8360] - Value [13012363.0]\n",
      "Header [DISK_READS_DELTA] - Location [8361] - Value [16360999.0]\n",
      "Header [DISK_READS_DELTA] - Location [8372] - Value [10723982.0]\n",
      "Header [DISK_READS_DELTA] - Location [8373] - Value [14324850.0]\n",
      "Header [DISK_READS_DELTA] - Location [8397] - Value [14403636.0]\n",
      "Header [DISK_READS_DELTA] - Location [8409] - Value [10470676.0]\n",
      "Header [DISK_READS_DELTA] - Location [8421] - Value [13449747.0]\n",
      "Header [DISK_READS_DELTA] - Location [8422] - Value [11785940.0]\n",
      "Header [DISK_READS_DELTA] - Location [8433] - Value [15550468.0]\n",
      "Header [DISK_READS_DELTA] - Location [8434] - Value [12750701.0]\n",
      "Header [DISK_READS_DELTA] - Location [8445] - Value [11140906.0]\n",
      "Header [DISK_READS_DELTA] - Location [8457] - Value [12454857.0]\n",
      "Header [DISK_READS_DELTA] - Location [8458] - Value [12715864.0]\n",
      "Header [DISK_READS_DELTA] - Location [8469] - Value [17919256.0]\n",
      "Header [DISK_READS_DELTA] - Location [8470] - Value [17112527.0]\n",
      "DF with outliers: (8489, 510)\n",
      "CPU_TIME_DELTA - [0] outliers\n",
      "DISK_READS_DELTA - [226] outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF with edited outliers: (8489, 510)\n"
     ]
    }
   ],
   "source": [
    "def get_outliers_quartile(df=None, headers=None):\n",
    "    \"\"\"\n",
    "    Detect and return which rows are considered outliers within the dataset, determined by :quartile_limit (99%)\n",
    "    \"\"\"\n",
    "    outlier_rows = [] # This list of lists consists of elements of the following notation [column,rowid]\n",
    "    for header in headers:\n",
    "        outlier_count = 0\n",
    "        try:\n",
    "            q25, q75 = np.percentile(df[header], 5), np.percentile(df[header], 95)\n",
    "            iqr = q75 - q25\n",
    "            cut_off = iqr * .6 # This values needs to remain as it. It was found to be a good value so as to capture the relavent outlier data\n",
    "            lower, upper = q25 - cut_off, q75 + cut_off\n",
    "           \n",
    "            series_row = (df[df[header] > upper].index)\n",
    "            outlier_count += len(list(np.array(series_row)))\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "           \n",
    "            series_row = (df[df[header] < lower].index)\n",
    "            outlier_count += len(list(np.array(series_row)))\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "            print(header + ' - [' + str(outlier_count) + '] outliers')\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "   \n",
    "    unique_outlier_rows = []\n",
    "    for col, rowid in outlier_rows:\n",
    "        unique_outlier_rows.append([col,rowid])\n",
    "    return unique_outlier_rows\n",
    "\n",
    "#Printing outliers to screen\n",
    "outliers = get_outliers_quartile(df=df,\n",
    "                                 headers=y_label)\n",
    "print('Total Outliers: [' + str(len(outliers)) + ']\\n')\n",
    "for label in y_label:\n",
    "    min_val = df[label].min()\n",
    "    max_val = df[label].max()\n",
    "    mean_val = df[label].mean()\n",
    "    std_val = df[label].std()\n",
    "    print('Label[' + label + '] - Min[' + str(min_val) + '] - Max[' + str(max_val) + '] - Mean[' + str(mean_val) + '] - Std[' + str(std_val) + ']')\n",
    "print('\\n---------------------------------------------\\n')\n",
    "for i in range(len(outliers)):\n",
    "    print('Header [' + str(outliers[i][0]) + '] - Location [' + str(outliers[i][1]) + '] - Value [' + str(df.iloc[outliers[i][1]][outliers[i][0]]) + ']')\n",
    "    \n",
    "def edit_outliers(df=None, headers=None):\n",
    "    \"\"\"\n",
    "    This method uses the interquartile method to edit all outliers to std.\n",
    "    \"\"\"\n",
    "    outliers = get_outliers_quartile(df=df,\n",
    "                                     headers=y_label)\n",
    "    for label in y_label:\n",
    "        min_val = df[label].min()\n",
    "        max_val = df[label].max()\n",
    "        mean_val = df[label].mean()\n",
    "        std_val = df[label].std()\n",
    "       \n",
    "        for i in range(len(outliers)):\n",
    "            if label == outliers[i][0]:\n",
    "                df[label].iloc[outliers[i][1]] = mean_val + std_val\n",
    "                # print('Header [' + str(outliers[i][0]) + '] - Location [' + str(outliers[i][1]) + '] - Value [' + str(df.iloc[outliers[i][1]][outliers[i][0]]) + ']')\n",
    "    return df\n",
    "\n",
    "print(\"DF with outliers: \" + str(df.shape))\n",
    "df = edit_outliers(df=df,\n",
    "                   headers=y_label)\n",
    "print(\"DF with edited outliers: \" + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Relavent Sources:\n",
    "\n",
    "* http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf\n",
    "* https://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a normalization preprocessing technique for data distribution that can closely approximate minimum and maximum observable values per column:\n",
    "\n",
    "<i>\"Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\"</i>\n",
    "\n",
    "Normalization formula is stated as follows: $$y=(x-min)/(max-min)$$\n",
    "\n",
    "### Standardization\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a standardization preprocessing technique for data distributions that observe a Gaussian spread, with a mean of 0 and a standard deviation of 1 (approximately close to these values):\n",
    "\n",
    "<i>\"Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\"</i>\n",
    "\n",
    "Standardization formula is stated as follows: $$y=(x-mean)/StandardDeviation$$\n",
    "Mean defined as: $$mean=sum(x)/count(x)$$\n",
    "Standard Deviation defined as: $$StandardDeviation=sqrt(sum((x-mean)^2)/count(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------BEFORE------------------\n",
      "------------------DF------------------\n",
      "(8489, 510)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------AFTER------------------\n",
      "------------------df------------------\n",
      "(8489, 510)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "df\n",
      "    SNAP_ID  OPTIMIZER_COST  SHARABLE_MEM  FETCHES_TOTAL  FETCHES_DELTA  \\\n",
      "0  0.000000    1.848303e-09      0.624616       0.279998       0.050271   \n",
      "1  0.000118    4.117774e-02      0.639240       0.334331       0.022320   \n",
      "2  0.000236    4.117775e-02      0.721480       0.332960       0.273457   \n",
      "3  0.000353    4.117775e-02      0.726337       0.333472       0.310146   \n",
      "4  0.000471    4.117775e-02      0.783259       0.334133       0.281408   \n",
      "\n",
      "   END_OF_FETCH_COUNT_TOTAL  END_OF_FETCH_COUNT_DELTA  SORTS_TOTAL  \\\n",
      "0                  0.104127                  0.028154     0.034384   \n",
      "1                  0.323886                  0.033323     0.329807   \n",
      "2                  0.301042                  0.317426     0.335798   \n",
      "3                  0.307513                  0.376597     0.337816   \n",
      "4                  0.314154                  0.340069     0.336113   \n",
      "\n",
      "   SORTS_DELTA  EXECUTIONS_TOTAL                 ...                  \\\n",
      "0     0.863990          0.081173                 ...                   \n",
      "1     0.065415          0.330263                 ...                   \n",
      "2     0.352655          0.311817                 ...                   \n",
      "3     0.403335          0.316890                 ...                   \n",
      "4     0.338731          0.322274                 ...                   \n",
      "\n",
      "   USER CALLS  USER COMMITS  USER LOGONS CUMULATIVE  USER LOGOUTS CUMULATIVE  \\\n",
      "0    0.000000      0.000000                0.000000                 0.000000   \n",
      "1    0.000019      0.000053                0.000019                 0.000019   \n",
      "2    0.000200      0.000103                0.000252                 0.000245   \n",
      "3    0.000402      0.000153                0.000525                 0.000518   \n",
      "4    0.000593      0.000212                0.000773                 0.000774   \n",
      "\n",
      "   USER ROLLBACKS  WORKAREA EXECUTIONS - ONEPASS  \\\n",
      "0        0.000000                       0.000000   \n",
      "1        0.000000                       0.000038   \n",
      "2        0.000006                       0.000225   \n",
      "3        0.000006                       0.000601   \n",
      "4        0.000081                       0.000826   \n",
      "\n",
      "   WORKAREA EXECUTIONS - OPTIMAL  WORKAREA MEMORY ALLOCATED  \\\n",
      "0                       0.000000                   0.148949   \n",
      "1                       0.000024                   0.015848   \n",
      "2                       0.000226                   0.522153   \n",
      "3                       0.000466                   0.500720   \n",
      "4                       0.000683                   0.000000   \n",
      "\n",
      "   WRITE CLONES CREATED IN BACKGROUND  WRITE CLONES CREATED IN FOREGROUND  \n",
      "0                                 0.0                            0.999173  \n",
      "1                                 0.0                            0.999173  \n",
      "2                                 0.0                            0.999174  \n",
      "3                                 0.0                            0.999174  \n",
      "4                                 0.0                            0.999174  \n",
      "\n",
      "[5 rows x 510 columns]\n"
     ]
    }
   ],
   "source": [
    "class Normalizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_scaler(dataframe):\n",
    "        \"\"\"\n",
    "        Normalize df using interquartile ranges as min-max, this way outliers do not play a heavy emphasis on the\n",
    "        normalization of values.\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.robust_scale(dataframe.values)\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "    @staticmethod\n",
    "    def minmax_scaler(dataframe):\n",
    "        \"\"\"\n",
    "        Normalize df using min-max ranges for normalization method\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.minmax_scale(dataframe.values, feature_range=(0, 1))\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(dataframe):\n",
    "        \"\"\"\n",
    "        The normalizer scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.normalize(dataframe.values)\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "print('------------------BEFORE------------------')\n",
    "print('------------------DF------------------')\n",
    "print(df.shape)\n",
    "print('\\n')\n",
    "#print(df.head())\n",
    "#\n",
    "# ROBUST SCALER\n",
    "# df = Normalizer.robust_scaler(dataframe=df)\n",
    "#\n",
    "# MINMAX SCALER\n",
    "df = Normalizer.minmax_scaler(dataframe=df)\n",
    "#\n",
    "# NORMALIZER\n",
    "#df = Normalizer.normalize(dataframe=df)\n",
    "\n",
    "print('\\n\\n------------------AFTER------------------')\n",
    "print('------------------df------------------')\n",
    "print(df.shape)\n",
    "print('\\n\\n')\n",
    "print('\\n\\ndf')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging Labels\n",
    "\n",
    "Removes the label column, and adds it at the beginning of the matrix for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ['CPU_TIME_DELTA', 'DISK_READS_DELTA'] shape: (8489, 2)\n",
      "Feature matrix shape: (8489, 510)\n",
      "    SNAP_ID  OPTIMIZER_COST  SHARABLE_MEM  FETCHES_TOTAL  FETCHES_DELTA  \\\n",
      "0  0.000000    1.848303e-09      0.624616       0.279998       0.050271   \n",
      "1  0.000118    4.117774e-02      0.639240       0.334331       0.022320   \n",
      "2  0.000236    4.117775e-02      0.721480       0.332960       0.273457   \n",
      "3  0.000353    4.117775e-02      0.726337       0.333472       0.310146   \n",
      "4  0.000471    4.117775e-02      0.783259       0.334133       0.281408   \n",
      "\n",
      "   END_OF_FETCH_COUNT_TOTAL  END_OF_FETCH_COUNT_DELTA  SORTS_TOTAL  \\\n",
      "0                  0.104127                  0.028154     0.034384   \n",
      "1                  0.323886                  0.033323     0.329807   \n",
      "2                  0.301042                  0.317426     0.335798   \n",
      "3                  0.307513                  0.376597     0.337816   \n",
      "4                  0.314154                  0.340069     0.336113   \n",
      "\n",
      "   SORTS_DELTA  EXECUTIONS_TOTAL                 ...                  \\\n",
      "0     0.863990          0.081173                 ...                   \n",
      "1     0.065415          0.330263                 ...                   \n",
      "2     0.352655          0.311817                 ...                   \n",
      "3     0.403335          0.316890                 ...                   \n",
      "4     0.338731          0.322274                 ...                   \n",
      "\n",
      "   USER CALLS  USER COMMITS  USER LOGONS CUMULATIVE  USER LOGOUTS CUMULATIVE  \\\n",
      "0    0.000000      0.000000                0.000000                 0.000000   \n",
      "1    0.000019      0.000053                0.000019                 0.000019   \n",
      "2    0.000200      0.000103                0.000252                 0.000245   \n",
      "3    0.000402      0.000153                0.000525                 0.000518   \n",
      "4    0.000593      0.000212                0.000773                 0.000774   \n",
      "\n",
      "   USER ROLLBACKS  WORKAREA EXECUTIONS - ONEPASS  \\\n",
      "0        0.000000                       0.000000   \n",
      "1        0.000000                       0.000038   \n",
      "2        0.000006                       0.000225   \n",
      "3        0.000006                       0.000601   \n",
      "4        0.000081                       0.000826   \n",
      "\n",
      "   WORKAREA EXECUTIONS - OPTIMAL  WORKAREA MEMORY ALLOCATED  \\\n",
      "0                       0.000000                   0.148949   \n",
      "1                       0.000024                   0.015848   \n",
      "2                       0.000226                   0.522153   \n",
      "3                       0.000466                   0.500720   \n",
      "4                       0.000683                   0.000000   \n",
      "\n",
      "   WRITE CLONES CREATED IN BACKGROUND  WRITE CLONES CREATED IN FOREGROUND  \n",
      "0                                 0.0                            0.999173  \n",
      "1                                 0.0                            0.999173  \n",
      "2                                 0.0                            0.999174  \n",
      "3                                 0.0                            0.999174  \n",
      "4                                 0.0                            0.999174  \n",
      "\n",
      "[5 rows x 510 columns]\n"
     ]
    }
   ],
   "source": [
    "y_df = df[y_label]\n",
    "X_df = df\n",
    "#X_df = df.drop(columns=y_label)\n",
    "print(\"Label \" + str(y_label) + \" shape: \" + str(y_df.shape))\n",
    "print(\"Feature matrix shape: \" + str(X_df.shape))\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Training\n",
    "\n",
    "This section converts the established features from the continuous domain into the discrete domain. Continous values will be converted into discrete, and used to train the model using such values (Utilizes bucket function).\n",
    "\n",
    "https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU y:\n",
      "[0 1]\n",
      "4766\n",
      "3723\n",
      "I/O y:\n",
      "[0 1]\n",
      "5091\n",
      "3398\n",
      "      CPU_TIME_DELTA  DISK_READS_DELTA\n",
      "0                  0                 0\n",
      "1                  0                 0\n",
      "2                  1                 1\n",
      "3                  1                 1\n",
      "4                  1                 1\n",
      "5                  0                 0\n",
      "6                  0                 0\n",
      "7                  1                 1\n",
      "8                  1                 1\n",
      "9                  1                 1\n",
      "10                 0                 0\n",
      "11                 0                 0\n",
      "12                 0                 0\n",
      "13                 0                 0\n",
      "14                 0                 1\n",
      "15                 1                 1\n",
      "16                 1                 1\n",
      "17                 0                 0\n",
      "18                 0                 0\n",
      "19                 1                 1\n",
      "20                 1                 1\n",
      "21                 1                 1\n",
      "22                 0                 0\n",
      "23                 0                 0\n",
      "24                 0                 0\n",
      "25                 0                 0\n",
      "26                 0                 0\n",
      "27                 1                 1\n",
      "28                 1                 1\n",
      "29                 0                 1\n",
      "...              ...               ...\n",
      "8459               1                 1\n",
      "8460               0                 0\n",
      "8461               0                 0\n",
      "8462               0                 0\n",
      "8463               0                 0\n",
      "8464               1                 1\n",
      "8465               1                 1\n",
      "8466               1                 1\n",
      "8467               0                 0\n",
      "8468               0                 0\n",
      "8469               1                 1\n",
      "8470               1                 1\n",
      "8471               1                 1\n",
      "8472               0                 0\n",
      "8473               0                 0\n",
      "8474               0                 0\n",
      "8475               0                 0\n",
      "8476               1                 1\n",
      "8477               1                 1\n",
      "8478               1                 1\n",
      "8479               0                 0\n",
      "8480               0                 0\n",
      "8481               1                 1\n",
      "8482               1                 1\n",
      "8483               1                 1\n",
      "8484               0                 0\n",
      "8485               0                 0\n",
      "8486               0                 0\n",
      "8487               0                 0\n",
      "8488               1                 1\n",
      "\n",
      "[8489 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "class BinClass:\n",
    "    \"\"\"\n",
    "    Takes data column, and scales them into discrete buckets. Parameter 'n' denotes number of buckets. This class needs\n",
    "    to be defined before the LSTM class, since it is referenced during the prediction stage. Since Keras models output a\n",
    "    continuous output (even when trained on discrete data), the 'BinClass' is required by the LSTM class.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def __bucket_val(val, avg):\n",
    "        \"\"\"\n",
    "        Receives threshold value and buckets the val according to the passed threshold\n",
    "        \"\"\"\n",
    "        return np.where(val > avg, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def discretize_value(X, threshold):\n",
    "        \"\"\"\n",
    "        param: X - Input data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            myfunc_vec = np.vectorize(lambda x: BinClass.__bucket_val(x, threshold))\n",
    "            return myfunc_vec(X)\n",
    "        except:\n",
    "            return BinClass.__bucket_val(X, threshold)\n",
    "\n",
    "cpu_avg = y_df[y_label[0]].mean()\n",
    "y_df_cpu = pd.DataFrame(BinClass.discretize_value(y_df[[y_label[0]]].values, cpu_avg), columns=[y_label[0]])\n",
    "print('CPU y:')\n",
    "print(np.unique(y_df_cpu.values))\n",
    "print(np.count_nonzero(y_df_cpu == 0))\n",
    "print(np.count_nonzero(y_df_cpu == 1))\n",
    "#\n",
    "io_avg = y_df[y_label[1]].mean()\n",
    "y_df_io = pd.DataFrame(BinClass.discretize_value(y_df[[y_label[1]]].values, io_avg), columns=[y_label[1]])\n",
    "print('I/O y:')\n",
    "print(np.unique(y_df_io.values))\n",
    "print(np.count_nonzero(y_df_io == 0))\n",
    "print(np.count_nonzero(y_df_io == 1))\n",
    "#\n",
    "y_df[y_label[0]] = y_df_cpu\n",
    "y_df[y_label[1]] = y_df_io\n",
    "print(y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['var1(t+1)', 'var2(t+1)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+2)', 'var2(t+2)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+3)', 'var2(t+3)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+4)', 'var2(t+4)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+5)', 'var2(t+5)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+6)', 'var2(t+6)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+7)', 'var2(t+7)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+8)', 'var2(t+8)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+9)', 'var2(t+9)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+10)', 'var2(t+10)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+11)', 'var2(t+11)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+12)', 'var2(t+12)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['var1(t+13)', 'var2(t+13)'], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index([], dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-13)', 'var2(t-13)', 'var3(t-13)', 'var4(t-13)', 'var5(t-13)',\n",
      "       'var6(t-13)', 'var7(t-13)', 'var8(t-13)', 'var9(t-13)', 'var10(t-13)',\n",
      "       ...\n",
      "       'var501(t+13)', 'var502(t+13)', 'var503(t+13)', 'var504(t+13)',\n",
      "       'var505(t+13)', 'var506(t+13)', 'var507(t+13)', 'var508(t+13)',\n",
      "       'var509(t+13)', 'var510(t+13)'],\n",
      "      dtype='object', length=13744)\n",
      "(8463, 13744)\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var2(t+1)', 'var1(t+2)', 'var2(t+2)', 'var1(t+3)',\n",
      "       'var2(t+3)', 'var1(t+4)', 'var2(t+4)', 'var1(t+5)', 'var2(t+5)',\n",
      "       'var1(t+6)', 'var2(t+6)', 'var1(t+7)', 'var2(t+7)', 'var1(t+8)',\n",
      "       'var2(t+8)', 'var1(t+9)', 'var2(t+9)', 'var1(t+10)', 'var2(t+10)',\n",
      "       'var1(t+11)', 'var2(t+11)', 'var1(t+12)', 'var2(t+12)', 'var1(t+13)',\n",
      "       'var2(t+13)'],\n",
      "      dtype='object')\n",
      "(8463, 26)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #     for header in headers:\n",
    "    #         if \"(t)\" in header:\n",
    "    #             dropped_headers.append(header)\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for header in headers:\n",
    "            if \"(t+\" + str(i) + \")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "\n",
    "    return df.drop(dropped_headers, axis=1)\n",
    "\n",
    "\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "\n",
    "# Separate labels from features\n",
    "y_row = []\n",
    "for i in range(lag + 1, (lag * 2) + 2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns) * i:len(df.columns) * i + len(y_label)]\n",
    "    y_row.append(y_df_column_names)\n",
    "    print(y_df_column_names)\n",
    "    print(type(y_df_column_names))\n",
    "y_df_column_names = []\n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "\n",
    "# # Delete middle timesteps\n",
    "# X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "# print('\\n-------------\\nFeatures After Time Shift')\n",
    "# print(X_df.columns)\n",
    "# print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "# print('\\n-------------\\nLabels After Time Shift')\n",
    "# print(y_df.columns)\n",
    "# print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Implements a recursive solution, where in features are eliminated based on an ensemble evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['var511(t-1)' 'var511(t)'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c14660f61b33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m  \u001b[1;34m'var504(t)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'var505(t)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'var506(t)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'var507(t)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'var508(t)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'var509(t)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m  'var510(t)', 'var511(t)']\n\u001b[1;32m--> 124\u001b[1;33m \u001b[0mX_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrecursively_eliminated_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2726\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['var511(t-1)' 'var511(t)'] not in index\""
     ]
    }
   ],
   "source": [
    "class FeatureEliminator:\n",
    "    \"\"\"\n",
    "    This class is dedicated to housing logic pertaining to feature selection - retaining only labels which are considered\n",
    "    important.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_df, y_df):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        :param X_df: (Pandas) Pandas feature matrix.\n",
    "        :param y_df: (Pandas) Pandas label matrix.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__X_df = X_df\n",
    "        self.__y_df = y_df\n",
    "    \n",
    "    def rfe_selector(self, test_split=.4, optimum_feature_count=0, parallel_degree=1, max_depth=None, max_features='sqrt', n_estimators=100):\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination Function. Isolates and eliminated features one by one, up till the desired amount, starting\n",
    "        by features which are considered less important.\n",
    "        :param test_split:            (Float) Denotes training/testing data split.\n",
    "        :param optimum_feature_count: (Integer) Denotes the best estimated number of features to retain before a performance drop\n",
    "                                                is estimated.\n",
    "        :param parallel_degree:       (Integer) Denotes model training parallel degree.\n",
    "        :param max_depth:             (Integer) Denotes number of leaves to evaluate during decision tree pruning.\n",
    "        :param max_features:          (Integer) Denotes number of features to consider during random subselection.\n",
    "        :param n_estimators:          (Integer) Number of estimators (trees) to build for decision making.\n",
    "        :return: (List) This list is composed of boolean values, which correspond to the input feature column headers. True List \n",
    "                        values denote columns which have been retained. False values denote eliminated feature headers.\n",
    "        :return: (List) This list denotes feature rankings, which correspond to the input feature column headers. Values of '1',\n",
    "                        denote that features have been retained.\n",
    "        \"\"\"\n",
    "        X_df = self.__X_df.values\n",
    "        y_df = self.__y_df[self.__y_df.columns[0]].values  # We can only use a single target column since RandomForests do not support multi target labels\n",
    "        print(X_df.shape)\n",
    "        print(y_df.shape)\n",
    "        optimum_feature_count = int(optimum_feature_count)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, \n",
    "                                                            y_df, \n",
    "                                                            test_size=test_split)\n",
    "        model = RandomForestRegressor(n_estimators=int(n_estimators), \n",
    "                                      n_jobs=parallel_degree,\n",
    "                                      max_depth=max_depth,\n",
    "                                      max_features='sqrt')\n",
    "\n",
    "        # create the RFE model and select N attributes\n",
    "        rfe_model = RFE(model, optimum_feature_count, step=1)\n",
    "        rfe_model = rfe_model.fit(X_train, y_train)\n",
    "\n",
    "        # summarize the selection of the attributes\n",
    "        print(rfe_model.support_)\n",
    "        print(rfe_model.ranking_)\n",
    "\n",
    "        # evaluate the model on testing set\n",
    "        pred_y = rfe_model.predict(X_test)\n",
    "        predictions = [round(value) for value in pred_y]\n",
    "        r2s = r2_score(y_test, predictions)\n",
    "        \n",
    "        return rfe_model.support_, rfe_model.ranking_\n",
    "    \n",
    "    def get_selected_features(self, column_mask):\n",
    "        \"\"\"\n",
    "        Retrieves features which have not been eliminated from the RFE function.\n",
    "        :param column_mask: (List) This list is composed of boolean values, which correspond to the input feature column headers. \n",
    "                                   True list values denote columns which have been retained. False values denote eliminated \n",
    "                                   feature headers. \n",
    "        :return: (Pandas) Pandas data matrix.\n",
    "        \"\"\"\n",
    "        recommended_columns = []\n",
    "        for i in range(len(self.__X_df.columns)):\n",
    "            if (column_mask[i]):\n",
    "                recommended_columns.append(self.__X_df.columns[i])\n",
    "                \n",
    "        return self.__X_df[recommended_columns]\n",
    "    \n",
    "# fe = FeatureEliminator(X_df=X_df,\n",
    "#                        y_df=y_df)\n",
    "# column_mask, column_rankings = fe.rfe_selector(test_split=test_split,\n",
    "#                                                optimum_feature_count=int(X_df.shape[1]/8),\n",
    "#                                                parallel_degree=2,\n",
    "#                                                max_depth=1,\n",
    "#                                                max_features='sqrt',\n",
    "#                                                n_estimators=n_estimators)\n",
    "# print(X_df.columns)\n",
    "# X_df = fe.get_selected_features(column_mask=column_mask)\n",
    "# print(X_df.columns)\n",
    "recursively_eliminated_columns = ['var422(t-1)', 'var423(t-1)', 'var424(t-1)', 'var425(t-1)', 'var426(t-1)',\n",
    " 'var427(t-1)', 'var428(t-1)', 'var429(t-1)', 'var430(t-1)', 'var431(t-1)',\n",
    " 'var432(t-1)', 'var433(t-1)', 'var434(t-1)', 'var435(t-1)', 'var436(t-1)',\n",
    " 'var437(t-1)', 'var438(t-1)', 'var439(t-1)', 'var440(t-1)', 'var441(t-1)',\n",
    " 'var442(t-1)', 'var443(t-1)', 'var444(t-1)', 'var445(t-1)', 'var446(t-1)',\n",
    " 'var447(t-1)', 'var448(t-1)', 'var449(t-1)', 'var450(t-1)', 'var451(t-1)',\n",
    " 'var452(t-1)', 'var453(t-1)', 'var454(t-1)', 'var455(t-1)', 'var456(t-1)',\n",
    " 'var457(t-1)', 'var458(t-1)', 'var459(t-1)', 'var460(t-1)', 'var461(t-1)',\n",
    " 'var462(t-1)', 'var463(t-1)', 'var464(t-1)', 'var465(t-1)', 'var466(t-1)',\n",
    " 'var467(t-1)', 'var468(t-1)', 'var469(t-1)', 'var470(t-1)', 'var471(t-1)',\n",
    " 'var472(t-1)', 'var473(t-1)', 'var474(t-1)', 'var475(t-1)', 'var476(t-1)',\n",
    " 'var477(t-1)', 'var478(t-1)', 'var479(t-1)', 'var480(t-1)', 'var481(t-1)',\n",
    " 'var482(t-1)', 'var483(t-1)', 'var484(t-1)', 'var485(t-1)', 'var486(t-1)',\n",
    " 'var487(t-1)', 'var488(t-1)', 'var489(t-1)', 'var490(t-1)', 'var491(t-1)',\n",
    " 'var492(t-1)', 'var493(t-1)', 'var494(t-1)', 'var495(t-1)', 'var496(t-1)',\n",
    " 'var497(t-1)', 'var498(t-1)', 'var499(t-1)', 'var500(t-1)', 'var501(t-1)',\n",
    " 'var502(t-1)', 'var503(t-1)', 'var504(t-1)', 'var505(t-1)', 'var506(t-1)',\n",
    " 'var507(t-1)', 'var508(t-1)', 'var509(t-1)', 'var510(t-1)', 'var511(t-1)',\n",
    " 'var1(t)', 'var2(t)', 'var3(t)', 'var4(t)', 'var6(t)', 'var7(t)', 'var8(t)',\n",
    " 'var9(t)', 'var10(t)', 'var11(t)', 'var12(t)', 'var13(t)', 'var16(t)',\n",
    " 'var17(t)', 'var18(t)', 'var19(t)', 'var20(t)', 'var21(t)', 'var24(t)',\n",
    " 'var25(t)', 'var26(t)', 'var28(t)', 'var32(t)', 'var37(t)', 'var38(t)',\n",
    " 'var48(t)', 'var53(t)', 'var54(t)', 'var56(t)', 'var57(t)', 'var67(t)',\n",
    " 'var80(t)', 'var124(t)', 'var146(t)', 'var162(t)', 'var194(t)', 'var197(t)',\n",
    " 'var199(t)', 'var200(t)', 'var201(t)', 'var202(t)', 'var208(t)', 'var211(t)',\n",
    " 'var212(t)', 'var213(t)', 'var214(t)', 'var222(t)', 'var226(t)', 'var227(t)',\n",
    " 'var230(t)', 'var231(t)', 'var235(t)', 'var240(t)', 'var243(t)', 'var245(t)',\n",
    " 'var247(t)', 'var248(t)', 'var249(t)', 'var250(t)', 'var252(t)', 'var254(t)',\n",
    " 'var259(t)', 'var266(t)', 'var267(t)', 'var273(t)', 'var275(t)', 'var278(t)',\n",
    " 'var280(t)', 'var282(t)', 'var286(t)', 'var288(t)', 'var292(t)', 'var293(t)',\n",
    " 'var294(t)', 'var296(t)', 'var299(t)', 'var301(t)', 'var304(t)', 'var306(t)',\n",
    " 'var307(t)', 'var308(t)', 'var309(t)', 'var310(t)', 'var311(t)', 'var314(t)',\n",
    " 'var318(t)', 'var319(t)', 'var320(t)', 'var323(t)', 'var327(t)', 'var329(t)',\n",
    " 'var330(t)', 'var333(t)', 'var334(t)', 'var335(t)', 'var336(t)', 'var337(t)',\n",
    " 'var338(t)', 'var339(t)', 'var342(t)', 'var344(t)', 'var416(t)', 'var493(t)',\n",
    " 'var504(t)', 'var505(t)', 'var506(t)', 'var507(t)', 'var508(t)', 'var509(t)',\n",
    " 'var510(t)', 'var511(t)']\n",
    "X_df = X_df[recursively_eliminated_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Decomposition\n",
    "\n",
    "Principal component analysis: Factor model in which the factors are based on summarizing the total variance. With PCA, unities are used in the diagonal of the correlation matrix computationally implying that all the variance is common or shared.\n",
    "\n",
    "https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrincipalComponentAnalysisClass:\n",
    "    \"\"\"\n",
    "    This class handles logic related to PCA data transformations.\n",
    "    https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
    "    \"\"\"\n",
    "    def __init__(self, X_df):\n",
    "        \"\"\"\n",
    "        Cosntructor method.\n",
    "        :param X_df: (Pandas) Dataframe consisting of input features, which will be subject to PCA.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__X_df = X_df\n",
    "        \n",
    "    def get_default_component_variances(self):\n",
    "        \"\"\"\n",
    "        Fitting the PCA algorithm with our Data.\n",
    "        :return: (Numpy array) Array of feature variances.\n",
    "        \"\"\"\n",
    "        pca = PCA().fit(self.__X_df.values)\n",
    "        return np.cumsum(pca.explained_variance_ratio_)\n",
    "        \n",
    "    def get_default_component_count(self, threshold=.99):\n",
    "        \"\"\"\n",
    "        Retrieves the recommended number of component decomposition, above which very little variance \n",
    "        gain is achieved. This treshold will be set at a 0.999 variance threshold.\n",
    "        :param threshold: (Float) Threshold value between 0 and 1. Stops immediately as soon the number\n",
    "                                  of required components exceeds the threshold value.\n",
    "        :return: (Integer) Returns the number of recommended components.\n",
    "        \"\"\"\n",
    "        variance_ratios = self.get_default_component_variances()\n",
    "        n = 0\n",
    "        for val in variance_ratios:\n",
    "            if val < threshold:\n",
    "                n += 1\n",
    "        return n\n",
    "    \n",
    "    def plot_variance_per_reduction(self):\n",
    "        \"\"\"\n",
    "        This method subjects the feature matrix to a PCA decomposition. The number of components is plot\n",
    "        vs the amount of retained variance.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        variance_ratios = self.get_default_component_variances()\n",
    "        \n",
    "        #Plotting the Cumulative Summation of the Explained Variance\n",
    "        plt.figure()\n",
    "        plt.plot(variance_ratios)\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Variance (%)') #for each component\n",
    "        plt.title(tpcds + ' Dataset Explained Variance')\n",
    "        plt.show()\n",
    "        \n",
    "    def apply_PCA(self, n_components):\n",
    "        \"\"\"\n",
    "        Applies Principle Component Analysis on the constructor passed data matrix, on a number of components.\n",
    "        A new pandas data matrix is returned, with renamed 'Principal Component' headers.\n",
    "        :param n_components: (Integer) Denotes number of component breakdown.\n",
    "        :return: (Pandas) Dataframe consisting of new decomposed components.\n",
    "        \"\"\"\n",
    "        pca = PCA(n_components=n_components)\n",
    "        dataset = pca.fit_transform(self.__X_df.values)\n",
    "        header_list = []\n",
    "        for i in range(dataset.shape[1]):\n",
    "            header_list.append('Component_' + str(i))\n",
    "        return pd.DataFrame(data=dataset, columns=header_list)\n",
    "\n",
    "# print(X_df.head())\n",
    "# print(X_df.shape)\n",
    "\n",
    "# pcac = PrincipalComponentAnalysisClass(X_df=X_df)\n",
    "# pcac.plot_variance_per_reduction()\n",
    "# component_count = pcac.get_default_component_count()\n",
    "# X_df = pcac.apply_PCA(n_components=component_count)\n",
    "\n",
    "# print('-'*30)\n",
    "# print(X_df.head())\n",
    "# print(X_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classification (Many to Many)\n",
    "### Designing the network\n",
    "\n",
    "- The first step is to define your network.\n",
    "- Neural networks are defined in Keras as a sequence of layers. The container for these layers is the **Sequential class**.\n",
    "- The first step is to create an instance of the Sequential class. Then you can create your layers and add them in the order that they should be connected.\n",
    "- The LSTM recurrent layer comprised of memory units is called LSTM().\n",
    "- A fully connected layer that often follows LSTM layers and is used for outputting a prediction is called Dense().\n",
    "- The first layer in the network must define the number of inputs to expect.\n",
    "- Input must be three-dimensional, comprised of samples, timesteps, and features.\n",
    "    - **Samples:** These are the rows in your data.\n",
    "    - **Timesteps:** These are the past observations for a feature, such as lag variables.\n",
    "    - **Features:** These are columns in your data.\n",
    "- Assuming your data is loaded as a NumPy array, you can convert a 2D dataset to a 3D dataset using the reshape() function in NumPy.\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "* https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "* https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/\n",
    "* https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "* https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Class\n",
    "class LSTM:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, lag, loss_func, activation, optimizer='sgd', lstm_layers=1, dropout=.0,\n",
    "                 stateful=False, y_labels=None, initializer='uniform'):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X             - (Numpy 2D Array) Training data used to train the model (Features).\n",
    "        :param y             - (Numpy 2D Array) Test data used to test the model (Labels\n",
    "        :param lag           - (Integer) Denotes lag step value\n",
    "        :param loss_function - (String)  Denotes mode of measure fitting of model (Fitting function).\n",
    "        :param activation    - (String)  Neuron activation function used to activate/trigger neurons.\n",
    "        :param optimizer     - (String)  Denotes which function to us to optimize the model build (eg: Gradient Descent).\n",
    "        :param lstm_layers   - (Integer) Denotes the number of LSTM layers to be included in the model build.\n",
    "        :param dropout       - (Float)   Denotes amount of dropout for model. This parameter must be a value between 0 and 1.\n",
    "        :param stateful      - (Boolean) Denotes whether state is used as initial state for next training batch.\n",
    "        :param: y_labels     - (List) List of target label names\n",
    "        :param: initializer  - (String)  String initializer which denotes starting weights.\n",
    "        \"\"\"\n",
    "        self.__lag = lag\n",
    "        self.__model = ke.models.Sequential()\n",
    "        self.__y_labels = y_labels\n",
    "\n",
    "        if dropout > 1 and dropout < 0:\n",
    "            raise ValueError('Dropout parameter exceeded! Must be a value between 0 and 1.')\n",
    "        \n",
    "        # self.__model.add(ke.layers.Embedding(2+1, 32, input_length=X.shape[1]))\n",
    "        for i in range(0, lstm_layers - 1):  # If lstm_layers == 1, this for loop logic is skipped.\n",
    "            if stateful:\n",
    "                if i == 0:\n",
    "                    self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                    batch_input_shape=(X.shape[0],\n",
    "                                                                       X.shape[1],\n",
    "                                                                       X.shape[2]),\n",
    "                                                    return_sequences=True,\n",
    "                                                    recurrent_dropout=dropout,\n",
    "                                                    recurrent_initializer=initializer,\n",
    "                                                    activation=activation,\n",
    "                                                    stateful=stateful))\n",
    "                else:\n",
    "                    self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                    input_shape=(X.shape[1],\n",
    "                                                                 X.shape[2]),\n",
    "                                                    return_sequences=True,\n",
    "                                                    recurrent_dropout=dropout,\n",
    "                                                    recurrent_initializer=initializer,\n",
    "                                                    activation=activation,\n",
    "                                                    stateful=stateful))\n",
    "            else:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                input_shape=(X.shape[1],\n",
    "                                                             X.shape[2]),\n",
    "                                                return_sequences=True,\n",
    "                                                recurrent_dropout=dropout,\n",
    "                                                recurrent_initializer=initializer,\n",
    "                                                activation=activation,\n",
    "                                                stateful=stateful))\n",
    "            self.__model.add(ke.layers.Dropout(dropout))\n",
    "        if lstm_layers > 1:\n",
    "            self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                            input_shape=(X.shape[1],\n",
    "                                                         X.shape[2]),\n",
    "                                            stateful=stateful,\n",
    "                                            recurrent_dropout=dropout,\n",
    "                                            recurrent_initializer=initializer,\n",
    "                                            activation=activation,\n",
    "                                            return_sequences=False))\n",
    "        else:\n",
    "            if stateful:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                batch_input_shape=(X.shape[0],\n",
    "                                                                   X.shape[1],\n",
    "                                                                   X.shape[2]),\n",
    "                                                stateful=stateful,\n",
    "                                                recurrent_dropout=dropout,\n",
    "                                                recurrent_initializer=initializer,\n",
    "                                                activation=activation,\n",
    "                                                return_sequences=False))\n",
    "            else:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                input_shape=(X.shape[1],\n",
    "                                                             X.shape[2]),\n",
    "                                                stateful=stateful,\n",
    "                                                recurrent_dropout=dropout,\n",
    "                                                recurrent_initializer=initializer,\n",
    "                                                activation=activation,\n",
    "                                                return_sequences=False))\n",
    "        self.__model.add(ke.layers.Dropout(dropout))\n",
    "        # self.__model.add(ke.layers.TimeDistributed(ke.layers.Dense(self.__lag * len(self.__y_labels), kernel_initializer=initializer)))\n",
    "        self.__model.add(ke.layers.Dense(self.__lag * len(self.__y_labels),\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='sigmoid'))\n",
    "        self.__model.compile(loss=loss_func, optimizer=optimizer, metrics=['mse','mae'])\n",
    "        print(self.__model.summary())\n",
    "\n",
    "    def fit_model(self, X_train=None, X_test=None, y_train=None, y_test=None, epochs=50, batch_size=50, verbose=2,\n",
    "                  shuffle=False, plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "\n",
    "        :param: X_train    - (Numpy 2D Array) Numpy matrix consisting of input training features\n",
    "        :param: X_test     - (Numpy 2D Array) Numpy matrix consisting of input validation/testing features\n",
    "        :param: y_train    - (Numpy 2D Array) Numpy matrix consisting of output training labels\n",
    "        :param: y_test     - (Numpy 2D Array) Numpy matrix consisting of output validation/testing labels\n",
    "        :param: epochs     - (Integer) Integer value denoting number of trained epochs\n",
    "        :param: batch_size - (Integer) Integer value denoting LSTM training batch_size\n",
    "        :param: verbose    - (Integer) Integer value denoting net verbosity (Amount of information shown to user during LSTM training)\n",
    "        :param: shuffle    - (Bool) Boolean value denoting whether or not to shuffle data. This parameter must always remain 'False' for time series datasets.\n",
    "        :param: plot       - (Bool) Boolean value denoting whether this function should plot out it's evaluation\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if X_test is not None and y_test is not None:\n",
    "            history = self.__model.fit(x=X_train,\n",
    "                                       y=y_train,\n",
    "                                       epochs=epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       validation_data=(X_test, y_test),\n",
    "                                       verbose=verbose,\n",
    "                                       shuffle=shuffle)\n",
    "        else:\n",
    "            history = self.__model.fit(x=X_train,\n",
    "                                       y=y_train,\n",
    "                                       epochs=epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       verbose=verbose,\n",
    "                                       shuffle=shuffle)\n",
    "\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(history.history['mean_squared_error'], label='mean_squared_error')\n",
    "            plt.plot(history.history['mean_absolute_error'], label='mean_absolute_error')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, X, batch_size):\n",
    "        \"\"\"\n",
    "        Predicts label/s from input feature 'X'\n",
    "        :param: X - Numpy matrix consisting of a single feature vector\n",
    "        :param: batch_size - (Integer) Denotes prediction batch size\n",
    "        :return: Numpy matrix of predicted label output\n",
    "        \"\"\"\n",
    "        yhat = self.__model.predict(X, batch_size=batch_size)\n",
    "        return yhat\n",
    "\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, batch, dropout, epoch, layer, activation, initializer,\n",
    "                              stateful, rmse, accuracy, f_score, time_train):\n",
    "        \"\"\"\n",
    "        Static method which is used for test harness utilities. This method attempts a grid search across many\n",
    "        trained LSTM models, each denoted with different configurations.\n",
    "\n",
    "        Attempted configurations:\n",
    "        * Varied data test split\n",
    "        * Varied batch sizes\n",
    "        * Varied epoch counts\n",
    "\n",
    "        Each configuration is denoted with a score, and used to identify the most optimal configuration.\n",
    "\n",
    "        :param: path       - (String) String denoting result csv output.\n",
    "        :param: iteration  - (Integer) Integer denoting test iteration (Unique per test configuration).\n",
    "        :param: lag        - (Integer) Denotes lag time shift\n",
    "        :param: test_split - (Float) Float denoting data sample sizes.\n",
    "        :param: batch      - (Integer) Integer denoting LSTM batch size.\n",
    "        :param: epoch      - (Integer) Integer denoting number of LSTM training iterations.\n",
    "        :param: layer      - (Integer) Integer denoting number of LSTM layers\n",
    "        :param: activation - (String) String denoting activation for LSTM layers.\n",
    "        :param: initializer- (String) String denoting LSTM initializing weights.\n",
    "        :param: stateful   - (Bool) Boolean flag which denotes whether LSTM model is trained in stateful mode or not.\n",
    "        :param: dropout    - (Float) Float denoting model dropout layer.\n",
    "        :param: rmse       - (Float) Float denoting experiment configuration RSME score.\n",
    "        :param: accuracy   - (Float) Float denoting experiment accuracy score.\n",
    "        :param: fscore     - (Float) Float denoting experiment fscore score.\n",
    "        :param: time_train - (Integer) Integer denoting number of seconds taken by LSTM training iteration.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a+') as csvfile:\n",
    "            headers = ['iteration', 'test_split', 'batch', 'epoch', 'layer', 'stateful', 'dropout', 'activation',\n",
    "                       'initializer', 'rmse', 'accuracy', 'f_score', 'time_train', 'lag']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'test_split': test_split,\n",
    "                             'batch': batch,\n",
    "                             'epoch': epoch,\n",
    "                             'layer': layer,\n",
    "                             'stateful': stateful,\n",
    "                             'dropout': dropout,\n",
    "                             'activation': activation,\n",
    "                             'initializer': initializer,\n",
    "                             'rmse': rmse,\n",
    "                             'accuracy': accuracy,\n",
    "                             'f_score': f_score,\n",
    "                             'time_train': time_train,\n",
    "                             'lag': lag})\n",
    "\n",
    "    @staticmethod\n",
    "    def lag_multiple(X, lag):\n",
    "        \"\"\"\n",
    "        Divides the total number of rows by the lag value, until a perfect multiple amount is retrieved.\n",
    "        :param X: (Numpy) 2D array consisting of input.\n",
    "        :param lag: (Integer) Denotes time shift value.\n",
    "        :return: (Numpy) 2D array consisting of a perfect lag multiple rows.\n",
    "        \"\"\"\n",
    "        n_rows = X.shape[0]\n",
    "        multiple = int(n_rows/lag)\n",
    "        max_new_rows = multiple * lag\n",
    "        return X[0:max_new_rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Training (Week Prior: Day 1 - 7)\n",
    "\n",
    "An LSTM model is trained on a week worth of data. Once the model is fit, it will then be used to establish predictions for upcoming days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=lag)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=lag)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=lag)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=lag)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / lag), lag, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / lag),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / lag), lag, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / lag),:]                    \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "   \n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='mean_squared_error',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layer,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             y_labels=y_label,\n",
    "             initializer=initializer)\n",
    "\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Validation: Day 8 - 14\n",
    "\n",
    "Evaluating model fit to day 1 forecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(io_avg)\n",
    "print(y_validate)\n",
    "n=7\n",
    "accuracy_per_day, f1score_per_day = [], []\n",
    "for i in range(0, n):\n",
    "    \n",
    "    print('Day ' + str(i+1))\n",
    "    \n",
    "    # Segregate data for specific day\n",
    "    X_validate_temp = X_validate[(int(X_validate.shape[0]/n)*i):(int(X_validate.shape[0]/n)*(i+1)),:]\n",
    "    y_validate_temp = y_validate[(int(y_validate.shape[0]/n)*i):(int(y_validate.shape[0]/n)*(i+1)),:]\n",
    "    print('Feature vectors: ' + str(X_validate_temp.shape))\n",
    "    print('Label vectors: ' + str(y_validate_temp.shape))\n",
    "\n",
    "    y_list, yhat_list = [], []\n",
    "    for i in range(0, X_validate_temp.shape[0]):\n",
    "        \n",
    "        X = np.array(np.array(X_validate_temp[i,:]))\n",
    "        X = X.reshape((int(X.shape[0] / lag), lag, X.shape[1]))\n",
    "        y = np.array(y_validate_temp[i, :])\n",
    "        yhat = model.predict(X, batch_size=batch)\n",
    "        \n",
    "        y = y.reshape(1,-1)\n",
    "        model.fit_model(X_train=X,\n",
    "                        y_train=y,\n",
    "                        epochs=2, \n",
    "                        batch_size=1,\n",
    "                        verbose=0, \n",
    "                        shuffle=False,\n",
    "                        plot=False) # Online Learning, Training on validation predictions. \n",
    "        \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "        \n",
    "        #y = BinClass.discretize_value(y\n",
    "        #print(yhat.shape)\n",
    "        for i in range(yhat.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                #print('CPU')\n",
    "                y[i] = BinClass.discretize_value(y[i], cpu_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], cpu_avg)\n",
    "            else:\n",
    "                #print('IO')\n",
    "                y[i] = BinClass.discretize_value(y[i], io_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], io_avg)\n",
    "        y_list.append(y)\n",
    "        yhat_list.append(yhat)\n",
    "        \n",
    "        print('Actual: ' + str(y))\n",
    "        print('Predicted: ' + str(yhat) + '\\n--------------------------')\n",
    "    \n",
    "    y_list = np.array(y_list)\n",
    "    yhat_list = np.array(yhat_list)\n",
    "    \n",
    "    acc_score_list, f1_score_list = [], []\n",
    "    for i in range(lag * len(y_label)):\n",
    "        print('Label: ' + str(i))\n",
    "        acc = accuracy_score(y_list[:,i],yhat_list[:,i])\n",
    "        f1 = f1_score(y_list[:,i],yhat_list[:,i], average='binary')\n",
    "        print('Accuracy: ' + str(acc) + '\\nF1Score: ' +  str(f1) + '\\n--------------------------')\n",
    "        acc_score_list.append(acc)\n",
    "        f1_score_list.append(f1)\n",
    "    accuracy_per_day.append(sum(acc_score_list)/len(acc_score_list))\n",
    "    f1score_per_day.append(sum(f1_score_list)/len(f1_score_list))\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring per day\n",
    "\n",
    "The following plot exhibits the general effectiveness of the model over the subsequent week upon which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.plot(accuracy_per_day, label='accuracy')\n",
    "plt.plot(f1score_per_day, label='f1score')\n",
    "plt.legend(['accuracy', 'f1score'], loc='upper left')\n",
    "plt.xlabel('Distribution over days')\n",
    "plt.ylabel('Score')\n",
    "plt.title(tpcds + ' model scoring over subsequent week')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
