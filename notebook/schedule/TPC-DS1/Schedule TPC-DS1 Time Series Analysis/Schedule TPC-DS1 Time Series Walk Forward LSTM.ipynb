{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Foward Evaluation\n",
    "\n",
    "This section mimics a real-life like approach, where a model is trained on a week worth of data, and then used to predict 7 days worth of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Installation and Importing Libraries\n",
    "\n",
    "* https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/\n",
    "* https://vertexai-plaidml.readthedocs-hosted.com/en/latest/installing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.16.1\n",
      "pandas: 0.24.1\n",
      "statsmodels: 0.9.0\n",
      "sklearn: 0.19.0\n",
      "theano: 1.0.3\n",
      "tensorflow: 1.11.0\n",
      "keras: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas.plotting import lag_plot\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# statsmodels\n",
    "import statsmodels\n",
    "print('statsmodels: %s' % statsmodels.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# theano\n",
    "import theano\n",
    "print('theano: %s' % theano.__version__)\n",
    "# tensorflow\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "# plaidml keras\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "# keras\n",
    "import keras as ke\n",
    "print('keras: %s' % ke.__version__)\n",
    "# math\n",
    "import math\n",
    "import csv\n",
    "import os.path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=12 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "nrows=None\n",
    "test_split=.5 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "sub_sample_start=350 # Denotes frist 0..n samples (Used for plotting purposes)\n",
    "y_label = ['CPU_TIME_DELTA', 'IOWAIT_DELTA']# Denotes which label to use for time series experiments\n",
    "\n",
    "# Feature Selection\n",
    "parallel_degree = -1\n",
    "n_estimators=100\n",
    "\n",
    "# LSTM Network Structure\n",
    "epochs=50\n",
    "batch=64\n",
    "activation='tanh'\n",
    "initializer='normal'\n",
    "dropout=0.4\n",
    "layer=2\n",
    "state=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-799e8fcd8f28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrep_hist_sysstat_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/rep_hist_sysstat.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mrep_hist_snapshot_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_snapshot_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mrep_hist_sysmetric_summary_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_sysmetric_summary_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mrep_hist_sysstat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_sysstat_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m     \"\"\"\n\u001b[0;32m    574\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Root path\n",
    "root_dir = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds\n",
    "#root_dir = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds\n",
    "\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = root_dir + '/rep_hist_snapshot.csv'\n",
    "rep_hist_sysmetric_summary_path = root_dir + '/rep_hist_sysmetric_summary.csv'\n",
    "rep_hist_sysstat_path = root_dir + '/rep_hist_sysstat.csv'\n",
    "\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path, nrows=nrows)\n",
    "rep_hist_sysmetric_summary_df = pd.read_csv(rep_hist_sysmetric_summary_path, nrows=nrows)\n",
    "rep_hist_sysstat_df = pd.read_csv(rep_hist_sysstat_path, nrows=nrows)\n",
    "\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "rep_hist_sysmetric_summary_df.columns = prettify_header(rep_hist_sysmetric_summary_df.columns.values)\n",
    "rep_hist_sysstat_df.columns = prettify_header(rep_hist_sysstat_df.columns.values)\n",
    "\n",
    "print(rep_hist_snapshot_df.columns.values)\n",
    "print(rep_hist_sysmetric_summary_df.columns.values)\n",
    "print(rep_hist_sysstat_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting Tables and Changing Matrix Shapes\n",
    "\n",
    "Changes all dataframe shapes to be similar to each other, where in a number of snap_id timestamps are cojoined with instance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Header Lengths [Before Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "\n",
    "# Table REP_HIST_SYSMETRIC_SUMMARY\n",
    "rep_hist_sysmetric_summary_df = rep_hist_sysmetric_summary_df.pivot(index='SNAP_ID', columns='METRIC_NAME', values='AVERAGE')\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "rep_hist_sysmetric_summary_df[['SNAP_ID']] = rep_hist_sysmetric_summary_df[['SNAP_ID']].astype(int)\n",
    "#rep_hist_sysmetric_summary_df = rep_hist_sysstat_df.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "rep_hist_sysmetric_summary_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "\n",
    "# Table REP_HIST_SYSSTAT\n",
    "rep_hist_sysstat_df = rep_hist_sysstat_df.pivot(index='SNAP_ID', columns='STAT_NAME', values='VALUE')\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "rep_hist_sysstat_df[['SNAP_ID']] = rep_hist_sysstat_df[['SNAP_ID']].astype(int)\n",
    "#rep_hist_sysstat_df = rep_hist_sysstat_df.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "rep_hist_sysstat_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "\n",
    "rep_hist_sysmetric_summary_df.rename(str.upper, inplace=True, axis='columns')\n",
    "rep_hist_sysstat_df.rename(str.upper, inplace=True, axis='columns')\n",
    "\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT)\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID','DBID','INSTANCE_NUMBER']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "\n",
    "print('\\nHeader Lengths [After Pivot]')\n",
    "print('REP_HIST_SNAPSHOT: ' + str(len(rep_hist_snapshot_df.columns)))\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY: ' + str(len(rep_hist_sysmetric_summary_df.columns)))\n",
    "print('REP_HIST_SYSSTAT: ' + str(len(rep_hist_sysstat_df.columns)))\n",
    "\n",
    "# DF Shape\n",
    "print('\\nDataframe shapes:\\nTable [REP_HIST_SNAPSHOT] - ' + str(rep_hist_snapshot_df.shape))\n",
    "print('Table [REP_HIST_SYSMETRIC_SUMMARY] - ' + str(rep_hist_sysmetric_summary_df.shape))\n",
    "print('Table [REP_HIST_SYSSTAT] - ' + str(rep_hist_sysstat_df.shape) + '\\n')\n",
    "\n",
    "print(rep_hist_snapshot_df.columns.values)\n",
    "print(rep_hist_sysmetric_summary_df.columns.values)\n",
    "print(rep_hist_sysstat_df.columns.values)\n",
    "print(rep_hist_snapshot_df.shape)\n",
    "print(rep_hist_sysmetric_summary_df.shape)\n",
    "print(rep_hist_sysstat_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Empty Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "\n",
    "print('N/A Columns\\n')\n",
    "print('\\n REP_HIST_SNAPSHOT Features ' + str(len(rep_hist_snapshot_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_snapshot_df,headers=rep_hist_snapshot_df.columns)) + \"\\n\")\n",
    "print('REP_HIST_SYSMETRIC_SUMMARY Features ' + str(len(rep_hist_sysmetric_summary_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_sysmetric_summary_df,headers=rep_hist_sysmetric_summary_df.columns)) + \"\\n\")\n",
    "print('REP_HIST_SYSSTAT Features ' + str(len(rep_hist_sysstat_df.columns)) + ': ' + str(get_na_columns(df=rep_hist_sysstat_df,headers=rep_hist_sysstat_df.columns)) + \"\\n\")\n",
    "\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Populating NaN values with amount '0'\n",
    "rep_hist_snapshot_df = fill_na(df=rep_hist_snapshot_df)\n",
    "rep_hist_sysmetric_summary_df = fill_na(df=rep_hist_sysmetric_summary_df)\n",
    "rep_hist_sysstat_df = fill_na(df=rep_hist_sysstat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Frames\n",
    "\n",
    "This part merges the following pandas data frame into a single frame:\n",
    "* REP_HIST_SNAPSHOT\n",
    "* REP_HIST_SYSMETRIC_SUMMARY\n",
    "* REP_HIST_SYSSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print\n",
    "df = pd.merge(rep_hist_snapshot_df, rep_hist_sysmetric_summary_df,how='inner',on ='SNAP_ID')\n",
    "df = pd.merge(df, rep_hist_sysstat_df,how='inner',on ='SNAP_ID')\n",
    "print(df.shape)\n",
    "print('----------------------------------')\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['SNAP_ID'], ascending=True, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point precision conversion\n",
    "\n",
    "Each column is converted into a column of type values which are floating point for higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype('float32', inplace=True)\n",
    "df = np.round(df, 3) # rounds to 3 dp\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundant Feature Removal\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "\n",
    "print('Before column drop:')\n",
    "print(df.shape)\n",
    "df = drop_flatline_columns(df=df)\n",
    "print('\\nAfter flatline column drop:')\n",
    "print(df.shape)\n",
    "dropped_columns_df = [ 'PLAN_HASH_VALUE',\n",
    "                       'OPTIMIZER_ENV_HASH_VALUE',\n",
    "                       'LOADED_VERSIONS',\n",
    "                       'VERSION_COUNT',\n",
    "                       'PARSING_SCHEMA_ID',\n",
    "                       'PARSING_USER_ID',\n",
    "                       'CON_DBID',\n",
    "                       'SNAP_LEVEL',\n",
    "                       'SNAP_FLAG',\n",
    "                       'COMMAND_TYPE']\n",
    "df.drop(columns=dropped_columns_df, inplace=True)\n",
    "print('\\nAfter additional column drop:')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection - Standard Deviation Method\n",
    "\n",
    "Detection and transformation of outliers, categorized as more than 3 standard deviations away.\n",
    "\n",
    "If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers.\n",
    "\n",
    "The Gaussian distribution has the property that the standard deviation from the mean can be used to reliably summarize the percentage of values in the sample.\n",
    "\n",
    "For example, within one standard deviation of the mean will cover 68% of the data.\n",
    "\n",
    "So, if the mean is 50 and the standard deviation is 5, as in the test dataset above, then all data in the sample between 45 and 55 will account for about 68% of the data sample. We can cover more of the data sample if we expand the range as follows:\n",
    "\n",
    "* 1 Standard Deviation from the Mean: 68%\n",
    "* 2 Standard Deviations from the Mean: 95%\n",
    "* 3 Standard Deviations from the Mean: 99.7%\n",
    "\n",
    "A value that falls outside of 3 standard deviations is part of the distribution, but it is an unlikely or rare event at approximately 1 in 370 samples.\n",
    "\n",
    "Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution. For smaller samples of data, perhaps a value of 2 standard deviations (95%) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9%) can be used.\n",
    "\n",
    "More infor here: https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_outliers_quartile(df=None, headers=None):\n",
    "#     \"\"\"\n",
    "#     Detect and return which rows are considered outliers within the dataset, determined by :quartile_limit (99%)\n",
    "#     \"\"\"\n",
    "#     outlier_rows = [] # This list of lists consists of elements of the following notation [column,rowid]\n",
    "#     for header in headers:\n",
    "#         outlier_count = 0\n",
    "#         try:\n",
    "#             q25, q75 = np.percentile(df[header], 5), np.percentile(df[header], 95)\n",
    "#             iqr = q75 - q25\n",
    "#             cut_off = iqr * .6 # This values needs to remain as it. It was found to be a good value so as to capture the relavent outlier data\n",
    "#             lower, upper = q25 - cut_off, q75 + cut_off\n",
    "           \n",
    "#             series_row = (df[df[header] > upper].index)\n",
    "#             outlier_count += len(list(np.array(series_row)))\n",
    "#             for id in list(np.array(series_row)):\n",
    "#                 outlier_rows.append([header,id])\n",
    "           \n",
    "#             series_row = (df[df[header] < lower].index)\n",
    "#             outlier_count += len(list(np.array(series_row)))\n",
    "#             for id in list(np.array(series_row)):\n",
    "#                 outlier_rows.append([header,id])\n",
    "#             print(header + ' - [' + str(outlier_count) + '] outliers')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "   \n",
    "#     unique_outlier_rows = []\n",
    "#     for col, rowid in outlier_rows:\n",
    "#         unique_outlier_rows.append([col,rowid])\n",
    "#     return unique_outlier_rows\n",
    "\n",
    "# #Printing outliers to screen\n",
    "# outliers = get_outliers_quartile(df=df,\n",
    "#                                  headers=y_label)\n",
    "# print('Total Outliers: [' + str(len(outliers)) + ']\\n')\n",
    "# for label in y_label:\n",
    "#     min_val = df[label].min()\n",
    "#     max_val = df[label].max()\n",
    "#     mean_val = df[label].mean()\n",
    "#     std_val = df[label].std()\n",
    "#     print('Label[' + label + '] - Min[' + str(min_val) + '] - Max[' + str(max_val) + '] - Mean[' + str(mean_val) + '] - Std[' + str(std_val) + ']')\n",
    "# print('\\n---------------------------------------------\\n')\n",
    "# for i in range(len(outliers)):\n",
    "#     print('Header [' + str(outliers[i][0]) + '] - Location [' + str(outliers[i][1]) + '] - Value [' + str(df.iloc[outliers[i][1]][outliers[i][0]]) + ']')\n",
    "    \n",
    "# def edit_outliers(df=None, headers=None):\n",
    "#     \"\"\"\n",
    "#     This method uses the interquartile method to edit all outliers to std.\n",
    "#     \"\"\"\n",
    "#     outliers = get_outliers_quartile(df=df,\n",
    "#                                      headers=y_label)\n",
    "#     for label in y_label:\n",
    "#         min_val = df[label].min()\n",
    "#         max_val = df[label].max()\n",
    "#         mean_val = df[label].mean()\n",
    "#         std_val = df[label].std()\n",
    "       \n",
    "#         for i in range(len(outliers)):\n",
    "#             if label == outliers[i][0]:\n",
    "#                 df[label].iloc[outliers[i][1]] = std_val + mean_val\n",
    "#                 #df[label].iloc[outliers[i][1]] = min_val\n",
    "#                 # print('Header [' + str(outliers[i][0]) + '] - Location [' + str(outliers[i][1]) + '] - Value [' + str(df.iloc[outliers[i][1]][outliers[i][0]]) + ']')\n",
    "#     return df\n",
    "\n",
    "# print(\"DF with outliers: \" + str(df.shape))\n",
    "# df = edit_outliers(df=df,\n",
    "#                    headers=y_label)\n",
    "# print(\"DF with edited outliers: \" + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Relavent Sources:\n",
    "\n",
    "* http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf\n",
    "* https://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a normalization preprocessing technique for data distribution that can closely approximate minimum and maximum observable values per column:\n",
    "\n",
    "<i>\"Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\"</i>\n",
    "\n",
    "Normalization formula is stated as follows: $$y=(x-min)/(max-min)$$\n",
    "\n",
    "### Standardization\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a standardization preprocessing technique for data distributions that observe a Gaussian spread, with a mean of 0 and a standard deviation of 1 (approximately close to these values):\n",
    "\n",
    "<i>\"Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\"</i>\n",
    "\n",
    "Standardization formula is stated as follows: $$y=(x-mean)/StandardDeviation$$\n",
    "Mean defined as: $$mean=sum(x)/count(x)$$\n",
    "Standard Deviation defined as: $$StandardDeviation=sqrt(sum((x-mean)^2)/count(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_scaler(dataframe):\n",
    "        \"\"\"\n",
    "        Normalize df using interquartile ranges as min-max, this way outliers do not play a heavy emphasis on the\n",
    "        normalization of values.\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.robust_scale(dataframe.values)\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "    @staticmethod\n",
    "    def minmax_scaler(dataframe):\n",
    "        \"\"\"\n",
    "        Normalize df using min-max ranges for normalization method\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.minmax_scale(dataframe.values, feature_range=(0, 1))\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(dataframe):\n",
    "        \"\"\"\n",
    "        The normalizer scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.normalize(dataframe.values)\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "print('------------------BEFORE------------------')\n",
    "print('------------------DF------------------')\n",
    "print(df.shape)\n",
    "print('\\n')\n",
    "#print(df.head())\n",
    "#\n",
    "# ROBUST SCALER\n",
    "# df = Normalizer.robust_scaler(dataframe=df)\n",
    "#\n",
    "# MINMAX SCALER\n",
    "df = Normalizer.minmax_scaler(dataframe=df)\n",
    "#\n",
    "# NORMALIZER\n",
    "#df = Normalizer.normalize(dataframe=df)\n",
    "\n",
    "print('\\n\\n------------------AFTER------------------')\n",
    "print('------------------df------------------')\n",
    "print(df.shape)\n",
    "print('\\n\\n')\n",
    "print('\\n\\ndf')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging Labels\n",
    "\n",
    "Removes the label column, and adds it at the beginning of the matrix for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = df[y_label]\n",
    "X_df = df\n",
    "print(\"Label \" + str(y_label) + \" shape: \" + str(y_df.shape))\n",
    "print(\"Feature matrix shape: \" + str(X_df.shape))\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Training\n",
    "\n",
    "This section converts the established features from the continuous domain into the discrete domain. Continous values will be converted into discrete, and used to train the model using such values (Utilizes bucket function).\n",
    "\n",
    "https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinClass:\n",
    "    \"\"\"\n",
    "    Takes data column, and scales them into discrete buckets. Parameter 'n' denotes number of buckets. This class needs\n",
    "    to be defined before the LSTM class, since it is referenced during the prediction stage. Since Keras models output a\n",
    "    continuous output (even when trained on discrete data), the 'BinClass' is required by the LSTM class.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def __bucket_val(val, avg):\n",
    "        \"\"\"\n",
    "        Receives threshold value and buckets the val according to the passed threshold\n",
    "        \"\"\"\n",
    "        return np.where(val > avg, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def discretize_value(X, threshold):\n",
    "        \"\"\"\n",
    "        param: X - Input data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            myfunc_vec = np.vectorize(lambda x: BinClass.__bucket_val(x, threshold))\n",
    "            return myfunc_vec(X)\n",
    "        except:\n",
    "            return BinClass.__bucket_val(X, threshold)\n",
    "\n",
    "cpu_avg = y_df[y_label[0]].mean()\n",
    "y_df_cpu = pd.DataFrame(BinClass.discretize_value(y_df[[y_label[0]]].values, cpu_avg), columns=[y_label[0]])\n",
    "print('CPU y:')\n",
    "print(np.unique(y_df_cpu.values))\n",
    "print('Number of 0s: ' + str(np.count_nonzero(y_df_cpu == 0)))\n",
    "print('Number of 1s: ' + str(np.count_nonzero(y_df_cpu == 1)))\n",
    "#\n",
    "io_avg = y_df[y_label[1]].mean()\n",
    "y_df_io = pd.DataFrame(BinClass.discretize_value(y_df[[y_label[1]]].values, io_avg), columns=[y_label[1]])\n",
    "print('I/O y:')\n",
    "print(np.unique(y_df_io.values))\n",
    "print('Number of 0s: ' + str(np.count_nonzero(y_df_io == 0)))\n",
    "print('Number of 1s: ' + str(np.count_nonzero(y_df_io == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #     for header in headers:\n",
    "    #         if \"(t)\" in header:\n",
    "    #             dropped_headers.append(header)\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for header in headers:\n",
    "            if \"(t+\" + str(i) + \")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "\n",
    "    return df.drop(dropped_headers, axis=1)\n",
    "\n",
    "\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "\n",
    "# Separate labels from features\n",
    "y_row = []\n",
    "for i in range(lag + 1, (lag * 2) + 2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns) * i:len(df.columns) * i + len(y_label)]\n",
    "    y_row.append(y_df_column_names)\n",
    "    print(y_df_column_names)\n",
    "    print(type(y_df_column_names))\n",
    "y_df_column_names = []\n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df\n",
    "# X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "\n",
    "# # Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "# print('\\n-------------\\nFeatures After Time Shift')\n",
    "# print(X_df.columns)\n",
    "# print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "# print('\\n-------------\\nLabels After Time Shift')\n",
    "# print(y_df.columns)\n",
    "# print(y_df.shape)\n",
    "\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Implements a recursive solution, where in features are eliminated based on an ensemble evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEliminator:\n",
    "    \"\"\"\n",
    "    This class is dedicated to housing logic pertaining to feature selection - retaining only labels which are considered\n",
    "    important.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_df, y_df):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        :param X_df: (Pandas) Pandas feature matrix.\n",
    "        :param y_df: (Pandas) Pandas label matrix.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__X_df = X_df\n",
    "        self.__y_df = y_df\n",
    "    \n",
    "    def rfe_selector(self, test_split=.4, optimum_feature_count=0, parallel_degree=1, max_depth=None, max_features='sqrt', n_estimators=100):\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination Function. Isolates and eliminated features one by one, up till the desired amount, starting\n",
    "        by features which are considered less important.\n",
    "        :param test_split:            (Float) Denotes training/testing data split.\n",
    "        :param optimum_feature_count: (Integer) Denotes the best estimated number of features to retain before a performance drop\n",
    "                                                is estimated.\n",
    "        :param parallel_degree:       (Integer) Denotes model training parallel degree.\n",
    "        :param max_depth:             (Integer) Denotes number of leaves to evaluate during decision tree pruning.\n",
    "        :param max_features:          (Integer) Denotes number of features to consider during random subselection.\n",
    "        :param n_estimators:          (Integer) Number of estimators (trees) to build for decision making.\n",
    "        :return: (List) This list is composed of boolean values, which correspond to the input feature column headers. True List \n",
    "                        values denote columns which have been retained. False values denote eliminated feature headers.\n",
    "        :return: (List) This list denotes feature rankings, which correspond to the input feature column headers. Values of '1',\n",
    "                        denote that features have been retained.\n",
    "        \"\"\"\n",
    "        X_df = self.__X_df.values\n",
    "        y_df = self.__y_df[self.__y_df.columns[0]].values  # We can only use a single target column since RandomForests do not support multi target labels\n",
    "        print(X_df.shape)\n",
    "        print(y_df.shape)\n",
    "        optimum_feature_count = int(optimum_feature_count)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, \n",
    "                                                            y_df, \n",
    "                                                            test_size=test_split)\n",
    "        model = RandomForestRegressor(n_estimators=int(n_estimators), \n",
    "                                      n_jobs=parallel_degree,\n",
    "                                      max_depth=max_depth,\n",
    "                                      max_features='sqrt')\n",
    "\n",
    "        # create the RFE model and select N attributes\n",
    "        rfe_model = RFE(model, optimum_feature_count, step=1)\n",
    "        rfe_model = rfe_model.fit(X_train, y_train)\n",
    "\n",
    "        # summarize the selection of the attributes\n",
    "        print(rfe_model.support_)\n",
    "        print(rfe_model.ranking_)\n",
    "\n",
    "        # evaluate the model on testing set\n",
    "        pred_y = rfe_model.predict(X_test)\n",
    "        predictions = [round(value) for value in pred_y]\n",
    "        r2s = r2_score(y_test, predictions)\n",
    "        \n",
    "        return rfe_model.support_, rfe_model.ranking_\n",
    "    \n",
    "    def get_selected_features(self, column_mask):\n",
    "        \"\"\"\n",
    "        Retrieves features which have not been eliminated from the RFE function.\n",
    "        :param column_mask: (List) This list is composed of boolean values, which correspond to the input feature column headers. \n",
    "                                   True list values denote columns which have been retained. False values denote eliminated \n",
    "                                   feature headers. \n",
    "        :return: (Pandas) Pandas data matrix.\n",
    "        \"\"\"\n",
    "        recommended_columns = []\n",
    "        for i in range(len(self.__X_df.columns)):\n",
    "            if (column_mask[i]):\n",
    "                recommended_columns.append(self.__X_df.columns[i])\n",
    "                \n",
    "        return self.__X_df[recommended_columns]\n",
    "    \n",
    "# fe = FeatureEliminator(X_df=X_df,\n",
    "#                        y_df=y_df)\n",
    "# column_mask, column_rankings = fe.rfe_selector(test_split=test_split,\n",
    "#                                                optimum_feature_count=int(X_df.shape[1]/8),\n",
    "#                                                parallel_degree=2,\n",
    "#                                                max_depth=1,\n",
    "#                                                max_features='sqrt',\n",
    "#                                                n_estimators=n_estimators)\n",
    "# print(X_df.columns)\n",
    "# X_df = fe.get_selected_features(column_mask=column_mask)\n",
    "# print(X_df.columns)\n",
    "recursively_eliminated_columns = ['var175(t-4)', 'var176(t-4)', 'var177(t-4)', 'var178(t-4)', 'var179(t-4)', 'var180(t-4)', 'var181(t-4)', 'var182(t-4)', 'var183(t-4)', 'var184(t-4)', 'var185(t-4)', 'var186(t-4)', 'var187(t-4)', 'var188(t-4)', 'var189(t-4)', 'var190(t-4)', 'var191(t-4)', 'var192(t-4)', 'var193(t-4)', 'var195(t-4)', 'var196(t-4)', 'var197(t-4)', 'var198(t-4)', 'var199(t-4)', 'var200(t-4)', 'var201(t-4)', 'var202(t-4)', 'var203(t-4)', 'var204(t-4)', 'var205(t-4)', 'var206(t-4)', 'var207(t-4)', 'var208(t-4)', 'var209(t-4)', 'var210(t-4)', 'var211(t-4)', 'var212(t-4)', 'var213(t-4)', 'var214(t-4)', 'var215(t-4)', 'var217(t-4)', 'var218(t-4)', 'var219(t-4)', 'var220(t-4)', 'var221(t-4)', 'var222(t-4)', 'var223(t-4)', 'var224(t-4)', 'var225(t-4)', 'var226(t-4)', 'var227(t-4)', 'var228(t-4)', 'var229(t-4)', 'var230(t-4)', 'var235(t-4)', 'var238(t-4)', 'var240(t-4)', 'var241(t-4)', 'var243(t-4)', 'var244(t-4)', 'var245(t-4)', 'var246(t-4)', 'var247(t-4)', 'var248(t-4)', 'var250(t-4)', 'var252(t-4)', 'var253(t-4)', 'var254(t-4)', 'var255(t-4)', 'var256(t-4)', 'var258(t-4)', 'var259(t-4)', 'var260(t-4)', 'var261(t-4)', 'var266(t-4)', 'var267(t-4)', 'var268(t-4)', 'var269(t-4)', 'var271(t-4)', 'var274(t-4)', 'var275(t-4)', 'var276(t-4)', 'var277(t-4)', 'var279(t-4)', 'var282(t-4)', 'var283(t-4)', 'var284(t-4)', 'var285(t-4)', 'var286(t-4)', 'var288(t-4)', 'var289(t-4)', 'var291(t-4)', 'var292(t-4)', 'var293(t-4)', 'var294(t-4)', 'var297(t-4)', 'var298(t-4)', 'var299(t-4)', 'var301(t-4)', 'var303(t-4)', 'var304(t-4)', 'var308(t-4)', 'var309(t-4)', 'var310(t-4)', 'var311(t-4)', 'var312(t-4)', 'var313(t-4)', 'var314(t-4)', 'var315(t-4)', 'var316(t-4)', 'var317(t-4)', 'var318(t-4)', 'var319(t-4)', 'var320(t-4)', 'var321(t-4)', 'var324(t-4)', 'var325(t-4)', 'var326(t-4)', 'var327(t-4)', 'var328(t-4)', 'var329(t-4)', 'var335(t-4)', 'var336(t-4)', 'var337(t-4)', 'var338(t-4)', 'var339(t-4)', 'var342(t-4)', 'var343(t-4)', 'var344(t-4)', 'var345(t-4)', 'var346(t-4)', 'var347(t-4)', 'var348(t-4)', 'var349(t-4)', 'var350(t-4)', 'var352(t-4)', 'var353(t-4)', 'var355(t-4)', 'var356(t-4)', 'var357(t-4)', 'var358(t-4)', 'var361(t-4)', 'var363(t-4)', 'var366(t-4)', 'var367(t-4)', 'var368(t-4)', 'var369(t-4)', 'var370(t-4)', 'var371(t-4)', 'var372(t-4)', 'var374(t-4)', 'var376(t-4)', 'var377(t-4)', 'var378(t-4)', 'var379(t-4)', 'var383(t-4)', 'var385(t-4)', 'var386(t-4)', 'var389(t-4)', 'var390(t-4)', 'var391(t-4)', 'var392(t-4)', 'var393(t-4)', 'var394(t-4)', 'var397(t-4)', 'var401(t-4)', 'var402(t-4)', 'var403(t-4)', 'var404(t-4)', 'var405(t-4)', 'var406(t-4)', 'var407(t-4)', 'var408(t-4)', 'var409(t-4)', 'var410(t-4)', 'var411(t-4)', 'var412(t-4)', 'var413(t-4)', 'var414(t-4)', 'var415(t-4)', 'var416(t-4)', 'var417(t-4)', 'var418(t-4)', 'var419(t-4)', 'var420(t-4)', 'var421(t-4)', 'var422(t-4)', 'var423(t-4)', 'var424(t-4)', 'var425(t-4)', 'var426(t-4)', 'var427(t-4)', 'var428(t-4)', 'var429(t-4)', 'var430(t-4)', 'var431(t-4)', 'var432(t-4)', 'var433(t-4)', 'var434(t-4)', 'var435(t-4)', 'var436(t-4)', 'var437(t-4)', 'var438(t-4)', 'var439(t-4)', 'var440(t-4)', 'var441(t-4)', 'var442(t-4)', 'var443(t-4)', 'var444(t-4)', 'var445(t-4)', 'var446(t-4)', 'var447(t-4)', 'var448(t-4)', 'var449(t-4)', 'var450(t-4)', 'var451(t-4)', 'var452(t-4)', 'var453(t-4)', 'var455(t-4)', 'var456(t-4)', 'var457(t-4)', 'var458(t-4)', 'var459(t-4)', 'var460(t-4)', 'var461(t-4)', 'var462(t-4)', 'var463(t-4)', 'var464(t-4)', 'var465(t-4)', 'var466(t-4)', 'var468(t-4)', 'var469(t-4)', 'var470(t-4)', 'var471(t-4)', 'var472(t-4)', 'var473(t-4)', 'var474(t-4)', 'var475(t-4)', 'var476(t-4)', 'var477(t-4)', 'var478(t-4)', 'var479(t-4)', 'var480(t-4)', 'var481(t-4)', 'var482(t-4)', 'var483(t-4)', 'var484(t-4)', 'var485(t-4)', 'var486(t-4)', 'var487(t-4)', 'var488(t-4)', 'var489(t-4)', 'var490(t-4)', 'var491(t-4)', 'var492(t-4)', 'var493(t-4)', 'var494(t-4)', 'var495(t-4)', 'var496(t-4)', 'var497(t-4)', 'var498(t-4)', 'var499(t-4)', 'var500(t-4)', 'var501(t-4)', 'var502(t-4)', 'var503(t-4)', 'var504(t-4)', 'var506(t-4)', 'var507(t-4)', 'var511(t-4)', 'var1(t-3)', 'var2(t-3)', 'var3(t-3)', 'var4(t-3)', 'var5(t-3)', 'var7(t-3)', 'var8(t-3)', 'var11(t-3)', 'var12(t-3)', 'var13(t-3)', 'var14(t-3)', 'var16(t-3)', 'var19(t-3)', 'var20(t-3)', 'var22(t-3)', 'var23(t-3)', 'var25(t-3)', 'var28(t-3)', 'var30(t-3)', 'var31(t-3)', 'var37(t-3)', 'var39(t-3)', 'var41(t-3)', 'var42(t-3)', 'var45(t-3)', 'var47(t-3)', 'var48(t-3)', 'var50(t-3)', 'var52(t-3)', 'var53(t-3)', 'var54(t-3)', 'var58(t-3)', 'var59(t-3)', 'var61(t-3)', 'var62(t-3)', 'var63(t-3)', 'var66(t-3)', 'var67(t-3)', 'var68(t-3)', 'var70(t-3)', 'var72(t-3)', 'var75(t-3)', 'var77(t-3)', 'var78(t-3)', 'var79(t-3)', 'var80(t-3)', 'var86(t-3)', 'var88(t-3)', 'var89(t-3)', 'var91(t-3)', 'var92(t-3)', 'var94(t-3)', 'var95(t-3)', 'var96(t-3)', 'var98(t-3)', 'var99(t-3)', 'var100(t-3)', 'var101(t-3)', 'var102(t-3)', 'var103(t-3)', 'var106(t-3)', 'var108(t-3)', 'var109(t-3)', 'var114(t-3)', 'var115(t-3)', 'var116(t-3)', 'var117(t-3)', 'var119(t-3)', 'var120(t-3)', 'var121(t-3)', 'var122(t-3)', 'var123(t-3)', 'var124(t-3)', 'var126(t-3)', 'var127(t-3)', 'var132(t-3)', 'var133(t-3)', 'var138(t-3)', 'var144(t-3)', 'var145(t-3)', 'var148(t-3)', 'var150(t-3)', 'var154(t-3)', 'var155(t-3)', 'var158(t-3)', 'var160(t-3)', 'var162(t-3)', 'var163(t-3)', 'var164(t-3)', 'var170(t-3)', 'var171(t-3)', 'var172(t-3)', 'var173(t-3)', 'var176(t-3)', 'var177(t-3)', 'var180(t-3)', 'var181(t-3)', 'var190(t-3)', 'var191(t-3)', 'var192(t-3)', 'var193(t-3)', 'var195(t-3)', 'var196(t-3)', 'var197(t-3)', 'var198(t-3)', 'var199(t-3)', 'var200(t-3)', 'var201(t-3)', 'var202(t-3)', 'var203(t-3)', 'var204(t-3)', 'var205(t-3)', 'var206(t-3)', 'var207(t-3)', 'var208(t-3)', 'var209(t-3)', 'var210(t-3)', 'var211(t-3)', 'var212(t-3)', 'var213(t-3)', 'var214(t-3)', 'var215(t-3)', 'var216(t-3)', 'var217(t-3)', 'var218(t-3)', 'var219(t-3)', 'var220(t-3)', 'var221(t-3)', 'var222(t-3)', 'var223(t-3)', 'var224(t-3)', 'var225(t-3)', 'var226(t-3)', 'var227(t-3)', 'var228(t-3)', 'var229(t-3)', 'var230(t-3)', 'var231(t-3)', 'var232(t-3)', 'var233(t-3)', 'var234(t-3)', 'var235(t-3)', 'var236(t-3)', 'var237(t-3)', 'var238(t-3)', 'var240(t-3)', 'var241(t-3)', 'var242(t-3)', 'var243(t-3)', 'var244(t-3)', 'var245(t-3)', 'var246(t-3)', 'var247(t-3)', 'var248(t-3)', 'var249(t-3)', 'var250(t-3)', 'var251(t-3)', 'var252(t-3)', 'var253(t-3)', 'var254(t-3)', 'var256(t-3)', 'var257(t-3)', 'var258(t-3)', 'var259(t-3)', 'var261(t-3)', 'var262(t-3)', 'var264(t-3)', 'var265(t-3)', 'var266(t-3)', 'var267(t-3)', 'var268(t-3)', 'var269(t-3)', 'var270(t-3)', 'var271(t-3)', 'var272(t-3)', 'var273(t-3)', 'var274(t-3)', 'var276(t-3)', 'var277(t-3)', 'var278(t-3)', 'var279(t-3)', 'var280(t-3)', 'var281(t-3)', 'var282(t-3)', 'var283(t-3)', 'var284(t-3)', 'var285(t-3)', 'var286(t-3)', 'var287(t-3)', 'var288(t-3)', 'var289(t-3)', 'var290(t-3)', 'var291(t-3)', 'var292(t-3)', 'var293(t-3)', 'var294(t-3)', 'var295(t-3)', 'var300(t-3)', 'var301(t-3)', 'var305(t-3)', 'var306(t-3)', 'var307(t-3)', 'var309(t-3)', 'var310(t-3)', 'var311(t-3)', 'var313(t-3)', 'var314(t-3)', 'var315(t-3)', 'var317(t-3)', 'var318(t-3)', 'var319(t-3)', 'var321(t-3)', 'var327(t-3)', 'var329(t-3)', 'var330(t-3)', 'var331(t-3)', 'var335(t-3)', 'var336(t-3)', 'var337(t-3)', 'var338(t-3)', 'var339(t-3)', 'var342(t-3)', 'var344(t-3)', 'var345(t-3)', 'var347(t-3)', 'var349(t-3)', 'var351(t-3)', 'var352(t-3)', 'var354(t-3)', 'var355(t-3)', 'var356(t-3)', 'var357(t-3)', 'var363(t-3)', 'var365(t-3)', 'var366(t-3)', 'var367(t-3)', 'var369(t-3)', 'var370(t-3)', 'var371(t-3)', 'var373(t-3)', 'var374(t-3)', 'var375(t-3)', 'var376(t-3)', 'var378(t-3)', 'var379(t-3)', 'var380(t-3)', 'var382(t-3)', 'var383(t-3)', 'var384(t-3)', 'var385(t-3)', 'var386(t-3)', 'var388(t-3)', 'var391(t-3)', 'var398(t-3)', 'var403(t-3)', 'var404(t-3)', 'var405(t-3)', 'var406(t-3)', 'var407(t-3)', 'var408(t-3)', 'var409(t-3)', 'var410(t-3)', 'var411(t-3)', 'var413(t-3)', 'var414(t-3)', 'var415(t-3)', 'var417(t-3)', 'var418(t-3)', 'var420(t-3)', 'var424(t-3)', 'var425(t-3)', 'var426(t-3)', 'var428(t-3)', 'var429(t-3)', 'var431(t-3)', 'var432(t-3)', 'var434(t-3)', 'var441(t-3)', 'var443(t-3)', 'var445(t-3)', 'var446(t-3)', 'var447(t-3)', 'var448(t-3)', 'var449(t-3)', 'var450(t-3)', 'var451(t-3)', 'var454(t-3)', 'var455(t-3)', 'var461(t-3)', 'var462(t-3)', 'var467(t-3)', 'var468(t-3)', 'var469(t-3)', 'var471(t-3)', 'var472(t-3)', 'var473(t-3)', 'var475(t-3)', 'var476(t-3)', 'var478(t-3)', 'var479(t-3)', 'var480(t-3)', 'var481(t-3)', 'var482(t-3)', 'var483(t-3)', 'var484(t-3)', 'var486(t-3)', 'var488(t-3)', 'var490(t-3)', 'var491(t-3)', 'var492(t-3)', 'var493(t-3)', 'var494(t-3)', 'var495(t-3)', 'var496(t-3)', 'var497(t-3)', 'var498(t-3)', 'var499(t-3)', 'var500(t-3)', 'var503(t-3)', 'var504(t-3)', 'var505(t-3)', 'var506(t-3)', 'var507(t-3)', 'var508(t-3)', 'var509(t-3)', 'var510(t-3)', 'var511(t-3)', 'var1(t-2)', 'var2(t-2)', 'var3(t-2)', 'var4(t-2)', 'var5(t-2)', 'var6(t-2)', 'var7(t-2)', 'var8(t-2)', 'var9(t-2)', 'var10(t-2)', 'var11(t-2)', 'var12(t-2)', 'var14(t-2)', 'var15(t-2)', 'var16(t-2)', 'var20(t-2)', 'var21(t-2)', 'var22(t-2)', 'var23(t-2)', 'var24(t-2)', 'var25(t-2)', 'var26(t-2)', 'var27(t-2)', 'var28(t-2)', 'var29(t-2)', 'var30(t-2)', 'var31(t-2)', 'var32(t-2)', 'var33(t-2)', 'var34(t-2)', 'var35(t-2)', 'var36(t-2)', 'var37(t-2)', 'var38(t-2)', 'var39(t-2)', 'var40(t-2)', 'var41(t-2)', 'var44(t-2)', 'var45(t-2)', 'var46(t-2)', 'var47(t-2)', 'var48(t-2)', 'var49(t-2)', 'var50(t-2)', 'var52(t-2)', 'var53(t-2)', 'var59(t-2)', 'var60(t-2)', 'var61(t-2)', 'var66(t-2)', 'var67(t-2)', 'var68(t-2)', 'var70(t-2)', 'var71(t-2)', 'var74(t-2)', 'var83(t-2)', 'var86(t-2)', 'var87(t-2)', 'var88(t-2)', 'var93(t-2)', 'var95(t-2)', 'var96(t-2)', 'var98(t-2)', 'var107(t-2)', 'var108(t-2)', 'var109(t-2)', 'var112(t-2)', 'var120(t-2)', 'var123(t-2)', 'var124(t-2)', 'var125(t-2)', 'var126(t-2)', 'var128(t-2)', 'var129(t-2)', 'var132(t-2)', 'var133(t-2)', 'var134(t-2)', 'var139(t-2)', 'var142(t-2)', 'var143(t-2)', 'var144(t-2)', 'var147(t-2)', 'var150(t-2)', 'var151(t-2)', 'var155(t-2)', 'var156(t-2)', 'var163(t-2)', 'var167(t-2)', 'var168(t-2)', 'var176(t-2)', 'var181(t-2)', 'var184(t-2)', 'var185(t-2)', 'var186(t-2)', 'var187(t-2)', 'var188(t-2)', 'var190(t-2)', 'var192(t-2)', 'var196(t-2)', 'var197(t-2)', 'var203(t-2)', 'var204(t-2)', 'var205(t-2)', 'var206(t-2)', 'var214(t-2)', 'var217(t-2)', 'var219(t-2)', 'var221(t-2)', 'var222(t-2)', 'var223(t-2)', 'var224(t-2)', 'var225(t-2)', 'var226(t-2)', 'var227(t-2)', 'var228(t-2)', 'var229(t-2)', 'var231(t-2)', 'var232(t-2)', 'var233(t-2)', 'var234(t-2)', 'var235(t-2)', 'var236(t-2)', 'var237(t-2)', 'var238(t-2)', 'var240(t-2)', 'var241(t-2)', 'var242(t-2)', 'var243(t-2)', 'var244(t-2)', 'var245(t-2)', 'var246(t-2)', 'var247(t-2)', 'var248(t-2)', 'var249(t-2)', 'var250(t-2)', 'var251(t-2)', 'var252(t-2)', 'var253(t-2)', 'var254(t-2)', 'var256(t-2)', 'var258(t-2)', 'var260(t-2)', 'var261(t-2)', 'var262(t-2)', 'var263(t-2)', 'var264(t-2)', 'var265(t-2)', 'var266(t-2)', 'var267(t-2)', 'var269(t-2)', 'var270(t-2)', 'var271(t-2)', 'var273(t-2)', 'var274(t-2)', 'var276(t-2)', 'var277(t-2)', 'var278(t-2)', 'var279(t-2)', 'var280(t-2)', 'var281(t-2)', 'var282(t-2)', 'var283(t-2)', 'var284(t-2)', 'var285(t-2)', 'var286(t-2)', 'var287(t-2)', 'var288(t-2)', 'var289(t-2)', 'var290(t-2)', 'var291(t-2)', 'var292(t-2)', 'var293(t-2)', 'var294(t-2)', 'var295(t-2)', 'var296(t-2)', 'var297(t-2)', 'var298(t-2)', 'var299(t-2)', 'var301(t-2)', 'var309(t-2)', 'var311(t-2)', 'var313(t-2)', 'var314(t-2)', 'var316(t-2)', 'var321(t-2)', 'var323(t-2)', 'var327(t-2)', 'var329(t-2)', 'var330(t-2)', 'var337(t-2)', 'var338(t-2)', 'var340(t-2)', 'var342(t-2)', 'var343(t-2)', 'var344(t-2)', 'var345(t-2)', 'var346(t-2)', 'var355(t-2)', 'var356(t-2)', 'var358(t-2)', 'var361(t-2)', 'var362(t-2)', 'var365(t-2)', 'var366(t-2)', 'var368(t-2)', 'var370(t-2)', 'var372(t-2)', 'var373(t-2)', 'var376(t-2)', 'var377(t-2)', 'var380(t-2)', 'var381(t-2)', 'var382(t-2)', 'var386(t-2)', 'var387(t-2)', 'var389(t-2)', 'var395(t-2)', 'var399(t-2)', 'var400(t-2)', 'var402(t-2)', 'var403(t-2)', 'var404(t-2)', 'var405(t-2)', 'var409(t-2)', 'var410(t-2)', 'var413(t-2)', 'var415(t-2)', 'var416(t-2)', 'var421(t-2)', 'var425(t-2)', 'var427(t-2)', 'var428(t-2)', 'var429(t-2)', 'var433(t-2)', 'var435(t-2)', 'var436(t-2)', 'var437(t-2)', 'var441(t-2)', 'var442(t-2)', 'var452(t-2)', 'var453(t-2)', 'var454(t-2)', 'var455(t-2)', 'var456(t-2)', 'var457(t-2)', 'var458(t-2)', 'var459(t-2)', 'var460(t-2)', 'var461(t-2)', 'var462(t-2)', 'var463(t-2)', 'var464(t-2)', 'var465(t-2)', 'var466(t-2)', 'var467(t-2)', 'var468(t-2)', 'var469(t-2)', 'var470(t-2)', 'var471(t-2)', 'var472(t-2)', 'var473(t-2)', 'var474(t-2)', 'var475(t-2)', 'var476(t-2)', 'var477(t-2)', 'var478(t-2)', 'var479(t-2)', 'var480(t-2)', 'var481(t-2)', 'var482(t-2)', 'var483(t-2)', 'var484(t-2)', 'var485(t-2)', 'var486(t-2)', 'var487(t-2)', 'var489(t-2)', 'var490(t-2)', 'var491(t-2)', 'var492(t-2)', 'var493(t-2)', 'var494(t-2)', 'var495(t-2)', 'var496(t-2)', 'var497(t-2)', 'var498(t-2)', 'var499(t-2)', 'var500(t-2)', 'var501(t-2)', 'var502(t-2)', 'var503(t-2)', 'var504(t-2)', 'var505(t-2)', 'var506(t-2)', 'var507(t-2)', 'var508(t-2)', 'var509(t-2)', 'var510(t-2)', 'var511(t-2)', 'var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)', 'var6(t-1)', 'var7(t-1)', 'var8(t-1)', 'var9(t-1)', 'var10(t-1)', 'var11(t-1)', 'var12(t-1)', 'var13(t-1)', 'var14(t-1)', 'var15(t-1)', 'var16(t-1)', 'var17(t-1)', 'var18(t-1)', 'var19(t-1)', 'var20(t-1)', 'var21(t-1)', 'var22(t-1)', 'var23(t-1)', 'var24(t-1)', 'var25(t-1)', 'var26(t-1)', 'var27(t-1)', 'var28(t-1)', 'var29(t-1)', 'var30(t-1)', 'var31(t-1)', 'var32(t-1)', 'var33(t-1)', 'var34(t-1)', 'var35(t-1)', 'var36(t-1)', 'var37(t-1)', 'var38(t-1)', 'var39(t-1)', 'var40(t-1)', 'var41(t-1)', 'var42(t-1)', 'var43(t-1)', 'var44(t-1)', 'var45(t-1)', 'var46(t-1)', 'var47(t-1)', 'var48(t-1)', 'var49(t-1)', 'var50(t-1)', 'var51(t-1)', 'var52(t-1)', 'var53(t-1)', 'var54(t-1)', 'var55(t-1)', 'var56(t-1)', 'var57(t-1)', 'var58(t-1)', 'var59(t-1)', 'var60(t-1)', 'var61(t-1)', 'var62(t-1)', 'var63(t-1)', 'var64(t-1)', 'var65(t-1)', 'var66(t-1)', 'var67(t-1)', 'var68(t-1)', 'var69(t-1)', 'var70(t-1)', 'var71(t-1)', 'var72(t-1)', 'var73(t-1)', 'var74(t-1)', 'var75(t-1)', 'var76(t-1)', 'var77(t-1)', 'var78(t-1)', 'var79(t-1)', 'var80(t-1)', 'var81(t-1)', 'var82(t-1)', 'var83(t-1)', 'var84(t-1)', 'var85(t-1)', 'var86(t-1)', 'var87(t-1)', 'var88(t-1)', 'var89(t-1)', 'var90(t-1)', 'var91(t-1)', 'var92(t-1)', 'var93(t-1)', 'var94(t-1)', 'var95(t-1)', 'var96(t-1)', 'var97(t-1)', 'var98(t-1)', 'var99(t-1)', 'var100(t-1)', 'var101(t-1)', 'var102(t-1)', 'var103(t-1)', 'var104(t-1)', 'var105(t-1)', 'var106(t-1)', 'var107(t-1)', 'var108(t-1)', 'var109(t-1)', 'var110(t-1)', 'var111(t-1)', 'var112(t-1)', 'var113(t-1)', 'var114(t-1)', 'var115(t-1)', 'var116(t-1)', 'var117(t-1)', 'var118(t-1)', 'var119(t-1)', 'var120(t-1)', 'var121(t-1)', 'var122(t-1)', 'var123(t-1)', 'var124(t-1)', 'var125(t-1)', 'var126(t-1)', 'var127(t-1)', 'var128(t-1)', 'var129(t-1)', 'var130(t-1)', 'var131(t-1)', 'var132(t-1)', 'var133(t-1)', 'var134(t-1)', 'var135(t-1)', 'var136(t-1)', 'var137(t-1)', 'var138(t-1)', 'var139(t-1)', 'var140(t-1)', 'var141(t-1)', 'var142(t-1)', 'var143(t-1)', 'var144(t-1)', 'var145(t-1)', 'var146(t-1)', 'var147(t-1)', 'var148(t-1)', 'var149(t-1)', 'var150(t-1)', 'var151(t-1)', 'var152(t-1)', 'var153(t-1)', 'var154(t-1)', 'var155(t-1)', 'var156(t-1)', 'var157(t-1)', 'var158(t-1)', 'var159(t-1)', 'var160(t-1)', 'var161(t-1)', 'var162(t-1)', 'var163(t-1)', 'var164(t-1)', 'var165(t-1)', 'var166(t-1)', 'var167(t-1)', 'var168(t-1)', 'var169(t-1)', 'var170(t-1)', 'var171(t-1)', 'var172(t-1)', 'var173(t-1)', 'var174(t-1)', 'var175(t-1)', 'var176(t-1)', 'var177(t-1)', 'var178(t-1)', 'var179(t-1)', 'var180(t-1)', 'var181(t-1)', 'var182(t-1)', 'var183(t-1)', 'var184(t-1)', 'var185(t-1)', 'var186(t-1)', 'var187(t-1)', 'var188(t-1)', 'var189(t-1)', 'var190(t-1)', 'var191(t-1)', 'var192(t-1)', 'var193(t-1)', 'var194(t-1)', 'var195(t-1)', 'var196(t-1)', 'var197(t-1)', 'var198(t-1)', 'var199(t-1)', 'var200(t-1)', 'var201(t-1)', 'var202(t-1)', 'var203(t-1)', 'var204(t-1)', 'var205(t-1)', 'var206(t-1)', 'var207(t-1)', 'var208(t-1)', 'var209(t-1)', 'var210(t-1)', 'var211(t-1)', 'var212(t-1)', 'var213(t-1)', 'var214(t-1)', 'var215(t-1)', 'var216(t-1)', 'var217(t-1)', 'var218(t-1)', 'var219(t-1)', 'var220(t-1)', 'var221(t-1)', 'var222(t-1)', 'var223(t-1)', 'var224(t-1)', 'var225(t-1)', 'var226(t-1)', 'var227(t-1)', 'var228(t-1)', 'var229(t-1)', 'var230(t-1)', 'var231(t-1)', 'var232(t-1)', 'var233(t-1)', 'var234(t-1)', 'var235(t-1)', 'var236(t-1)', 'var237(t-1)', 'var238(t-1)', 'var239(t-1)', 'var240(t-1)', 'var241(t-1)', 'var242(t-1)', 'var243(t-1)', 'var244(t-1)', 'var245(t-1)', 'var246(t-1)', 'var247(t-1)', 'var248(t-1)', 'var249(t-1)', 'var250(t-1)', 'var251(t-1)', 'var252(t-1)', 'var253(t-1)', 'var254(t-1)', 'var255(t-1)', 'var256(t-1)', 'var257(t-1)', 'var258(t-1)', 'var259(t-1)', 'var260(t-1)', 'var261(t-1)', 'var262(t-1)', 'var263(t-1)', 'var264(t-1)', 'var265(t-1)', 'var266(t-1)', 'var267(t-1)', 'var268(t-1)', 'var269(t-1)', 'var270(t-1)', 'var271(t-1)', 'var272(t-1)', 'var273(t-1)', 'var274(t-1)', 'var275(t-1)', 'var276(t-1)', 'var277(t-1)', 'var278(t-1)', 'var279(t-1)', 'var280(t-1)', 'var281(t-1)', 'var282(t-1)', 'var283(t-1)', 'var284(t-1)', 'var285(t-1)', 'var286(t-1)', 'var287(t-1)', 'var288(t-1)', 'var289(t-1)', 'var290(t-1)', 'var291(t-1)', 'var292(t-1)', 'var293(t-1)', 'var294(t-1)', 'var295(t-1)', 'var296(t-1)', 'var297(t-1)', 'var298(t-1)', 'var299(t-1)', 'var300(t-1)', 'var301(t-1)', 'var302(t-1)', 'var303(t-1)', 'var304(t-1)', 'var305(t-1)', 'var306(t-1)', 'var307(t-1)', 'var308(t-1)', 'var309(t-1)', 'var310(t-1)', 'var311(t-1)', 'var312(t-1)', 'var313(t-1)', 'var314(t-1)', 'var315(t-1)', 'var316(t-1)', 'var317(t-1)', 'var318(t-1)', 'var319(t-1)', 'var320(t-1)', 'var321(t-1)', 'var322(t-1)', 'var323(t-1)', 'var324(t-1)', 'var325(t-1)', 'var326(t-1)', 'var327(t-1)', 'var328(t-1)', 'var329(t-1)', 'var330(t-1)', 'var331(t-1)', 'var332(t-1)', 'var333(t-1)', 'var334(t-1)', 'var335(t-1)', 'var336(t-1)', 'var337(t-1)', 'var338(t-1)', 'var339(t-1)', 'var340(t-1)', 'var341(t-1)', 'var342(t-1)', 'var343(t-1)', 'var344(t-1)', 'var345(t-1)', 'var346(t-1)', 'var347(t-1)', 'var348(t-1)', 'var349(t-1)', 'var350(t-1)', 'var351(t-1)', 'var352(t-1)', 'var353(t-1)', 'var354(t-1)', 'var355(t-1)', 'var356(t-1)', 'var357(t-1)', 'var358(t-1)', 'var359(t-1)', 'var360(t-1)', 'var361(t-1)', 'var362(t-1)', 'var363(t-1)', 'var364(t-1)', 'var365(t-1)', 'var366(t-1)', 'var367(t-1)', 'var368(t-1)', 'var369(t-1)', 'var370(t-1)', 'var371(t-1)', 'var372(t-1)', 'var373(t-1)', 'var374(t-1)', 'var375(t-1)', 'var376(t-1)', 'var377(t-1)', 'var378(t-1)', 'var379(t-1)', 'var380(t-1)', 'var381(t-1)', 'var382(t-1)', 'var383(t-1)', 'var384(t-1)', 'var385(t-1)', 'var386(t-1)', 'var387(t-1)', 'var388(t-1)', 'var389(t-1)', 'var390(t-1)', 'var391(t-1)', 'var392(t-1)', 'var393(t-1)', 'var394(t-1)', 'var395(t-1)', 'var396(t-1)', 'var397(t-1)', 'var398(t-1)', 'var399(t-1)', 'var400(t-1)', 'var401(t-1)', 'var402(t-1)', 'var403(t-1)', 'var404(t-1)', 'var405(t-1)', 'var406(t-1)', 'var407(t-1)', 'var408(t-1)', 'var409(t-1)', 'var410(t-1)', 'var411(t-1)', 'var412(t-1)', 'var413(t-1)', 'var414(t-1)', 'var415(t-1)', 'var416(t-1)', 'var417(t-1)', 'var418(t-1)', 'var419(t-1)', 'var420(t-1)', 'var421(t-1)', 'var422(t-1)', 'var423(t-1)', 'var424(t-1)', 'var425(t-1)', 'var426(t-1)', 'var427(t-1)', 'var428(t-1)', 'var429(t-1)', 'var430(t-1)', 'var431(t-1)', 'var432(t-1)', 'var433(t-1)', 'var434(t-1)', 'var435(t-1)', 'var436(t-1)', 'var437(t-1)', 'var438(t-1)', 'var439(t-1)', 'var440(t-1)', 'var441(t-1)', 'var442(t-1)', 'var443(t-1)', 'var444(t-1)', 'var445(t-1)', 'var446(t-1)', 'var447(t-1)', 'var448(t-1)', 'var449(t-1)', 'var450(t-1)', 'var451(t-1)', 'var452(t-1)', 'var453(t-1)', 'var454(t-1)', 'var455(t-1)', 'var456(t-1)', 'var457(t-1)', 'var458(t-1)', 'var459(t-1)', 'var460(t-1)', 'var461(t-1)', 'var462(t-1)', 'var463(t-1)', 'var464(t-1)', 'var465(t-1)', 'var466(t-1)', 'var467(t-1)', 'var468(t-1)', 'var469(t-1)', 'var470(t-1)', 'var471(t-1)', 'var472(t-1)', 'var473(t-1)', 'var474(t-1)', 'var475(t-1)', 'var476(t-1)', 'var477(t-1)', 'var478(t-1)', 'var479(t-1)', 'var480(t-1)', 'var481(t-1)', 'var482(t-1)', 'var483(t-1)', 'var484(t-1)', 'var485(t-1)', 'var486(t-1)', 'var487(t-1)', 'var488(t-1)', 'var489(t-1)', 'var490(t-1)', 'var491(t-1)', 'var492(t-1)', 'var493(t-1)', 'var494(t-1)', 'var495(t-1)', 'var496(t-1)', 'var497(t-1)', 'var498(t-1)', 'var499(t-1)', 'var500(t-1)', 'var501(t-1)', 'var502(t-1)', 'var503(t-1)', 'var504(t-1)', 'var505(t-1)', 'var506(t-1)', 'var507(t-1)', 'var508(t-1)', 'var509(t-1)', 'var510(t-1)', 'var511(t-1)', 'var1(t)', 'var2(t)', 'var3(t)', 'var4(t)', 'var5(t)', 'var6(t)', 'var7(t)', 'var8(t)', 'var9(t)', 'var10(t)', 'var11(t)', 'var12(t)', 'var13(t)', 'var14(t)', 'var15(t)', 'var16(t)', 'var17(t)', 'var18(t)', 'var19(t)', 'var20(t)', 'var21(t)', 'var22(t)', 'var23(t)', 'var24(t)', 'var25(t)', 'var26(t)', 'var27(t)', 'var28(t)', 'var29(t)', 'var30(t)', 'var31(t)', 'var32(t)', 'var33(t)', 'var34(t)', 'var35(t)', 'var36(t)', 'var37(t)', 'var38(t)', 'var39(t)', 'var40(t)', 'var41(t)', 'var42(t)', 'var43(t)', 'var44(t)', 'var45(t)', 'var46(t)', 'var47(t)', 'var48(t)', 'var49(t)', 'var50(t)', 'var51(t)', 'var52(t)', 'var53(t)', 'var54(t)', 'var55(t)', 'var56(t)', 'var57(t)', 'var58(t)', 'var59(t)', 'var60(t)', 'var61(t)', 'var62(t)', 'var63(t)', 'var64(t)', 'var65(t)', 'var66(t)', 'var67(t)', 'var68(t)', 'var69(t)', 'var70(t)', 'var71(t)', 'var72(t)', 'var73(t)', 'var74(t)', 'var75(t)', 'var76(t)', 'var77(t)', 'var78(t)', 'var79(t)', 'var80(t)', 'var81(t)', 'var82(t)', 'var83(t)', 'var84(t)', 'var85(t)', 'var86(t)', 'var87(t)', 'var88(t)', 'var89(t)', 'var90(t)', 'var91(t)', 'var92(t)', 'var93(t)', 'var94(t)', 'var95(t)', 'var96(t)', 'var97(t)', 'var98(t)', 'var99(t)', 'var100(t)', 'var101(t)', 'var102(t)', 'var103(t)', 'var104(t)', 'var105(t)', 'var106(t)', 'var107(t)', 'var108(t)', 'var109(t)', 'var110(t)', 'var111(t)', 'var112(t)', 'var113(t)', 'var114(t)', 'var115(t)', 'var116(t)', 'var117(t)', 'var118(t)', 'var119(t)', 'var120(t)', 'var121(t)', 'var122(t)', 'var123(t)', 'var124(t)', 'var125(t)', 'var126(t)', 'var127(t)', 'var128(t)', 'var129(t)', 'var130(t)', 'var131(t)', 'var132(t)', 'var133(t)', 'var134(t)', 'var135(t)', 'var136(t)', 'var137(t)', 'var138(t)', 'var139(t)', 'var141(t)', 'var142(t)', 'var143(t)', 'var144(t)', 'var145(t)', 'var146(t)', 'var147(t)', 'var148(t)', 'var149(t)', 'var150(t)', 'var151(t)', 'var152(t)', 'var153(t)', 'var154(t)', 'var155(t)', 'var156(t)', 'var157(t)', 'var158(t)', 'var159(t)', 'var160(t)', 'var161(t)', 'var162(t)', 'var163(t)', 'var164(t)', 'var165(t)', 'var166(t)', 'var167(t)', 'var168(t)', 'var169(t)', 'var170(t)', 'var171(t)', 'var172(t)', 'var173(t)', 'var174(t)', 'var176(t)', 'var177(t)', 'var178(t)', 'var179(t)', 'var180(t)', 'var182(t)', 'var183(t)', 'var184(t)', 'var185(t)', 'var186(t)', 'var187(t)', 'var188(t)', 'var189(t)', 'var190(t)', 'var191(t)', 'var192(t)', 'var193(t)', 'var194(t)', 'var195(t)', 'var196(t)', 'var197(t)', 'var198(t)', 'var199(t)', 'var200(t)', 'var201(t)', 'var202(t)', 'var203(t)', 'var204(t)', 'var205(t)', 'var206(t)', 'var207(t)', 'var208(t)', 'var209(t)', 'var210(t)', 'var211(t)', 'var212(t)', 'var213(t)', 'var214(t)', 'var215(t)', 'var216(t)', 'var217(t)', 'var218(t)', 'var219(t)', 'var220(t)', 'var221(t)', 'var222(t)', 'var223(t)', 'var224(t)', 'var225(t)', 'var226(t)', 'var227(t)', 'var228(t)', 'var229(t)', 'var230(t)', 'var231(t)', 'var232(t)', 'var233(t)', 'var234(t)', 'var235(t)', 'var236(t)', 'var237(t)', 'var238(t)', 'var239(t)', 'var240(t)', 'var241(t)', 'var242(t)', 'var243(t)', 'var244(t)', 'var245(t)', 'var246(t)', 'var247(t)', 'var248(t)', 'var249(t)', 'var250(t)', 'var251(t)', 'var252(t)', 'var253(t)', 'var254(t)', 'var255(t)', 'var256(t)', 'var257(t)', 'var259(t)', 'var260(t)', 'var261(t)', 'var262(t)', 'var263(t)', 'var265(t)', 'var266(t)', 'var267(t)', 'var268(t)', 'var270(t)', 'var271(t)', 'var272(t)', 'var273(t)', 'var274(t)', 'var275(t)', 'var276(t)', 'var277(t)', 'var278(t)', 'var279(t)', 'var280(t)', 'var281(t)', 'var282(t)', 'var283(t)', 'var284(t)', 'var285(t)', 'var286(t)', 'var287(t)', 'var288(t)', 'var289(t)', 'var290(t)', 'var291(t)', 'var292(t)', 'var293(t)', 'var294(t)', 'var295(t)', 'var296(t)', 'var297(t)', 'var298(t)', 'var299(t)', 'var300(t)', 'var301(t)', 'var302(t)', 'var303(t)', 'var304(t)', 'var305(t)', 'var306(t)', 'var307(t)', 'var308(t)', 'var309(t)', 'var310(t)', 'var311(t)', 'var312(t)', 'var313(t)', 'var314(t)', 'var315(t)', 'var316(t)', 'var317(t)', 'var319(t)', 'var320(t)', 'var321(t)', 'var322(t)', 'var323(t)', 'var324(t)', 'var325(t)', 'var326(t)', 'var328(t)', 'var330(t)', 'var331(t)', 'var332(t)', 'var333(t)', 'var334(t)', 'var336(t)', 'var337(t)', 'var338(t)', 'var339(t)', 'var340(t)', 'var341(t)', 'var342(t)', 'var343(t)', 'var344(t)', 'var345(t)', 'var346(t)', 'var347(t)', 'var348(t)', 'var349(t)', 'var350(t)', 'var351(t)', 'var352(t)', 'var353(t)', 'var354(t)', 'var355(t)', 'var356(t)', 'var357(t)', 'var358(t)', 'var359(t)', 'var360(t)', 'var361(t)', 'var362(t)', 'var363(t)', 'var364(t)', 'var365(t)', 'var366(t)', 'var367(t)', 'var368(t)', 'var369(t)', 'var371(t)', 'var372(t)', 'var373(t)', 'var374(t)', 'var375(t)', 'var376(t)', 'var377(t)', 'var378(t)', 'var379(t)', 'var380(t)', 'var381(t)', 'var382(t)', 'var383(t)', 'var384(t)', 'var385(t)', 'var386(t)', 'var387(t)', 'var388(t)', 'var389(t)', 'var390(t)', 'var391(t)', 'var392(t)', 'var393(t)']\n",
    "X_df = X_df[recursively_eliminated_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Decomposition\n",
    "\n",
    "Principal component analysis: Factor model in which the factors are based on summarizing the total variance. With PCA, unities are used in the diagonal of the correlation matrix computationally implying that all the variance is common or shared.\n",
    "\n",
    "https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrincipalComponentAnalysisClass:\n",
    "    \"\"\"\n",
    "    This class handles logic related to PCA data transformations.\n",
    "    https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
    "    \"\"\"\n",
    "    def __init__(self, X_df):\n",
    "        \"\"\"\n",
    "        Cosntructor method.\n",
    "        :param X_df: (Pandas) Dataframe consisting of input features, which will be subject to PCA.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__X_df = X_df\n",
    "        \n",
    "    def get_default_component_variances(self):\n",
    "        \"\"\"\n",
    "        Fitting the PCA algorithm with our Data.\n",
    "        :return: (Numpy array) Array of feature variances.\n",
    "        \"\"\"\n",
    "        pca = PCA().fit(self.__X_df.values)\n",
    "        return np.cumsum(pca.explained_variance_ratio_)\n",
    "        \n",
    "    def get_default_component_count(self, threshold=.99):\n",
    "        \"\"\"\n",
    "        Retrieves the recommended number of component decomposition, above which very little variance \n",
    "        gain is achieved. This treshold will be set at a 0.999 variance threshold.\n",
    "        :param threshold: (Float) Threshold value between 0 and 1. Stops immediately as soon the number\n",
    "                                  of required components exceeds the threshold value.\n",
    "        :return: (Integer) Returns the number of recommended components.\n",
    "        \"\"\"\n",
    "        variance_ratios = self.get_default_component_variances()\n",
    "        n = 0\n",
    "        for val in variance_ratios:\n",
    "            if val < threshold:\n",
    "                n += 1\n",
    "        return n\n",
    "    \n",
    "    def plot_variance_per_reduction(self):\n",
    "        \"\"\"\n",
    "        This method subjects the feature matrix to a PCA decomposition. The number of components is plot\n",
    "        vs the amount of retained variance.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        variance_ratios = self.get_default_component_variances()\n",
    "        \n",
    "        #Plotting the Cumulative Summation of the Explained Variance\n",
    "        plt.figure()\n",
    "        plt.plot(variance_ratios)\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Variance (%)') #for each component\n",
    "        plt.title(tpcds + ' Dataset Explained Variance')\n",
    "        plt.show()\n",
    "        \n",
    "    def apply_PCA(self, n_components):\n",
    "        \"\"\"\n",
    "        Applies Principle Component Analysis on the constructor passed data matrix, on a number of components.\n",
    "        A new pandas data matrix is returned, with renamed 'Principal Component' headers.\n",
    "        :param n_components: (Integer) Denotes number of component breakdown.\n",
    "        :return: (Pandas) Dataframe consisting of new decomposed components.\n",
    "        \"\"\"\n",
    "        pca = PCA(n_components=n_components)\n",
    "        dataset = pca.fit_transform(self.__X_df.values)\n",
    "        header_list = []\n",
    "        for i in range(dataset.shape[1]):\n",
    "            header_list.append('Component_' + str(i))\n",
    "        return pd.DataFrame(data=dataset, columns=header_list)\n",
    "\n",
    "print(X_df.head())\n",
    "print(X_df.shape)\n",
    "\n",
    "pcac = PrincipalComponentAnalysisClass(X_df=X_df)\n",
    "pcac.plot_variance_per_reduction()\n",
    "component_count = pcac.get_default_component_count()\n",
    "X_df = pcac.apply_PCA(n_components=component_count)\n",
    "\n",
    "print('-'*30)\n",
    "print(X_df.head())\n",
    "print(X_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classification (Many to Many)\n",
    "### Designing the network\n",
    "\n",
    "- The first step is to define your network.\n",
    "- Neural networks are defined in Keras as a sequence of layers. The container for these layers is the **Sequential class**.\n",
    "- The first step is to create an instance of the Sequential class. Then you can create your layers and add them in the order that they should be connected.\n",
    "- The LSTM recurrent layer comprised of memory units is called LSTM().\n",
    "- A fully connected layer that often follows LSTM layers and is used for outputting a prediction is called Dense().\n",
    "- The first layer in the network must define the number of inputs to expect.\n",
    "- Input must be three-dimensional, comprised of samples, timesteps, and features.\n",
    "    - **Samples:** These are the rows in your data.\n",
    "    - **Timesteps:** These are the past observations for a feature, such as lag variables.\n",
    "    - **Features:** These are columns in your data.\n",
    "- Assuming your data is loaded as a NumPy array, you can convert a 2D dataset to a 3D dataset using the reshape() function in NumPy.\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "* https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "* https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/\n",
    "* https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "* https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Class\n",
    "class LSTM:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, lag, loss_func, activation, optimizer='sgd', lstm_layers=1, dropout=.0,\n",
    "                 stateful=False, y_labels=None, initializer='uniform'):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X             - (Numpy 2D Array) Training data used to train the model (Features).\n",
    "        :param y             - (Numpy 2D Array) Test data used to test the model (Labels\n",
    "        :param lag           - (Integer) Denotes lag step value\n",
    "        :param loss_function - (String)  Denotes mode of measure fitting of model (Fitting function).\n",
    "        :param activation    - (String)  Neuron activation function used to activate/trigger neurons.\n",
    "        :param optimizer     - (String)  Denotes which function to us to optimize the model build (eg: Gradient Descent).\n",
    "        :param lstm_layers   - (Integer) Denotes the number of LSTM layers to be included in the model build.\n",
    "        :param dropout       - (Float)   Denotes amount of dropout for model. This parameter must be a value between 0 and 1.\n",
    "        :param stateful      - (Boolean) Denotes whether state is used as initial state for next training batch.\n",
    "        :param: y_labels     - (List) List of target label names\n",
    "        :param: initializer  - (String)  String initializer which denotes starting weights.\n",
    "        \"\"\"\n",
    "        self.__lag = lag\n",
    "        self.__model = ke.models.Sequential()\n",
    "        self.__y_labels = y_labels\n",
    "\n",
    "        if dropout > 1 and dropout < 0:\n",
    "            raise ValueError('Dropout parameter exceeded! Must be a value between 0 and 1.')\n",
    "        \n",
    "        # self.__model.add(ke.layers.Embedding(2+1, 32, input_length=X.shape[1]))\n",
    "        for i in range(0, lstm_layers - 1):  # If lstm_layers == 1, this for loop logic is skipped.\n",
    "            if stateful:\n",
    "                if i == 0:\n",
    "                    self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                    batch_input_shape=(X.shape[0],\n",
    "                                                                       X.shape[1],\n",
    "                                                                       X.shape[2]),\n",
    "                                                    return_sequences=True,\n",
    "                                                    recurrent_dropout=dropout,\n",
    "                                                    recurrent_initializer=initializer,\n",
    "                                                    activation=activation,\n",
    "                                                    stateful=stateful))\n",
    "                else:\n",
    "                    self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                    input_shape=(X.shape[1],\n",
    "                                                                 X.shape[2]),\n",
    "                                                    return_sequences=True,\n",
    "                                                    recurrent_dropout=dropout,\n",
    "                                                    recurrent_initializer=initializer,\n",
    "                                                    activation=activation,\n",
    "                                                    stateful=stateful))\n",
    "            else:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                input_shape=(X.shape[1],\n",
    "                                                             X.shape[2]),\n",
    "                                                return_sequences=True,\n",
    "                                                recurrent_dropout=dropout,\n",
    "                                                recurrent_initializer=initializer,\n",
    "                                                activation=activation,\n",
    "                                                stateful=stateful))\n",
    "            self.__model.add(ke.layers.Dropout(dropout))\n",
    "        if lstm_layers > 1:\n",
    "            self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                            input_shape=(X.shape[1],\n",
    "                                                         X.shape[2]),\n",
    "                                            stateful=stateful,\n",
    "                                            recurrent_dropout=dropout,\n",
    "                                            recurrent_initializer=initializer,\n",
    "                                            activation=activation,\n",
    "                                            return_sequences=False))\n",
    "        else:\n",
    "            if stateful:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                batch_input_shape=(X.shape[0],\n",
    "                                                                   X.shape[1],\n",
    "                                                                   X.shape[2]),\n",
    "                                                stateful=stateful,\n",
    "                                                recurrent_dropout=dropout,\n",
    "                                                recurrent_initializer=initializer,\n",
    "                                                activation=activation,\n",
    "                                                return_sequences=False))\n",
    "            else:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                input_shape=(X.shape[1],\n",
    "                                                             X.shape[2]),\n",
    "                                                stateful=stateful,\n",
    "                                                recurrent_dropout=dropout,\n",
    "                                                recurrent_initializer=initializer,\n",
    "                                                activation=activation,\n",
    "                                                return_sequences=False))\n",
    "        self.__model.add(ke.layers.Dropout(dropout))\n",
    "        # self.__model.add(ke.layers.TimeDistributed(ke.layers.Dense(self.__lag * len(self.__y_labels), kernel_initializer=initializer)))\n",
    "        self.__model.add(ke.layers.Dense(y.shape[1],\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='sigmoid'))\n",
    "        self.__model.compile(loss=loss_func, optimizer=optimizer, metrics=['mse','mae'])\n",
    "        print(self.__model.summary())\n",
    "\n",
    "    def fit_model(self, X_train=None, X_test=None, y_train=None, y_test=None, epochs=50, batch_size=50, verbose=2,\n",
    "                  shuffle=False, plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "\n",
    "        :param: X_train    - (Numpy 2D Array) Numpy matrix consisting of input training features\n",
    "        :param: X_test     - (Numpy 2D Array) Numpy matrix consisting of input validation/testing features\n",
    "        :param: y_train    - (Numpy 2D Array) Numpy matrix consisting of output training labels\n",
    "        :param: y_test     - (Numpy 2D Array) Numpy matrix consisting of output validation/testing labels\n",
    "        :param: epochs     - (Integer) Integer value denoting number of trained epochs\n",
    "        :param: batch_size - (Integer) Integer value denoting LSTM training batch_size\n",
    "        :param: verbose    - (Integer) Integer value denoting net verbosity (Amount of information shown to user during LSTM training)\n",
    "        :param: shuffle    - (Bool) Boolean value denoting whether or not to shuffle data. This parameter must always remain 'False' for time series datasets.\n",
    "        :param: plot       - (Bool) Boolean value denoting whether this function should plot out it's evaluation\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if X_test is not None and y_test is not None:\n",
    "            history = self.__model.fit(x=X_train,\n",
    "                                       y=y_train,\n",
    "                                       epochs=epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       validation_data=(X_test, y_test),\n",
    "                                       verbose=verbose,\n",
    "                                       shuffle=shuffle)\n",
    "        else:\n",
    "            history = self.__model.fit(x=X_train,\n",
    "                                       y=y_train,\n",
    "                                       epochs=epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       verbose=verbose,\n",
    "                                       shuffle=shuffle)\n",
    "\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(history.history['mean_squared_error'], label='mean_squared_error')\n",
    "            plt.plot(history.history['mean_absolute_error'], label='mean_absolute_error')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, X, batch_size):\n",
    "        \"\"\"\n",
    "        Predicts label/s from input feature 'X'\n",
    "        :param: X - Numpy matrix consisting of a single feature vector\n",
    "        :param: batch_size - (Integer) Denotes prediction batch size\n",
    "        :return: Numpy matrix of predicted label output\n",
    "        \"\"\"\n",
    "        yhat = self.__model.predict(X, batch_size=batch_size)\n",
    "        return yhat\n",
    "\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, batch, dropout, epoch, layer, activation, initializer,\n",
    "                              stateful, rmse, accuracy, f_score, time_train):\n",
    "        \"\"\"\n",
    "        Static method which is used for test harness utilities. This method attempts a grid search across many\n",
    "        trained LSTM models, each denoted with different configurations.\n",
    "\n",
    "        Attempted configurations:\n",
    "        * Varied data test split\n",
    "        * Varied batch sizes\n",
    "        * Varied epoch counts\n",
    "\n",
    "        Each configuration is denoted with a score, and used to identify the most optimal configuration.\n",
    "\n",
    "        :param: path       - (String) String denoting result csv output.\n",
    "        :param: iteration  - (Integer) Integer denoting test iteration (Unique per test configuration).\n",
    "        :param: lag        - (Integer) Denotes lag time shift\n",
    "        :param: test_split - (Float) Float denoting data sample sizes.\n",
    "        :param: batch      - (Integer) Integer denoting LSTM batch size.\n",
    "        :param: epoch      - (Integer) Integer denoting number of LSTM training iterations.\n",
    "        :param: layer      - (Integer) Integer denoting number of LSTM layers\n",
    "        :param: activation - (String) String denoting activation for LSTM layers.\n",
    "        :param: initializer- (String) String denoting LSTM initializing weights.\n",
    "        :param: stateful   - (Bool) Boolean flag which denotes whether LSTM model is trained in stateful mode or not.\n",
    "        :param: dropout    - (Float) Float denoting model dropout layer.\n",
    "        :param: rmse       - (Float) Float denoting experiment configuration RSME score.\n",
    "        :param: accuracy   - (Float) Float denoting experiment accuracy score.\n",
    "        :param: fscore     - (Float) Float denoting experiment fscore score.\n",
    "        :param: time_train - (Integer) Integer denoting number of seconds taken by LSTM training iteration.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a+') as csvfile:\n",
    "            headers = ['iteration', 'test_split', 'batch', 'epoch', 'layer', 'stateful', 'dropout', 'activation',\n",
    "                       'initializer', 'rmse', 'accuracy', 'f_score', 'time_train', 'lag']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'test_split': test_split,\n",
    "                             'batch': batch,\n",
    "                             'epoch': epoch,\n",
    "                             'layer': layer,\n",
    "                             'stateful': stateful,\n",
    "                             'dropout': dropout,\n",
    "                             'activation': activation,\n",
    "                             'initializer': initializer,\n",
    "                             'rmse': rmse,\n",
    "                             'accuracy': accuracy,\n",
    "                             'f_score': f_score,\n",
    "                             'time_train': time_train,\n",
    "                             'lag': lag})\n",
    "\n",
    "    @staticmethod\n",
    "    def lag_multiple(X, lag):\n",
    "        \"\"\"\n",
    "        Divides the total number of rows by the lag value, until a perfect multiple amount is retrieved.\n",
    "        :param X: (Numpy) 2D array consisting of input.\n",
    "        :param lag: (Integer) Denotes time shift value.\n",
    "        :return: (Numpy) 2D array consisting of a perfect lag multiple rows.\n",
    "        \"\"\"\n",
    "        n_rows = X.shape[0]\n",
    "        multiple = int(n_rows/lag)\n",
    "        max_new_rows = multiple * lag\n",
    "        return X[0:max_new_rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Step (1) - Training (CPU)\n",
    "\n",
    "Timestep unrolls the LSTM model N steps during the training phase. This means that the model is influenced from prior data points during training. Training is carried out on half the trace dataset, which equates to about 7 days. The rest is left out, so as to validate the model's accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_labels2 = []\n",
    "for label in y_df.columns:\n",
    "    if 'var1' in label:\n",
    "        y_labels2.append(label)\n",
    "y_df2 = y_df[y_labels2]\n",
    "timestep = 1\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df2, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=timestep)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=timestep)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=timestep)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=timestep)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / timestep), timestep, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / timestep),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / timestep), timestep, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / timestep),:]             \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "   \n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='mean_squared_error',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layer,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             y_labels=y_label,\n",
    "             initializer=initializer)\n",
    "\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep (1) - Evaluation  (CPU)\n",
    "\n",
    "The remaining left out data (the other half of t he dataset) is used to validate the trained model. This validation set equates to 7 days worth of data points. Below's test replicates a 7 day distribution, wherein the accuracy and f1 score of the model are evaluated at the end of each day, fit to the model in an online manner, and the process repeated for each of the following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n=7\n",
    "accuracy_per_day, f1score_per_day = [], []\n",
    "for i in range(0, n):\n",
    "    \n",
    "    print('Day ' + str(i+1))\n",
    "    \n",
    "    # Segregate data for specific day\n",
    "    X_validate_temp = X_validate[(int(X_validate.shape[0]/n)*i):(int(X_validate.shape[0]/n)*(i+1)),:]\n",
    "    y_validate_temp = y_validate[(int(y_validate.shape[0]/n)*i):(int(y_validate.shape[0]/n)*(i+1)),:]\n",
    "    print('Feature vectors: ' + str(X_validate_temp.shape))\n",
    "    print('Label vectors: ' + str(y_validate_temp.shape))\n",
    "\n",
    "    y_list, yhat_list = [], []\n",
    "    for i in range(0, X_validate_temp.shape[0]):\n",
    "        \n",
    "        X = np.array(np.array(X_validate_temp[i,:]))\n",
    "        X = X.reshape((int(X.shape[0] / timestep), timestep, X.shape[1]))\n",
    "        y = np.array(y_validate_temp[i, :])\n",
    "        yhat = model.predict(X, batch_size=batch)\n",
    "        \n",
    "        y = y.reshape(1,-1)\n",
    "        model.fit_model(X_train=X,\n",
    "                        y_train=y,\n",
    "                        epochs=2, \n",
    "                        batch_size=1,\n",
    "                        verbose=0, \n",
    "                        shuffle=False,\n",
    "                        plot=False) # Online Learning, Training on validation predictions. \n",
    "        \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "        \n",
    "        for i in range(yhat.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                #print('CPU')\n",
    "                y[i] = BinClass.discretize_value(y[i], cpu_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], cpu_avg)\n",
    "            else:\n",
    "                #print('IO')\n",
    "                y[i] = BinClass.discretize_value(y[i], io_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], io_avg)\n",
    "        y_list.append(y)\n",
    "        yhat_list.append(yhat)\n",
    "        \n",
    "#         print('Actual: ' + str(y))\n",
    "#         print('Predicted: ' + str(yhat) + '\\n--------------------------')\n",
    "    \n",
    "    y_list = np.array(y_list)\n",
    "    yhat_list = np.array(yhat_list)\n",
    "    \n",
    "    acc_score_list, f1_score_list = [], []\n",
    "    for i in range(len(y_label2)):\n",
    "        print('Label: ' + str(i))\n",
    "        acc = accuracy_score(y_list[:,i],yhat_list[:,i])\n",
    "        f1 = f1_score(y_list[:,i],yhat_list[:,i], average='binary')\n",
    "        print('Accuracy: ' + str(acc) + '\\nF1Score: ' +  str(f1) + '\\n--------------------------')\n",
    "        acc_score_list.append(acc)\n",
    "        f1_score_list.append(f1)\n",
    "    accuracy_per_day.append(sum(acc_score_list)/len(acc_score_list))\n",
    "    f1score_per_day.append(sum(f1_score_list)/len(f1_score_list))\n",
    "    print('Averaged Day ' + str(i+1) + ' Accuracy: ' + str(sum(accuracy_per_day)/len(accuracy_per_day)) + '\\nF1Score: ' + str(sum(f1score_per_day)/len(f1score_per_day)) + '\\n--------------------------')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring per day  (CPU)\n",
    "\n",
    "The following plot exhibits the general effectiveness of the model over the subsequent week upon which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.plot(accuracy_per_day, label='accuracy')\n",
    "plt.plot(f1score_per_day, label='f1score')\n",
    "plt.legend(['accuracy', 'f1score'], loc='upper left')\n",
    "plt.xlabel('Distribution over days')\n",
    "plt.ylabel('Score')\n",
    "plt.title(tpcds + ' model scoring over subsequent week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Step (1) - Training (IO)\n",
    "\n",
    "Timestep unrolls the LSTM model N steps during the training phase. This means that the model is influenced from prior data points during training. Training is carried out on half the trace dataset, which equates to about 7 days. The rest is left out, so as to validate the model's accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_labels2 = []\n",
    "for label in y_df.columns:\n",
    "    if 'var2' in label:\n",
    "        y_labels2.append(label)\n",
    "y_df2 = y_df[y_labels2]\n",
    "timestep = 1\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df2, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=timestep)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=timestep)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=timestep)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=timestep)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / timestep), timestep, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / timestep),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / timestep), timestep, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / timestep),:]             \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "   \n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='mean_squared_error',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layer,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             y_labels=y_label,\n",
    "             initializer=initializer)\n",
    "\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep (1) - Evaluation  (IO)\n",
    "\n",
    "The remaining left out data (the other half of t he dataset) is used to validate the trained model. This validation set equates to 7 days worth of data points. Below's test replicates a 7 day distribution, wherein the accuracy and f1 score of the model are evaluated at the end of each day, fit to the model in an online manner, and the process repeated for each of the following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n=7\n",
    "accuracy_per_day, f1score_per_day = [], []\n",
    "for i in range(0, n):\n",
    "    \n",
    "    print('Day ' + str(i+1))\n",
    "    \n",
    "    # Segregate data for specific day\n",
    "    X_validate_temp = X_validate[(int(X_validate.shape[0]/n)*i):(int(X_validate.shape[0]/n)*(i+1)),:]\n",
    "    y_validate_temp = y_validate[(int(y_validate.shape[0]/n)*i):(int(y_validate.shape[0]/n)*(i+1)),:]\n",
    "    print('Feature vectors: ' + str(X_validate_temp.shape))\n",
    "    print('Label vectors: ' + str(y_validate_temp.shape))\n",
    "\n",
    "    y_list, yhat_list = [], []\n",
    "    for i in range(0, X_validate_temp.shape[0]):\n",
    "        \n",
    "        X = np.array(np.array(X_validate_temp[i,:]))\n",
    "        X = X.reshape((int(X.shape[0] / timestep), timestep, X.shape[1]))\n",
    "        y = np.array(y_validate_temp[i, :])\n",
    "        yhat = model.predict(X, batch_size=batch)\n",
    "        \n",
    "        y = y.reshape(1,-1)\n",
    "        model.fit_model(X_train=X,\n",
    "                        y_train=y,\n",
    "                        epochs=2, \n",
    "                        batch_size=1,\n",
    "                        verbose=0, \n",
    "                        shuffle=False,\n",
    "                        plot=False) # Online Learning, Training on validation predictions. \n",
    "        \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "        \n",
    "        for i in range(yhat.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                #print('CPU')\n",
    "                y[i] = BinClass.discretize_value(y[i], cpu_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], cpu_avg)\n",
    "            else:\n",
    "                #print('IO')\n",
    "                y[i] = BinClass.discretize_value(y[i], io_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], io_avg)\n",
    "        y_list.append(y)\n",
    "        yhat_list.append(yhat)\n",
    "        \n",
    "#         print('Actual: ' + str(y))\n",
    "#         print('Predicted: ' + str(yhat) + '\\n--------------------------')\n",
    "    \n",
    "    y_list = np.array(y_list)\n",
    "    yhat_list = np.array(yhat_list)\n",
    "    \n",
    "    acc_score_list, f1_score_list = [], []\n",
    "    for i in range(len(y_label2)):\n",
    "        print('Label: ' + str(i))\n",
    "        acc = accuracy_score(y_list[:,i],yhat_list[:,i])\n",
    "        f1 = f1_score(y_list[:,i],yhat_list[:,i], average='binary')\n",
    "        print('Accuracy: ' + str(acc) + '\\nF1Score: ' +  str(f1) + '\\n--------------------------')\n",
    "        acc_score_list.append(acc)\n",
    "        f1_score_list.append(f1)\n",
    "    accuracy_per_day.append(sum(acc_score_list)/len(acc_score_list))\n",
    "    f1score_per_day.append(sum(f1_score_list)/len(f1_score_list))\n",
    "    print('Averaged Day ' + str(i+1) + ' Accuracy: ' + str(sum(accuracy_per_day)/len(accuracy_per_day)) + '\\nF1Score: ' + str(sum(f1score_per_day)/len(f1score_per_day)) + '\\n--------------------------')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring per day  (IO)\n",
    "\n",
    "The following plot exhibits the general effectiveness of the model over the subsequent week upon which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.plot(accuracy_per_day, label='accuracy')\n",
    "plt.plot(f1score_per_day, label='f1score')\n",
    "plt.legend(['accuracy', 'f1score'], loc='upper left')\n",
    "plt.xlabel('Distribution over days')\n",
    "plt.ylabel('Score')\n",
    "plt.title(tpcds + ' model scoring over subsequent week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Step (1) - Training (CPU + IO)\n",
    "\n",
    "Timestep unrolls the LSTM model N steps during the training phase. This means that the model is influenced from prior data points during training. Training is carried out on half the trace dataset, which equates to about 7 days. The rest is left out, so as to validate the model's accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "timestep = 1\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=timestep)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=timestep)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=timestep)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=timestep)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / timestep), timestep, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / timestep),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / timestep), timestep, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / timestep),:]             \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "   \n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='mean_squared_error',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layer,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             y_labels=y_label,\n",
    "             initializer=initializer)\n",
    "\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep (1) - Evaluation  (CPU + IO)\n",
    "\n",
    "The remaining left out data (the other half of t he dataset) is used to validate the trained model. This validation set equates to 7 days worth of data points. Below's test replicates a 7 day distribution, wherein the accuracy and f1 score of the model are evaluated at the end of each day, fit to the model in an online manner, and the process repeated for each of the following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n=7\n",
    "accuracy_per_day, f1score_per_day = [], []\n",
    "for i in range(0, n):\n",
    "    \n",
    "    print('Day ' + str(i+1))\n",
    "    \n",
    "    # Segregate data for specific day\n",
    "    X_validate_temp = X_validate[(int(X_validate.shape[0]/n)*i):(int(X_validate.shape[0]/n)*(i+1)),:]\n",
    "    y_validate_temp = y_validate[(int(y_validate.shape[0]/n)*i):(int(y_validate.shape[0]/n)*(i+1)),:]\n",
    "    print('Feature vectors: ' + str(X_validate_temp.shape))\n",
    "    print('Label vectors: ' + str(y_validate_temp.shape))\n",
    "\n",
    "    y_list, yhat_list = [], []\n",
    "    for i in range(0, X_validate_temp.shape[0]):\n",
    "        \n",
    "        X = np.array(np.array(X_validate_temp[i,:]))\n",
    "        X = X.reshape((int(X.shape[0] / timestep), timestep, X.shape[1]))\n",
    "        y = np.array(y_validate_temp[i, :])\n",
    "        yhat = model.predict(X, batch_size=batch)\n",
    "        \n",
    "        y = y.reshape(1,-1)\n",
    "        model.fit_model(X_train=X,\n",
    "                        y_train=y,\n",
    "                        epochs=2, \n",
    "                        batch_size=1,\n",
    "                        verbose=0, \n",
    "                        shuffle=False,\n",
    "                        plot=False) # Online Learning, Training on validation predictions. \n",
    "        \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "        \n",
    "        for i in range(yhat.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                #print('CPU')\n",
    "                y[i] = BinClass.discretize_value(y[i], cpu_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], cpu_avg)\n",
    "            else:\n",
    "                #print('IO')\n",
    "                y[i] = BinClass.discretize_value(y[i], io_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], io_avg)\n",
    "        y_list.append(y)\n",
    "        yhat_list.append(yhat)\n",
    "        \n",
    "#         print('Actual: ' + str(y))\n",
    "#         print('Predicted: ' + str(yhat) + '\\n--------------------------')\n",
    "    \n",
    "    y_list = np.array(y_list)\n",
    "    yhat_list = np.array(yhat_list)\n",
    "    \n",
    "    acc_score_list, f1_score_list = [], []\n",
    "    for i in range(lag * len(y_label)):\n",
    "        print('Label: ' + str(i))\n",
    "        acc = accuracy_score(y_list[:,i],yhat_list[:,i])\n",
    "        f1 = f1_score(y_list[:,i],yhat_list[:,i], average='binary')\n",
    "        print('Accuracy: ' + str(acc) + '\\nF1Score: ' +  str(f1) + '\\n--------------------------')\n",
    "        acc_score_list.append(acc)\n",
    "        f1_score_list.append(f1)\n",
    "    accuracy_per_day.append(sum(acc_score_list)/len(acc_score_list))\n",
    "    f1score_per_day.append(sum(f1_score_list)/len(f1_score_list))\n",
    "    print('Averaged Day ' + str(i+1) + ' Accuracy: ' + str(sum(accuracy_per_day)/len(accuracy_per_day)) + '\\nF1Score: ' + str(sum(f1score_per_day)/len(f1score_per_day)) + '\\n--------------------------')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring per day  (CPU + IO)\n",
    "\n",
    "The following plot exhibits the general effectiveness of the model over the subsequent week upon which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.plot(accuracy_per_day, label='accuracy')\n",
    "plt.plot(f1score_per_day, label='f1score')\n",
    "plt.legend(['accuracy', 'f1score'], loc='upper left')\n",
    "plt.xlabel('Distribution over days')\n",
    "plt.ylabel('Score')\n",
    "plt.title(tpcds + ' model scoring over subsequent week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Step (2) - Training  (CPU + IO)\n",
    "\n",
    "Timestep unrolls the LSTM model N steps during the training phase. This means that the model is influenced from prior data points during training. Training is carried out on half the trace dataset, which equates to about 7 days. The rest is left out, so as to validate the model's accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "timestep = 2\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=timestep)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=timestep)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=timestep)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=timestep)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / timestep), timestep, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / timestep),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / timestep), timestep, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / timestep),:]             \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "   \n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='mean_squared_error',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layer,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             y_labels=y_label,\n",
    "             initializer=initializer)\n",
    "\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep (2) - Evaluation  (CPU + IO)\n",
    "\n",
    "The remaining left out data (the other half of t he dataset) is used to validate the trained model. This validation set equates to 7 days worth of data points. Below's test replicates a 7 day distribution, wherein the accuracy and f1 score of the model are evaluated at the end of each day, fit to the model in an online manner, and the process repeated for each of the following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n=7\n",
    "accuracy_per_day, f1score_per_day = [], []\n",
    "for i in range(0, n):\n",
    "    \n",
    "    print('Day ' + str(i+1))\n",
    "    \n",
    "    # Segregate data for specific day\n",
    "    X_validate_temp = X_validate[(int(X_validate.shape[0]/n)*i):(int(X_validate.shape[0]/n)*(i+1)),:]\n",
    "    y_validate_temp = y_validate[(int(y_validate.shape[0]/n)*i):(int(y_validate.shape[0]/n)*(i+1)),:]\n",
    "    print('Feature vectors: ' + str(X_validate_temp.shape))\n",
    "    print('Label vectors: ' + str(y_validate_temp.shape))\n",
    "\n",
    "    y_list, yhat_list = [], []\n",
    "    for i in range(0, X_validate_temp.shape[0]):\n",
    "        \n",
    "        X = np.array(np.array(X_validate_temp[i,:]))\n",
    "        X = X.reshape((int(X.shape[0] / timestep), timestep, X.shape[1]))\n",
    "        y = np.array(y_validate_temp[i, :])\n",
    "        yhat = model.predict(X, batch_size=batch)\n",
    "        \n",
    "        y = y.reshape(1,-1)\n",
    "        model.fit_model(X_train=X,\n",
    "                        y_train=y,\n",
    "                        epochs=2, \n",
    "                        batch_size=1,\n",
    "                        verbose=0, \n",
    "                        shuffle=False,\n",
    "                        plot=False) # Online Learning, Training on validation predictions. \n",
    "        \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "        \n",
    "        for i in range(yhat.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                #print('CPU')\n",
    "                y[i] = BinClass.discretize_value(y[i], cpu_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], cpu_avg)\n",
    "            else:\n",
    "                #print('IO')\n",
    "                y[i] = BinClass.discretize_value(y[i], io_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], io_avg)\n",
    "        y_list.append(y)\n",
    "        yhat_list.append(yhat)\n",
    "        \n",
    "#         print('Actual: ' + str(y))\n",
    "#         print('Predicted: ' + str(yhat) + '\\n--------------------------')\n",
    "    \n",
    "    y_list = np.array(y_list)\n",
    "    yhat_list = np.array(yhat_list)\n",
    "    \n",
    "    acc_score_list, f1_score_list = [], []\n",
    "    for i in range(lag * len(y_label)):\n",
    "        print('Label: ' + str(i))\n",
    "        acc = accuracy_score(y_list[:,i],yhat_list[:,i])\n",
    "        f1 = f1_score(y_list[:,i],yhat_list[:,i], average='binary')\n",
    "        print('Accuracy: ' + str(acc) + '\\nF1Score: ' +  str(f1) + '\\n--------------------------')\n",
    "        acc_score_list.append(acc)\n",
    "        f1_score_list.append(f1)\n",
    "    accuracy_per_day.append(sum(acc_score_list)/len(acc_score_list))\n",
    "    f1score_per_day.append(sum(f1_score_list)/len(f1_score_list))\n",
    "    print('Averaged Day ' + str(i+1) + ' Accuracy: ' + str(sum(accuracy_per_day)/len(accuracy_per_day)) + '\\nF1Score: ' + str(sum(f1score_per_day)/len(f1score_per_day)) + '\\n--------------------------')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring per day  (CPU + IO)\n",
    "\n",
    "The following plot exhibits the general effectiveness of the model over the subsequent week upon which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.plot(accuracy_per_day, label='accuracy')\n",
    "plt.plot(f1score_per_day, label='f1score')\n",
    "plt.legend(['accuracy', 'f1score'], loc='upper left')\n",
    "plt.xlabel('Distribution over days')\n",
    "plt.ylabel('Score')\n",
    "plt.title(tpcds + ' model scoring over subsequent week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Step (3) - Training  (CPU + IO)\n",
    "\n",
    "Timestep unrolls the LSTM model N steps during the training phase. This means that the model is influenced from prior data points during training. Training is carried out on half the trace dataset, which equates to about 7 days. The rest is left out, so as to validate the model's accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "timestep = 3\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=timestep)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=timestep)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=timestep)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=timestep)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / timestep), timestep, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / timestep),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / timestep), timestep, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / timestep),:]             \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "   \n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='mean_squared_error',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layer,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             y_labels=y_label,\n",
    "             initializer=initializer)\n",
    "\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep (3) - Evaluation  (CPU + IO)\n",
    "\n",
    "The remaining left out data (the other half of t he dataset) is used to validate the trained model. This validation set equates to 7 days worth of data points. Below's test replicates a 7 day distribution, wherein the accuracy and f1 score of the model are evaluated at the end of each day, fit to the model in an online manner, and the process repeated for each of the following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n=7\n",
    "accuracy_per_day, f1score_per_day = [], []\n",
    "for i in range(0, n):\n",
    "    \n",
    "    print('Day ' + str(i+1))\n",
    "    \n",
    "    # Segregate data for specific day\n",
    "    X_validate_temp = X_validate[(int(X_validate.shape[0]/n)*i):(int(X_validate.shape[0]/n)*(i+1)),:]\n",
    "    y_validate_temp = y_validate[(int(y_validate.shape[0]/n)*i):(int(y_validate.shape[0]/n)*(i+1)),:]\n",
    "    print('Feature vectors: ' + str(X_validate_temp.shape))\n",
    "    print('Label vectors: ' + str(y_validate_temp.shape))\n",
    "\n",
    "    y_list, yhat_list = [], []\n",
    "    for i in range(0, X_validate_temp.shape[0]):\n",
    "        \n",
    "        X = np.array(np.array(X_validate_temp[i,:]))\n",
    "        X = X.reshape((int(X.shape[0] / timestep), timestep, X.shape[1]))\n",
    "        y = np.array(y_validate_temp[i, :])\n",
    "        yhat = model.predict(X, batch_size=batch)\n",
    "        \n",
    "        y = y.reshape(1,-1)\n",
    "        model.fit_model(X_train=X,\n",
    "                        y_train=y,\n",
    "                        epochs=2, \n",
    "                        batch_size=1,\n",
    "                        verbose=0, \n",
    "                        shuffle=False,\n",
    "                        plot=False) # Online Learning, Training on validation predictions. \n",
    "        \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "        \n",
    "        for i in range(yhat.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                #print('CPU')\n",
    "                y[i] = BinClass.discretize_value(y[i], cpu_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], cpu_avg)\n",
    "            else:\n",
    "                #print('IO')\n",
    "                y[i] = BinClass.discretize_value(y[i], io_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], io_avg)\n",
    "        y_list.append(y)\n",
    "        yhat_list.append(yhat)\n",
    "        \n",
    "#         print('Actual: ' + str(y))\n",
    "#         print('Predicted: ' + str(yhat) + '\\n--------------------------')\n",
    "    \n",
    "    y_list = np.array(y_list)\n",
    "    yhat_list = np.array(yhat_list)\n",
    "    \n",
    "    acc_score_list, f1_score_list = [], []\n",
    "    for i in range(lag * len(y_label)):\n",
    "        print('Label: ' + str(i))\n",
    "        acc = accuracy_score(y_list[:,i],yhat_list[:,i])\n",
    "        f1 = f1_score(y_list[:,i],yhat_list[:,i], average='binary')\n",
    "        print('Accuracy: ' + str(acc) + '\\nF1Score: ' +  str(f1) + '\\n--------------------------')\n",
    "        acc_score_list.append(acc)\n",
    "        f1_score_list.append(f1)\n",
    "    accuracy_per_day.append(sum(acc_score_list)/len(acc_score_list))\n",
    "    f1score_per_day.append(sum(f1_score_list)/len(f1_score_list))\n",
    "    print('Averaged Day ' + str(i+1) + ' Accuracy: ' + str(sum(accuracy_per_day)/len(accuracy_per_day)) + '\\nF1Score: ' + str(sum(f1score_per_day)/len(f1score_per_day)) + '\\n--------------------------')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring per day  (CPU + IO)\n",
    "\n",
    "The following plot exhibits the general effectiveness of the model over the subsequent week upon which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.plot(accuracy_per_day, label='accuracy')\n",
    "plt.plot(f1score_per_day, label='f1score')\n",
    "plt.legend(['accuracy', 'f1score'], loc='upper left')\n",
    "plt.xlabel('Distribution over days')\n",
    "plt.ylabel('Score')\n",
    "plt.title(tpcds + ' model scoring over subsequent week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Step (Equal to Lag) - Training  (CPU + IO)\n",
    "\n",
    "Timestep unrolls the LSTM model N steps during the training phase. This means that the model is influenced from prior data points during training. Training is carried out on half the trace dataset, which equates to about 7 days. The rest is left out, so as to validate the model's accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "timestep = lag\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=timestep)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=timestep)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=timestep)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=timestep)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / timestep), timestep, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / timestep),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / timestep), timestep, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / timestep),:]             \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "   \n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='mean_squared_error',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layer,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             y_labels=y_label,\n",
    "             initializer=initializer)\n",
    "\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep (Equal to Lag) - Evaluation  (CPU + IO)\n",
    "\n",
    "The remaining left out data (the other half of t he dataset) is used to validate the trained model. This validation set equates to 7 days worth of data points. Below's test replicates a 7 day distribution, wherein the accuracy and f1 score of the model are evaluated at the end of each day, fit to the model in an online manner, and the process repeated for each of the following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n=7\n",
    "accuracy_per_day, f1score_per_day = [], []\n",
    "for i in range(0, n):\n",
    "    \n",
    "    print('Day ' + str(i+1))\n",
    "    \n",
    "    # Segregate data for specific day\n",
    "    X_validate_temp = X_validate[(int(X_validate.shape[0]/n)*i):(int(X_validate.shape[0]/n)*(i+1)),:]\n",
    "    y_validate_temp = y_validate[(int(y_validate.shape[0]/n)*i):(int(y_validate.shape[0]/n)*(i+1)),:]\n",
    "    print('Feature vectors: ' + str(X_validate_temp.shape))\n",
    "    print('Label vectors: ' + str(y_validate_temp.shape))\n",
    "\n",
    "    y_list, yhat_list = [], []\n",
    "    for i in range(0, X_validate_temp.shape[0]):\n",
    "        \n",
    "        X = np.array(np.array(X_validate_temp[i,:]))\n",
    "        X = X.reshape((int(X.shape[0] / timestep), timestep, X.shape[1]))\n",
    "        y = np.array(y_validate_temp[i, :])\n",
    "        yhat = model.predict(X, batch_size=batch)\n",
    "        \n",
    "        y = y.reshape(1,-1)\n",
    "        model.fit_model(X_train=X,\n",
    "                        y_train=y,\n",
    "                        epochs=2, \n",
    "                        batch_size=1,\n",
    "                        verbose=0, \n",
    "                        shuffle=False,\n",
    "                        plot=False) # Online Learning, Training on validation predictions. \n",
    "        \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "        \n",
    "        for i in range(yhat.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                #print('CPU')\n",
    "                y[i] = BinClass.discretize_value(y[i], cpu_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], cpu_avg)\n",
    "            else:\n",
    "                #print('IO')\n",
    "                y[i] = BinClass.discretize_value(y[i], io_avg)\n",
    "                yhat[i] = BinClass.discretize_value(yhat[i], io_avg)\n",
    "        y_list.append(y)\n",
    "        yhat_list.append(yhat)\n",
    "        \n",
    "#         print('Actual: ' + str(y))\n",
    "#         print('Predicted: ' + str(yhat) + '\\n--------------------------')\n",
    "    \n",
    "    y_list = np.array(y_list)\n",
    "    yhat_list = np.array(yhat_list)\n",
    "    \n",
    "    acc_score_list, f1_score_list = [], []\n",
    "    for i in range(lag * len(y_label)):\n",
    "        print('Label: ' + str(i))\n",
    "        acc = accuracy_score(y_list[:,i],yhat_list[:,i])\n",
    "        f1 = f1_score(y_list[:,i],yhat_list[:,i], average='binary')\n",
    "        print('Accuracy: ' + str(acc) + '\\nF1Score: ' +  str(f1) + '\\n--------------------------')\n",
    "        acc_score_list.append(acc)\n",
    "        f1_score_list.append(f1)\n",
    "    accuracy_per_day.append(sum(acc_score_list)/len(acc_score_list))\n",
    "    f1score_per_day.append(sum(f1_score_list)/len(f1_score_list))\n",
    "    print('Averaged Day ' + str(i+1) + ' Accuracy: ' + str(sum(accuracy_per_day)/len(accuracy_per_day)) + '\\nF1Score: ' + str(sum(f1score_per_day)/len(f1score_per_day)) + '\\n--------------------------')\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring per day  (CPU + IO)\n",
    "\n",
    "The following plot exhibits the general effectiveness of the model over the subsequent week upon which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.plot(accuracy_per_day, label='accuracy')\n",
    "plt.plot(f1score_per_day, label='f1score')\n",
    "plt.legend(['accuracy', 'f1score'], loc='upper left')\n",
    "plt.xlabel('Distribution over days')\n",
    "plt.ylabel('Score')\n",
    "plt.title(tpcds + ' model scoring over subsequent week')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
