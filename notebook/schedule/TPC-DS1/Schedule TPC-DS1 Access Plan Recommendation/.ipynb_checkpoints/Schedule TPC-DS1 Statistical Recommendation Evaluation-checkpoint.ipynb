{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule TPC-DS1 Statistical Recommendation Evaluation\n",
    "\n",
    "This experiment is intended at quantifying the statistical recommendation technique, through comparison of two query streams. The query streams are denoted as follows:\n",
    "\n",
    "* Expected Stream - Denotes a sequence of baseline query plans, against which comparison will be made.\n",
    "* Variation Stream - Denotes a sequence of upcoming query plans. Queries found within the upcoming stream mirror those established in the Expected Stream, with a number of exceptions. These exceptions are considered as query variants, and contain a degree of change from the original queries taken from the prior stream.\n",
    "\n",
    "Query variants are denoted below, and are therefore eligable to be flagged during the evaluation phase:\n",
    "\n",
    "* Query 5  \n",
    "* Query 10\n",
    "* Query 14\n",
    "* Query 18\n",
    "* Query 22\n",
    "* Query 27\n",
    "* Query 35\n",
    "* Query 36\n",
    "* Query 51\n",
    "* Query 67\n",
    "* Query 70\n",
    "* Query 77\n",
    "* Query 80\n",
    "* Query 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 0.24.1\n",
      "numpy: 1.16.1\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# sklearn\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "#\n",
    "# AnyTree\n",
    "from anytree import Node, RenderTree, PostOrderIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "test_split=.2\n",
    "y_labels = ['COST',\n",
    "            'CARDINALITY',\n",
    "            'BYTES',\n",
    "            'CPU_COST',\n",
    "            'IO_COST',\n",
    "            'TEMP_SPACE',\n",
    "            'TIME']\n",
    "black_list = ['TIMESTAMP',\n",
    "              'SQL_ID',\n",
    "              'OPERATION',\n",
    "              'OPTIONS',\n",
    "              'OBJECT_NAME',\n",
    "              'OBJECT_OWNER',\n",
    "              'PARTITION_STOP',\n",
    "              'PARTITION_START'] # Columns which will be ignored during type conversion, and later used for aggregation\n",
    "nrows = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ('DBID',)    ('SQL_ID',)  ('PLAN_HASH_VALUE',)  ('ID',)    ('OPERATION',)  \\\n",
      "0  2634225673  dxv968j0352kb             103598129        0  SELECT STATEMENT   \n",
      "1  2634225673  dxv968j0352kb             103598129        1              SORT   \n",
      "2  2634225673  dxv968j0352kb             103598129        2    PX COORDINATOR   \n",
      "3  2634225673  dxv968j0352kb             103598129        3           PX SEND   \n",
      "4  2634225673  dxv968j0352kb             103598129        4              SORT   \n",
      "\n",
      "  ('OPTIONS',) ('OBJECT_NODE',)  ('OBJECT#',) ('OBJECT_OWNER',)  \\\n",
      "0          NaN              NaN           NaN               NaN   \n",
      "1     GROUP BY              NaN           NaN               NaN   \n",
      "2          NaN              NaN           NaN               NaN   \n",
      "3  QC (RANDOM)           :Q1001           NaN               SYS   \n",
      "4     GROUP BY           :Q1001           NaN               NaN   \n",
      "\n",
      "  ('OBJECT_NAME',)  ... ('ACCESS_PREDICATES',) ('FILTER_PREDICATES',)  \\\n",
      "0              NaN  ...                    NaN                    NaN   \n",
      "1              NaN  ...                    NaN                    NaN   \n",
      "2              NaN  ...                    NaN                    NaN   \n",
      "3         :TQ10001  ...                    NaN                    NaN   \n",
      "4              NaN  ...                    NaN                    NaN   \n",
      "\n",
      "  ('PROJECTION',)  ('TIME',)  ('QBLOCK_NAME',)  ('REMARKS',)  \\\n",
      "0             NaN        NaN               NaN           NaN   \n",
      "1             NaN        NaN             SEL$1           NaN   \n",
      "2             NaN        NaN               NaN           NaN   \n",
      "3             NaN        NaN               NaN           NaN   \n",
      "4             NaN        NaN               NaN           NaN   \n",
      "\n",
      "        ('TIMESTAMP',)                                     ('OTHER_XML',)  \\\n",
      "0  2018-10-07 15:52:33                                                NaN   \n",
      "1  2018-10-07 15:52:33  <other_xml><info type=\"db_version\">12.1.0.2</i...   \n",
      "2  2018-10-07 15:52:33                                                NaN   \n",
      "3  2018-10-07 15:52:33                                                NaN   \n",
      "4  2018-10-07 15:52:33                                                NaN   \n",
      "\n",
      "   ('CON_DBID',) ('CON_ID',)  \n",
      "0     2634225673           0  \n",
      "1     2634225673           0  \n",
      "2     2634225673           0  \n",
      "3     2634225673           0  \n",
      "4     2634225673           0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "------------------------------------------\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Root path\n",
    "base_dir = 'C:/Users/gabriel.sammut/University/'\n",
    "#base_dir = 'D:/Projects/ICS5200/'\n",
    "root_dir = base_dir + 'Data_ICS5200/Schedule/' + tpcds\n",
    "src_dir = base_dir + 'ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/'\n",
    "\n",
    "rep_vsql_plan_path = root_dir + '/rep_vsql_plan.csv'\n",
    "#rep_vsql_plan_path = root_dir + '/rep_vsql_plan.csv'\n",
    "\n",
    "dtype={'COST':'int64',\n",
    "       'CARDINALITY':'int64',\n",
    "       'BYTES':'int64',\n",
    "       'CPU_COST':'int64',\n",
    "       'IO_COST':'int64',\n",
    "       'TEMP_SPACE':'int64',\n",
    "       'TIME':'int64',\n",
    "       'OPERATION':'str',\n",
    "       'OBJECT_NAME':'str'}\n",
    "rep_vsql_plan_df = pd.read_csv(rep_vsql_plan_path, nrows=nrows, dtype=dtype)\n",
    "print(rep_vsql_plan_df.head())\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_vsql_plan_df.columns = prettify_header(rep_vsql_plan_df.columns.values)\n",
    "print('------------------------------------------')\n",
    "print(rep_vsql_plan_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read outlier data from file into pandas dataframes and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465, 35)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12354  11/20/2018 08:23:55     NaN  SELECT STATEMENT              NaN   \n",
      "1   12354  11/20/2018 08:23:55     NaN             COUNT          STOPKEY   \n",
      "2   12354  11/20/2018 08:23:55     NaN              VIEW              NaN   \n",
      "3   12354  11/20/2018 08:23:55     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12354  11/20/2018 08:23:55     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN       TPCDS1         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN       TPCDS1         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE  ...                                          OTHER_XML  \\\n",
      "0             NaN  ...                                                NaN   \n",
      "1             NaN  ...  <other_xml><info type=\"db_version\">12.1.0.2</i...   \n",
      "2              18  ...                                                NaN   \n",
      "3             NaN  ...                                                NaN   \n",
      "4              19  ...                                                NaN   \n",
      "\n",
      "  DISTRIBUTION    CPU_COST IO_COST TEMP_SPACE ACCESS_PREDICATES  \\\n",
      "0          NaN  1657360333   13630        NaN               NaN   \n",
      "1          NaN         NaN     NaN        NaN               NaN   \n",
      "2          NaN  1657360333   13630        NaN               NaN   \n",
      "3          NaN  1657360333   13630        NaN               NaN   \n",
      "4          NaN  1625075317   13630        NaN               NaN   \n",
      "\n",
      "  FILTER_PREDICATES                                         PROJECTION TIME  \\\n",
      "0               NaN                                                NaN    1   \n",
      "1       ROWNUM<=100  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN   \n",
      "2               NaN  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    1   \n",
      "3               NaN  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    1   \n",
      "4               NaN  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    1   \n",
      "\n",
      "  QBLOCK_NAME  \n",
      "0         NaN  \n",
      "1      SEL$11  \n",
      "2      SEL$12  \n",
      "3      SEL$12  \n",
      "4       SET$4  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "------------------------------------------\n",
      "(491, 35)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12372  11/20/2018 08:40:58     NaN  SELECT STATEMENT              NaN   \n",
      "1   12372  11/20/2018 08:40:58     NaN             COUNT          STOPKEY   \n",
      "2   12372  11/20/2018 08:40:58     NaN              VIEW              NaN   \n",
      "3   12372  11/20/2018 08:40:58     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12372  11/20/2018 08:40:58     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN       TPCDS1         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN       TPCDS1         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE  ...                                          OTHER_XML  \\\n",
      "0             NaN  ...                                                NaN   \n",
      "1             NaN  ...  <other_xml><info type=\"db_version\">12.1.0.2</i...   \n",
      "2              18  ...                                                NaN   \n",
      "3             NaN  ...                                                NaN   \n",
      "4              19  ...                                                NaN   \n",
      "\n",
      "  DISTRIBUTION   CPU_COST IO_COST TEMP_SPACE ACCESS_PREDICATES  \\\n",
      "0          NaN  242094911    2549        NaN               NaN   \n",
      "1          NaN        NaN     NaN        NaN               NaN   \n",
      "2          NaN  242094911    2549        NaN               NaN   \n",
      "3          NaN  242094911    2549        NaN               NaN   \n",
      "4          NaN  209150318    2549        NaN               NaN   \n",
      "\n",
      "  FILTER_PREDICATES                                         PROJECTION TIME  \\\n",
      "0               NaN                                                NaN    1   \n",
      "1       ROWNUM<=100  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN   \n",
      "2               NaN  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    1   \n",
      "3               NaN  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    1   \n",
      "4               NaN  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    1   \n",
      "\n",
      "  QBLOCK_NAME  \n",
      "0         NaN  \n",
      "1      SEL$11  \n",
      "2      SEL$12  \n",
      "3      SEL$12  \n",
      "4       SET$4  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "------------------------------------------\n",
      "(500, 35)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12386  11/20/2018 08:52:27     NaN  SELECT STATEMENT              NaN   \n",
      "1   12386  11/20/2018 08:52:27     NaN             COUNT          STOPKEY   \n",
      "2   12386  11/20/2018 08:52:27     NaN              VIEW              NaN   \n",
      "3   12386  11/20/2018 08:52:27     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12386  11/20/2018 08:52:27     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN       TPCDS1         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN       TPCDS1         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE  ...                                          OTHER_XML  \\\n",
      "0             NaN  ...                                                NaN   \n",
      "1             NaN  ...  <other_xml><info type=\"db_version\">12.1.0.2</i...   \n",
      "2              18  ...                                                NaN   \n",
      "3             NaN  ...                                                NaN   \n",
      "4              19  ...                                                NaN   \n",
      "\n",
      "  DISTRIBUTION   CPU_COST IO_COST TEMP_SPACE ACCESS_PREDICATES  \\\n",
      "0          NaN  226837615    1563        NaN               NaN   \n",
      "1          NaN        NaN     NaN        NaN               NaN   \n",
      "2          NaN  226837615    1563        NaN               NaN   \n",
      "3          NaN  226837615    1563        NaN               NaN   \n",
      "4          NaN  194552599    1563        NaN               NaN   \n",
      "\n",
      "  FILTER_PREDICATES                                         PROJECTION TIME  \\\n",
      "0               NaN                                                NaN    1   \n",
      "1     ROWNUM<=10000  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN   \n",
      "2               NaN  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    1   \n",
      "3               NaN  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    1   \n",
      "4               NaN  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    1   \n",
      "\n",
      "  QBLOCK_NAME  \n",
      "0         NaN  \n",
      "1      SEL$11  \n",
      "2      SEL$12  \n",
      "3      SEL$12  \n",
      "4       SET$4  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# CSV Outlier Paths\n",
    "outlier_hints_q5_path = src_dir + 'hints/output/query_5.csv'\n",
    "outlier_hints_q10_path = src_dir + 'hints/output/query_10.csv'\n",
    "outlier_hints_q14_path = src_dir + 'hints/output/query_14.csv'\n",
    "outlier_hints_q18_path = src_dir + 'hints/output/query_18.csv'\n",
    "outlier_hints_q22_path = src_dir + 'hints/output/query_22.csv'\n",
    "outlier_hints_q27_path = src_dir + 'hints/output/query_27.csv'\n",
    "outlier_hints_q35_path = src_dir + 'hints/output/query_35.csv'\n",
    "outlier_hints_q36_path = src_dir + 'hints/output/query_36.csv'\n",
    "outlier_hints_q51_path = src_dir + 'hints/output/query_51.csv'\n",
    "outlier_hints_q67_path = src_dir + 'hints/output/query_67.csv'\n",
    "outlier_hints_q70_path = src_dir + 'hints/output/query_70.csv'\n",
    "outlier_hints_q77_path = src_dir + 'hints/output/query_77.csv'\n",
    "outlier_hints_q80_path = src_dir + 'hints/output/query_80.csv'\n",
    "outlier_hints_q86_path = src_dir + 'hints/output/query_86.csv'\n",
    "#\n",
    "outlier_predicates_q5_path = src_dir + 'predicates/output/query_5.csv'\n",
    "outlier_predicates_q10_path = src_dir + 'predicates/output/query_10.csv'\n",
    "outlier_predicates_q14_path = src_dir + 'predicates/output/query_14.csv'\n",
    "outlier_predicates_q18_path = src_dir + 'predicates/output/query_18.csv'\n",
    "outlier_predicates_q22_path = src_dir + 'predicates/output/query_22.csv'\n",
    "outlier_predicates_q27_path = src_dir + 'predicates/output/query_27.csv'\n",
    "outlier_predicates_q35_path = src_dir + 'predicates/output/query_35.csv'\n",
    "outlier_predicates_q36_path = src_dir + 'predicates/output/query_36.csv'\n",
    "outlier_predicates_q51_path = src_dir + 'predicates/output/query_51.csv'\n",
    "outlier_predicates_q67_path = src_dir + 'predicates/output/query_67.csv'\n",
    "outlier_predicates_q70_path = src_dir + 'predicates/output/query_70.csv'\n",
    "outlier_predicates_q77_path = src_dir + 'predicates/output/query_77.csv'\n",
    "outlier_predicates_q80_path = src_dir + 'predicates/output/query_80.csv'\n",
    "outlier_predicates_q86_path = src_dir + 'predicates/output/query_86.csv'\n",
    "#\n",
    "outlier_rownum_q5_path = src_dir + 'rownum/output/query_5.csv'\n",
    "outlier_rownum_q10_path = src_dir + 'rownum/output/query_10.csv'\n",
    "outlier_rownum_q14_path = src_dir + 'rownum/output/query_14.csv'\n",
    "outlier_rownum_q18_path = src_dir + 'rownum/output/query_18.csv'\n",
    "outlier_rownum_q22_path = src_dir + 'rownum/output/query_22.csv'\n",
    "outlier_rownum_q27_path = src_dir + 'rownum/output/query_27.csv'\n",
    "outlier_rownum_q35_path = src_dir + 'rownum/output/query_35.csv'\n",
    "outlier_rownum_q36_path = src_dir + 'rownum/output/query_36.csv'\n",
    "outlier_rownum_q51_path = src_dir + 'rownum/output/query_51.csv'\n",
    "outlier_rownum_q67_path = src_dir + 'rownum/output/query_67.csv'\n",
    "outlier_rownum_q70_path = src_dir + 'rownum/output/query_70.csv'\n",
    "outlier_rownum_q77_path = src_dir + 'rownum/output/query_77.csv'\n",
    "outlier_rownum_q80_path = src_dir + 'rownum/output/query_80.csv'\n",
    "outlier_rownum_q86_path = src_dir + 'rownum/output/query_86.csv'\n",
    "#\n",
    "# Read CSV Paths\n",
    "outlier_hints_q5_df = pd.read_csv(outlier_hints_q5_path,dtype=str)\n",
    "outlier_hints_q10_df = pd.read_csv(outlier_hints_q10_path,dtype=str)\n",
    "outlier_hints_q14_df = pd.read_csv(outlier_hints_q14_path,dtype=str)\n",
    "outlier_hints_q18_df = pd.read_csv(outlier_hints_q18_path,dtype=str)\n",
    "outlier_hints_q22_df = pd.read_csv(outlier_hints_q22_path,dtype=str)\n",
    "outlier_hints_q27_df = pd.read_csv(outlier_hints_q27_path,dtype=str)\n",
    "outlier_hints_q35_df = pd.read_csv(outlier_hints_q35_path,dtype=str)\n",
    "outlier_hints_q36_df = pd.read_csv(outlier_hints_q36_path,dtype=str)\n",
    "outlier_hints_q51_df = pd.read_csv(outlier_hints_q51_path,dtype=str)\n",
    "outlier_hints_q67_df = pd.read_csv(outlier_hints_q67_path,dtype=str)\n",
    "outlier_hints_q70_df = pd.read_csv(outlier_hints_q70_path,dtype=str)\n",
    "outlier_hints_q77_df = pd.read_csv(outlier_hints_q77_path,dtype=str)\n",
    "outlier_hints_q80_df = pd.read_csv(outlier_hints_q80_path,dtype=str)\n",
    "outlier_hints_q86_df = pd.read_csv(outlier_hints_q86_path,dtype=str)\n",
    "#\n",
    "outlier_predicates_q5_df = pd.read_csv(outlier_predicates_q5_path,dtype=str)\n",
    "outlier_predicates_q10_df = pd.read_csv(outlier_predicates_q10_path,dtype=str)\n",
    "outlier_predicates_q14_df = pd.read_csv(outlier_predicates_q14_path,dtype=str)\n",
    "outlier_predicates_q18_df = pd.read_csv(outlier_predicates_q18_path,dtype=str)\n",
    "outlier_predicates_q22_df = pd.read_csv(outlier_predicates_q22_path,dtype=str)\n",
    "outlier_predicates_q27_df = pd.read_csv(outlier_predicates_q27_path,dtype=str)\n",
    "outlier_predicates_q35_df = pd.read_csv(outlier_predicates_q35_path,dtype=str)\n",
    "outlier_predicates_q36_df = pd.read_csv(outlier_predicates_q36_path,dtype=str)\n",
    "outlier_predicates_q51_df = pd.read_csv(outlier_predicates_q51_path,dtype=str)\n",
    "outlier_predicates_q67_df = pd.read_csv(outlier_predicates_q67_path,dtype=str)\n",
    "outlier_predicates_q70_df = pd.read_csv(outlier_predicates_q70_path,dtype=str)\n",
    "outlier_predicates_q77_df = pd.read_csv(outlier_predicates_q77_path,dtype=str)\n",
    "outlier_predicates_q80_df = pd.read_csv(outlier_predicates_q80_path,dtype=str)\n",
    "outlier_predicates_q86_df = pd.read_csv(outlier_predicates_q86_path,dtype=str)\n",
    "#\n",
    "outlier_rownum_q5_df = pd.read_csv(outlier_rownum_q5_path,dtype=str)\n",
    "outlier_rownum_q10_df = pd.read_csv(outlier_rownum_q10_path,dtype=str)\n",
    "outlier_rownum_q14_df = pd.read_csv(outlier_rownum_q14_path,dtype=str)\n",
    "outlier_rownum_q18_df = pd.read_csv(outlier_rownum_q18_path,dtype=str)\n",
    "outlier_rownum_q22_df = pd.read_csv(outlier_rownum_q22_path,dtype=str)\n",
    "outlier_rownum_q27_df = pd.read_csv(outlier_rownum_q27_path,dtype=str)\n",
    "outlier_rownum_q35_df = pd.read_csv(outlier_rownum_q35_path,dtype=str)\n",
    "outlier_rownum_q36_df = pd.read_csv(outlier_rownum_q36_path,dtype=str)\n",
    "outlier_rownum_q51_df = pd.read_csv(outlier_rownum_q51_path,dtype=str)\n",
    "outlier_rownum_q67_df = pd.read_csv(outlier_rownum_q67_path,dtype=str)\n",
    "outlier_rownum_q70_df = pd.read_csv(outlier_rownum_q70_path,dtype=str)\n",
    "outlier_rownum_q77_df = pd.read_csv(outlier_rownum_q77_path,dtype=str)\n",
    "outlier_rownum_q80_df = pd.read_csv(outlier_rownum_q80_path,dtype=str)\n",
    "outlier_rownum_q86_df = pd.read_csv(outlier_rownum_q86_path,dtype=str)\n",
    "#\n",
    "# Merge dataframes into a single pandas matrix\n",
    "df_hints_outliers = pd.concat([outlier_hints_q5_df, outlier_hints_q10_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q14_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q18_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q22_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q27_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q35_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q36_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q51_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q67_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q70_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q77_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q80_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q86_df], sort=False)\n",
    "#\n",
    "df_predicate_outliers = pd.concat([outlier_predicates_q5_df, outlier_predicates_q10_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q14_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q18_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q22_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q27_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q35_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q36_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q51_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q67_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q70_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q77_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q80_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q86_df], sort=False)\n",
    "#\n",
    "df_rownum_outliers = pd.concat([outlier_rownum_q5_df, outlier_rownum_q10_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q14_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q18_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q22_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q27_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q35_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q36_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q51_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q67_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q70_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q77_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q80_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q86_df], sort=False)\n",
    "#\n",
    "print(df_hints_outliers.shape)\n",
    "print(df_hints_outliers.head())\n",
    "print('------------------------------------------')\n",
    "print(df_predicate_outliers.shape)\n",
    "print(df_predicate_outliers.head())\n",
    "print('------------------------------------------')\n",
    "print(df_rownum_outliers.shape)\n",
    "print(df_rownum_outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A Columns\n",
      "\n",
      "\n",
      "REP_VSQL_PLAN Features 39: ['OPTIONS', 'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'COST', 'CARDINALITY', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'IO_COST', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME', 'REMARKS', 'OTHER_XML']\n",
      "\n",
      "\n",
      "DF_HINT_OUTLIERS Features 35: ['REMARKS', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME']\n",
      "\n",
      "\n",
      "DF_PREDICATE_OUTLIERS Features 35: ['REMARKS', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME']\n",
      "\n",
      "\n",
      "DF_ROWNUM_OUTLIERS Features 35: ['REMARKS', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "#\n",
    "print('N/A Columns\\n')\n",
    "print('\\nREP_VSQL_PLAN Features ' + str(len(rep_vsql_plan_df.columns)) + ': ' + str(get_na_columns(df=rep_vsql_plan_df,headers=rep_vsql_plan_df.columns)) + \"\\n\")\n",
    "print('\\nDF_HINT_OUTLIERS Features ' + str(len(df_hints_outliers.columns)) + ': ' + str(get_na_columns(df=df_hints_outliers,headers=df_hints_outliers.columns)) + \"\\n\")\n",
    "print('\\nDF_PREDICATE_OUTLIERS Features ' + str(len(df_predicate_outliers.columns)) + ': ' + str(get_na_columns(df=df_predicate_outliers,headers=df_predicate_outliers.columns)) + \"\\n\")\n",
    "print('\\nDF_ROWNUM_OUTLIERS Features ' + str(len(df_rownum_outliers.columns)) + ': ' + str(get_na_columns(df=df_rownum_outliers,headers=df_rownum_outliers.columns)) + \"\\n\")\n",
    "#\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "#\n",
    "# Populating NaN values with amount '0'\n",
    "df = fill_na(df=rep_vsql_plan_df)\n",
    "df_hints_outliers = fill_na(df=df_hints_outliers)\n",
    "df_predicate_outliers = fill_na(df=df_predicate_outliers)\n",
    "df_rownum_outliers = fill_na(df=df_rownum_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversion\n",
    "\n",
    "Each column is converted into a column of type values which are Integer64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Dropped column [OBJECT_ALIAS]\n",
      "Dropped column [OBJECT_TYPE]\n",
      "Dropped column [OPTIMIZER]\n",
      "Dropped column [OTHER_XML]\n",
      "Dropped column [ACCESS_PREDICATES]\n",
      "Dropped column [FILTER_PREDICATES]\n",
      "Dropped column [PROJECTION]\n",
      "Dropped column [QBLOCK_NAME]\n",
      "-------------------------------------------------------------\n",
      "Dropped column [OBJECT_ALIAS]\n",
      "Dropped column [OBJECT_TYPE]\n",
      "Dropped column [OPTIMIZER]\n",
      "Dropped column [OTHER_XML]\n",
      "Dropped column [ACCESS_PREDICATES]\n",
      "Dropped column [FILTER_PREDICATES]\n",
      "Dropped column [PROJECTION]\n",
      "Dropped column [QBLOCK_NAME]\n",
      "-------------------------------------------------------------\n",
      "Dropped column [OBJECT_ALIAS]\n",
      "Dropped column [OBJECT_TYPE]\n",
      "Dropped column [OPTIMIZER]\n",
      "Dropped column [OTHER_XML]\n",
      "Dropped column [ACCESS_PREDICATES]\n",
      "Dropped column [FILTER_PREDICATES]\n",
      "Dropped column [PROJECTION]\n",
      "Dropped column [QBLOCK_NAME]\n",
      "-------------------------------------------------------------\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def handle_numeric_overflows(x):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe column, and \n",
    "    \"\"\"\n",
    "    try:\n",
    "        #df = df.astype('int64')\n",
    "        x1 = pd.DataFrame([x],dtype='int64')\n",
    "    except ValueError:\n",
    "        x = 9223372036854775807 # Max int size\n",
    "    return x\n",
    "#\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df[col] = df[col].apply(handle_numeric_overflows)\n",
    "        df[col].astype('int64',inplace=True)\n",
    "    except:\n",
    "        df.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')\n",
    "#\n",
    "print('-------------------------------------------------------------')\n",
    "#\n",
    "for col in df_hints_outliers.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df_hints_outliers[col] = df_hints_outliers[col].astype('int64')\n",
    "    except OverflowError:\n",
    "        #\n",
    "        # Handles numeric overflow conversions by replacing such values with max value inside the dataset.\n",
    "        df_hints_outliers[col] = df_hints_outliers[col].apply(handle_numeric_overflows)\n",
    "        df_hints_outliers[col] = df_hints_outliers[col].astype('int64')\n",
    "    except Exception as e:\n",
    "        df_hints_outliers.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')\n",
    "#\n",
    "print('-------------------------------------------------------------')\n",
    "#\n",
    "for col in df_predicate_outliers.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df_predicate_outliers[col] = df_predicate_outliers[col].astype('int64')\n",
    "    except OverflowError:\n",
    "        #\n",
    "        # Handles numeric overflow conversions by replacing such values with max value inside the dataset.\n",
    "        df_predicate_outliers[col] = df_predicate_outliers[col].apply(handle_numeric_overflows)\n",
    "        df_predicate_outliers[col] = df_predicate_outliers[col].astype('int64')\n",
    "    except Exception as e:\n",
    "        df_predicate_outliers.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')       \n",
    "#\n",
    "print('-------------------------------------------------------------')\n",
    "#\n",
    "for col in df_rownum_outliers.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df_rownum_outliers[col] = df_rownum_outliers[col].astype('int64')\n",
    "    except OverflowError:\n",
    "        #\n",
    "        # Handles numeric overflow conversions by replacing such values with max value inside the dataset.\n",
    "        df_rownum_outliers[col] = df_rownum_outliers[col].apply(handle_numeric_overflows)\n",
    "        df_rownum_outliers[col] = df_rownum_outliers[col].astype('int64')\n",
    "    except Exception as e:\n",
    "        df_rownum_outliers.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')    \n",
    "#\n",
    "print('-------------------------------------------------------------')\n",
    "#        \n",
    "print(df.columns)\n",
    "print(df_hints_outliers.columns)\n",
    "print(df_predicate_outliers.columns)\n",
    "print(df_rownum_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Elimination\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape before changes: [(10000, 39)]\n",
      "Shape after changes: [(10000, 30)]\n",
      "Dropped a total [9]\n",
      "\n",
      "Shape before changes: [(465, 27)]\n",
      "Shape after changes: [(465, 21)]\n",
      "Dropped a total [6]\n",
      "\n",
      "Shape before changes: [(491, 27)]\n",
      "Shape after changes: [(491, 21)]\n",
      "Dropped a total [6]\n",
      "\n",
      "Shape before changes: [(500, 27)]\n",
      "Shape after changes: [(500, 21)]\n",
      "Dropped a total [6]\n",
      "\n",
      "After flatline column drop:\n",
      "(10000, 30)\n",
      "Index(['SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME', 'QBLOCK_NAME', 'TIMESTAMP',\n",
      "       'OTHER_XML'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier flatline column drop [df_hints_outliers]:\n",
      "(465, 21)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'OPERATION', 'OPTIONS', 'OBJECT_OWNER',\n",
      "       'OBJECT_NAME', 'OBJECT_INSTANCE', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID',\n",
      "       'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier flatline column drop [df_predicate_outliers]:\n",
      "(491, 21)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'OPERATION', 'OPTIONS', 'OBJECT_OWNER',\n",
      "       'OBJECT_NAME', 'OBJECT_INSTANCE', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID',\n",
      "       'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier flatline column drop [df_rownum_outliers]:\n",
      "(500, 21)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'OPERATION', 'OPTIONS', 'OBJECT_OWNER',\n",
      "       'OBJECT_NAME', 'OBJECT_INSTANCE', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID',\n",
      "       'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            #\n",
    "            if columns[i] in black_list:\n",
    "                continue\n",
    "            #\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "df = drop_flatline_columns(df=df)\n",
    "df_hints_outliers = drop_flatline_columns(df=df_hints_outliers)\n",
    "df_predicate_outliers = drop_flatline_columns(df=df_predicate_outliers)\n",
    "df_rownum_outliers = drop_flatline_columns(df=df_rownum_outliers)\n",
    "#\n",
    "print('\\nAfter flatline column drop:')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier flatline column drop [df_hints_outliers]:')\n",
    "print(df_hints_outliers.shape)\n",
    "print(df_hints_outliers.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier flatline column drop [df_predicate_outliers]:')\n",
    "print(df_predicate_outliers.shape)\n",
    "print(df_predicate_outliers.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier flatline column drop [df_rownum_outliers]:')\n",
    "print(df_rownum_outliers.shape)\n",
    "print(df_rownum_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling columns\n",
    "\n",
    "This section attempts to process a number of data columns through a MinMax Scaler. This is done, to normalize data on a similar scaler, particularly before comparing column measurements using a euclidean based measure. The following columns will be targetted:\n",
    "\n",
    "* CARDINALITY\n",
    "* BYTES\n",
    "* PARTITION_START\n",
    "* PARTITION_STOP\n",
    "* CPU_COST\n",
    "* IO_COST\n",
    "* TEMP_SPACE\n",
    "* TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "Minimal Vector Points: [0.000e+00 2.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.156e+06\n",
      " 0.000e+00]\n",
      "Maximal Vector Points: [2.85716704e+16 9.22337204e+18 0.00000000e+00 0.00000000e+00\n",
      " 9.22337204e+18 7.78425143e+09 9.22337204e+18 1.58287675e+08]\n",
      "\n",
      "After scaled column transformation:\n",
      "(10000, 30)\n",
      "Index(['SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME', 'QBLOCK_NAME', 'TIMESTAMP',\n",
      "       'OTHER_XML'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier scaled column transformation [df_hints_outliers]:\n",
      "(465, 21)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'OPERATION', 'OPTIONS', 'OBJECT_OWNER',\n",
      "       'OBJECT_NAME', 'OBJECT_INSTANCE', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID',\n",
      "       'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier scaled column transformation [df_predicate_outliers]:\n",
      "(491, 21)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'OPERATION', 'OPTIONS', 'OBJECT_OWNER',\n",
      "       'OBJECT_NAME', 'OBJECT_INSTANCE', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID',\n",
      "       'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier scaled column transformation [df_rownum_outliers]:\n",
      "(500, 21)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'OPERATION', 'OPTIONS', 'OBJECT_OWNER',\n",
      "       'OBJECT_NAME', 'OBJECT_INSTANCE', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID',\n",
      "       'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_columns = ['CARDINALITY',\n",
    "                'BYTES',\n",
    "                'PARTITION_START',\n",
    "                'PARTITION_STOP',\n",
    "                'CPU_COST',\n",
    "                'IO_COST',\n",
    "                'TEMP_SPACE',\n",
    "                'TIME']\n",
    "print(df['PARTITION_START'].iloc[0])\n",
    "df[scaled_columns] = scaler.fit_transform(df[scaled_columns])\n",
    "print(df['PARTITION_START'].iloc[0])\n",
    "print(\"Minimal Vector Points: \" + str(scaler.data_min_))\n",
    "print(\"Maximal Vector Points: \" + str(scaler.data_max_))\n",
    "#\n",
    "print('\\nAfter scaled column transformation:')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier scaled column transformation [df_hints_outliers]:')\n",
    "print(df_hints_outliers.shape)\n",
    "print(df_hints_outliers.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier scaled column transformation [df_predicate_outliers]:')\n",
    "print(df_predicate_outliers.shape)\n",
    "print(df_predicate_outliers.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier scaled column transformation [df_rownum_outliers]:')\n",
    "print(df_rownum_outliers.shape)\n",
    "print(df_rownum_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Grouping Column\n",
    "\n",
    "An extra column is added to allow access plans to be isolated per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before transformation: (10000, 30)\n",
      "Shape after transformation: (10000, 31)\n",
      "Shape before transformation: (465, 21)\n",
      "Shape after transformation: (465, 22)\n",
      "Shape before transformation: (491, 21)\n",
      "Shape after transformation: (491, 22)\n",
      "Shape before transformation: (500, 21)\n",
      "Shape after transformation: (500, 22)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Adds a columns per SQL_ID, PLAN_HASH_VALUE grouping, which can be used to group instances together\n",
    "def add_grouping_column(df, column_identifier):\n",
    "    \"\"\"\n",
    "    Receives a pandas dataframe, and adds a new column which allows dataframe to be aggregated per \n",
    "    SQL_ID, PLAN_HASH_VALUE combination.\n",
    "    \n",
    "    :param: df                - Pandas Dataframe\n",
    "    :param: column_identifier - String denoting matrix column to group by\n",
    "    \n",
    "    :return: Pandas Dataframe, with added column    \n",
    "    \"\"\"\n",
    "    print('Shape before transformation: ' + str(df.shape))\n",
    "    new_grouping_col = []\n",
    "    counter = 0\n",
    "    last_sql_id = df[column_identifier].iloc(0) # Starts with first SQL_ID\n",
    "    for index, row in df.iterrows():\n",
    "        if column_identifier == 'SQL_ID':\n",
    "            if last_sql_id != row.SQL_ID:\n",
    "                last_sql_id = row.SQL_ID\n",
    "                counter += 1\n",
    "        elif column_identifier == 'PLAN_ID':\n",
    "            if last_sql_id != row.PLAN_ID:\n",
    "                last_sql_id = row.PLAN_ID\n",
    "                counter += 1\n",
    "        else:\n",
    "            raise ValueError('Column does not exist!')\n",
    "        new_grouping_col.append(counter)\n",
    "    #\n",
    "    # Append list as new column\n",
    "    new_col = pd.Series(new_grouping_col)\n",
    "    df['PLAN_INSTANCE'] = new_col.values\n",
    "    print('Shape after transformation: ' + str(df.shape))\n",
    "    return df\n",
    "#\n",
    "df = add_grouping_column(df=df,column_identifier='SQL_ID')\n",
    "df_hints_outliers = add_grouping_column(df=df_hints_outliers,column_identifier='PLAN_ID')\n",
    "df_predicate_outliers = add_grouping_column(df=df_predicate_outliers,column_identifier='PLAN_ID')\n",
    "df_rownum_outliers = add_grouping_column(df=df_rownum_outliers,column_identifier='PLAN_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Formatting\n",
    "\n",
    "Constructs the tree plan structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanTreeModeller:\n",
    "    \"\"\"\n",
    "    This class simulates an access plan in the form of a tree structure\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def __create_node(node_name, parent=None):\n",
    "        \"\"\"\n",
    "        Builds a node which will be added to the tree. If the parent is 'None', it is assumed that this\n",
    "        node will be used as the root/parent Node.\n",
    "        \n",
    "        :param: node_name - String specifying node name.\n",
    "        :param: parent    - Parent node specifying parent node name.\n",
    "        \n",
    "        :return: anytree object\n",
    "        \"\"\"\n",
    "        if node_name is None:\n",
    "            raise ValueError('Node name was not specified!')\n",
    "        \n",
    "        if parent is None:\n",
    "            node = Node(node_name)\n",
    "        else:\n",
    "            node = Node(node_name, parent=parent)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_tree(df):\n",
    "        \"\"\"\n",
    "        This method receives a pandas dataframe, and converts it into a searchable python tree\n",
    "        \n",
    "        :param: df - Pandas Dataframe, pertaining to input access plan\n",
    "        \n",
    "        :return: Dictionary object, consisting of node objects (which are linked in a tree fashion)\n",
    "        \"\"\"\n",
    "        parent_node = None\n",
    "        node_dict = {}\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            # Build Node and add to parent\n",
    "            row_id = int(row['ID'])\n",
    "            parent_id = int(row['PARENT_ID'])\n",
    "            \n",
    "            if row_id == 0:\n",
    "                node = PlanTreeModeller.__create_node(node_name=row_id)\n",
    "            else:\n",
    "                parent_node = node_dict[parent_id]\n",
    "                node = PlanTreeModeller.__create_node(node_name=row_id, parent=parent_node)\n",
    "            node_dict[row_id] = node\n",
    "        \n",
    "        return node_dict # Dictionary consisting of tree nodes\n",
    "    \n",
    "    @staticmethod\n",
    "    def __retrieve_plan_details(df, node_name):\n",
    "        \"\"\"\n",
    "        Accepts a dataframe, and the node_name. Retrieves features pertaining to the row id in the access plan\n",
    "        \n",
    "        :param: df - Dataframe consisting of access plan features\n",
    "        :param: id - String id denoting which row to retrieve from the parameter dataframe\n",
    "        \n",
    "        :return: Dictionary consisting of access plan attributes\n",
    "        \"\"\"\n",
    "        operation = str(df[df['ID'] == node_name]['OPERATION'].iloc[0])\n",
    "        options = str(df[df['ID'] == node_name]['OPTIONS'].iloc[0])\n",
    "        object_name = str(df[df['ID'] == node_name]['OBJECT_NAME'].iloc[0])\n",
    "        cardinality = int(df[df['ID'] == node_name]['CARDINALITY'].iloc[0])\n",
    "        bytess = int(df[df['ID'] == node_name]['BYTES'].iloc[0])\n",
    "        partition_delta = int(df[df['ID'] == node_name]['PARTITION_STOP'].iloc[0]) - int(df[df['ID'] == node_name]['PARTITION_START'].iloc[0])\n",
    "        cpu_cost = int(df[df['ID'] == node_name]['CPU_COST'].iloc[0])\n",
    "        io_cost = int(df[df['ID'] == node_name]['IO_COST'].iloc[0])\n",
    "        temp_space = int(df[df['ID'] == node_name]['TEMP_SPACE'].iloc[0])\n",
    "        time = int(df[df['ID'] == node_name]['TIME'].iloc[0]) \n",
    "        \n",
    "        return {'OPERATION':operation,\n",
    "                'OPTIONS':options,\n",
    "                'OBJECT_NAME':object_name,\n",
    "                'CARDINALITY':cardinality,\n",
    "                'BYTES':bytess,\n",
    "                'PARTITION_DELTA':partition_delta,\n",
    "                'CPU_COST':cpu_cost,\n",
    "                'IO_COST':io_cost,\n",
    "                'TEMP_SPACE':temp_space,\n",
    "                'TIME':time}\n",
    "    \n",
    "    @staticmethod\n",
    "    def __tree_node_euclidean(tree_dict1, tree_dict2):\n",
    "        \"\"\"\n",
    "        This method calculates the eucldiean distance between two vectors.\n",
    "        \n",
    "        :param: tree_dict1 - Dictionary denoting a single node within plan / tree 1\n",
    "        :param: tree_dict2 - Dictionary denoting a single node within plan / tree 2\n",
    "        \n",
    "        :return: List denoting euclidean distance\n",
    "        \"\"\"\n",
    "        tree_vector_1 = [tree_dict1['CARDINALITY'],\n",
    "                         tree_dict1['BYTES'],\n",
    "                         tree_dict1['PARTITION_DELTA'],\n",
    "                         tree_dict1['CPU_COST'],\n",
    "                         tree_dict1['IO_COST'],\n",
    "                         tree_dict1['TEMP_SPACE'],\n",
    "                         tree_dict1['TIME']]\n",
    "        \n",
    "        tree_vector_2 = [tree_dict2['CARDINALITY'],\n",
    "                         tree_dict2['BYTES'],\n",
    "                         tree_dict2['PARTITION_DELTA'],\n",
    "                         tree_dict2['CPU_COST'],\n",
    "                         tree_dict2['IO_COST'],\n",
    "                         tree_dict2['TEMP_SPACE'],\n",
    "                         tree_dict2['TIME']]\n",
    "        \n",
    "        euc_distance = euclidean_distances([tree_vector_1],[tree_vector_2])\n",
    "        return euc_distance[0][0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def render_tree(tree, df):\n",
    "        \"\"\"\n",
    "        Renders Tree by printing to screen\n",
    "        \n",
    "        :param: tree - AnyTree object, representing tree modelled access plan\n",
    "        :param: df   - Pandas dataframe representatnt of the access plan about to be rendered\n",
    "        \n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for pre, fill, node in RenderTree(tree):\n",
    "            \n",
    "            access_plan_dict = PlanTreeModeller.__retrieve_plan_details(df=df,\n",
    "                                                                        node_name = node.name)\n",
    "            \n",
    "            if access_plan_dict['OBJECT_NAME'] == '0':\n",
    "                print(\"%s%s > %s\" % (pre, node.name, access_plan_dict['OPERATION']))\n",
    "            else:\n",
    "                if access_plan_dict['OPTIONS'] == '0': \n",
    "                    print(\"%s%s > %s (%s)\" % (pre, node.name, access_plan_dict['OPERATION'], access_plan_dict['OBJECT_NAME']))\n",
    "                else:\n",
    "                    print(\"%s%s > %s | %s (%s)\" % (pre, node.name, access_plan_dict['OPERATION'], access_plan_dict['OPTIONS'], access_plan_dict['OBJECT_NAME']))\n",
    "    \n",
    "    @staticmethod\n",
    "    def __postorder(tree):\n",
    "        \"\"\"\n",
    "        Accepts a tree, and iterates in post order fashion (left,right,root)\n",
    "        \n",
    "        :param: tree - Dictionary consisting of AnyTree Nodes\n",
    "        \n",
    "        :return: List consisting of tree traversal order\n",
    "        \"\"\"\n",
    "        post_order_traversal = [node.name for node in PostOrderIter(tree[0])]\n",
    "        return post_order_traversal\n",
    "    # \n",
    "    @staticmethod\n",
    "    def tree_compare(tree1, tree2, df1, df2):\n",
    "        \"\"\"\n",
    "        Accepts two trees of type 'AnyTree', along with respective dataframe denoting each respective access\n",
    "        path.\n",
    "        \n",
    "        :param: tree1 - Dictionary consisting of 'AnyTree' nodes, belonging to tree 1\n",
    "        :param: tree2 - Dictionary consisting of 'AnyTree' nodes, belonging to tree 2\n",
    "        :param: df1   - Pandas dataframe consisting of access plan instructions opted for by tree 1\n",
    "        :param: df2   - Pandas dataframe consisting of access plan instructions opted for by tree 2\n",
    "        \n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieves traversal order for both trees\n",
    "        post_order_traversal1 = PlanTreeModeller.__postorder(tree1)\n",
    "        post_order_traversal2 = PlanTreeModeller.__postorder(tree2)\n",
    "        \n",
    "        # Iterates over traversal order, until a change is encountered\n",
    "        max_range = max(len(post_order_traversal1),len(post_order_traversal2))\n",
    "        delta_flag = True\n",
    "        euclidean_measure = []\n",
    "        for i in range(0,max_range):\n",
    "            \n",
    "            # This check avoids a list IndexError for scebarious when one plan is bigger than the others,\n",
    "            # and consequently the number of node traversals is bigger than the other tree.\n",
    "            if i >= len(post_order_traversal1) or i >= len(post_order_traversal2):\n",
    "                break\n",
    "            \n",
    "            id_1 = post_order_traversal1[i]\n",
    "            id_2 = post_order_traversal2[i]\n",
    "            \n",
    "            pd_tree1 = PlanTreeModeller.__retrieve_plan_details(df=df1, node_name=id_1)\n",
    "            pd_tree2 = PlanTreeModeller.__retrieve_plan_details(df=df2, node_name=id_2)\n",
    "            \n",
    "            if (pd_tree1['OPERATION'] != pd_tree2['OPERATION'] or pd_tree1['OBJECT_NAME'] != pd_tree2['OBJECT_NAME'] or pd_tree1['OPTIONS'] != pd_tree2['OPTIONS']) and delta_flag:\n",
    "                print('Access Predicate Difference detected!')\n",
    "                print('Tree 1 difference at node [' + str(id_1) + '] operator > ' + pd_tree1['OPERATION'] + '(' + pd_tree1['OPTIONS'] + ') on object [' + pd_tree1['OBJECT_NAME'] + ']')\n",
    "                print('Tree 2 difference at node [' + str(id_2) + '] operator > ' + pd_tree2['OPERATION'] + '(' + pd_tree2['OPTIONS'] + ') on object [' + pd_tree2['OBJECT_NAME'] + ']')\n",
    "                delta_flag = False\n",
    "            \n",
    "            # Calculate Node Euclidean Measure\n",
    "            euclidean_vector = PlanTreeModeller.__tree_node_euclidean(tree_dict1=pd_tree1,\n",
    "                                                                      tree_dict2=pd_tree2)\n",
    "            euclidean_measure.append(euclidean_vector)\n",
    "        \n",
    "        if delta_flag:\n",
    "            print('No plan differences detected.')\n",
    "        \n",
    "        print('Total computed delta score [' + str(sum(euclidean_measure)) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Comparison with Hint Based Outliers\n",
    "\n",
    "Compares the expected stream with variation stream. Variations found here will be composed of SQL optimizer hint injections to purposely skew the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dxv968j0352kb' '0aq14dznn91rg' '4q1rzhn63sgpt' 'g0b4snpj74cv5'\n",
      " '03ggjrmy0wa1w' '93n8wp5a8xyxn' 'gdh0vpjkmbtjw' '1r7b985mxqj71'\n",
      " '1jhyrdp21f2q6' 'bwsf4tnh0gcgv' '29mjaymwt5p6d' 'aggcw7yk1a7s6'\n",
      " '8t26unxsrxj72' '71uursqtj1j2m' '59zh1b9759nf4' '0f60bzgt9127c'\n",
      " '9vmcsc3prvxpa' '76ds5wxsv7f5t' '20bqsr6btd9x9' '84ntdbh48ctu9'\n",
      " 'fx7sjdj48pn6z' '2wuhkcaz4uhs5' '2pz0tqbv91m11' '1u97hwfu7dcmz'\n",
      " '39nyc1pykjg41' '9ffht8tuysgx9' '7fbzhzg6ysu25' '14f5ngrj3cc5h'\n",
      " '4u268zn6r57tm' '6zcux9jb78w36' '2hnpu9m861609' '33vw5865cwyyn'\n",
      " 'gvcadr1hm4arv' '1pv23p59mjs0v' '7vtvbg7s3zcyp' '6fvfqaw68q59b'\n",
      " '06dymzb481vnd' '6u001adh62r0f' 'cqrk7q7f6nk3d' 'fc0va0vju750z'\n",
      " 'a6g97rawd3ggv' 'd6vqfmt62hypx' 'c1y1djkqjcd88' 'd7w1dugmzb9n9'\n",
      " '87gtj5jaq4a3t' '85cmvvurya34f' 'gh5w0gcyfaujs' '0ga8vk4nftz45'\n",
      " '13a9r2xkx1bxb' 'cjq93m442uprp' '7w116jy6ysqpm' 'au8ztarrm6vvs'\n",
      " '5g88vmdgd99f7' 'gkjkxbzzptg00' '4cgbvpjc134nu' 'a6fy23us0jz84'\n",
      " '64wct1dn3b771' 'fs2q92rb37cb0' 'cw6vxf0kbz3v1' 'dyk4dprp70d74'\n",
      " 'd6vwqbw6r2ffk' '7hys3h7ysgf9m' '664j0xx9u5wvu' '2tzs6v7mwnznt'\n",
      " '68uhsqwvcfqab' '130r442w3nfny' 'as8sqm1c84n07' '4vcvqy7vvy5zm'\n",
      " 'a2ttbs5gf0xpu' '6kckz8dk87m45' 'fnghrcwz4mfb3' '8ymq1gb13rfab'\n",
      " '1ms8wj24sqny4' 'fur78fafyqrkn' '2j5bk3tn2zt0g' '5r6n3gv8bjpf9'\n",
      " '5nn31913tabu9' 'bndp6dxkmkxyc' 'd0dnpw2mcxqxh' '4w3rn6bvafgfc'\n",
      " '1fn8v91f0arf0' 'b4j3z1g2nwfys' 'ckfvfhzy0qrws' '7m8xtjmn5zv0g'\n",
      " '4v69jmg5x7mvg' '0y080mnfaqk3u' '5pxcsstbnzj04' 'cbhzzp1d7dhkc'\n",
      " '4j2p57b4stn2a' '9yw1cvrf3wmcz' '2046z7kdh823h' '8hb1p1z9z4wfb'\n",
      " '6zs29hb3gpcf5' '0jj0ct4x4gy27' '95y5r5ksf94b1' 'cfk18fw3fynvs'\n",
      " 'dm7wjadqjyqna' '7uzyw6t048658' '01tp87bk1t2zv' '6nc5c1qczss8j'\n",
      " 'cwzcht2y54876' '7ts87fuvk3dhz']\n",
      "<class 'numpy.ndarray'>\n",
      "[ 103598129  389704395 1638991323 3471116783 3995561526    8791066\n",
      "  888381772  896925451 1487882830 4289615928  778470866 2032279852\n",
      " 2233562380 4117237197 4221041338 4070789595  663089694 1837516559\n",
      " 3017540644 3503409340  677218938  949440783 2618171612 1644021806\n",
      " 3944593978 3588723355 3930294564 2481783843 2658544436 3740804340\n",
      " 2509173122  260010185  711609758  773196278 1205956875 3472009245\n",
      " 3652725360 3947299421 1425259211  711190416 2686176978 2855263950\n",
      " 3559947703 3136665886 1608395336 2768986670 2982806768 3648363749\n",
      " 2147809191 2713034311  726239412 1012361507 2848590504 1174638588\n",
      " 2758431304 1611919487 1631025766 3344603960 2888767244  763821765\n",
      " 3395708625 1397742550 3903848004 3415526022 2647400149  172444278\n",
      " 2315578998 3685871082 3918373730 3317586809 3455506344 2718200651\n",
      " 1629483895    5300452 1388734953 2561120634 3584228551 2359725110\n",
      " 1728224257 2045682895 1945151352 3880375670   51727314 1046989779\n",
      "  983553774 2166267056  370496416 4047012398 2077827037 1613917314\n",
      " 3317924239 3642958835 3357682813 1357821411  467260136 3753392066\n",
      " 3513624734 2959902222 1711662062 2461107595 1448881271 2979453018\n",
      " 2882652417 3303824731 4253423378 3888506696 3260127605 1557348133\n",
      " 4192698878 1062555550 1979267904  785807998 1812246240 3793985226\n",
      " 3965336319  680292473 2589408485 1535817157   81502418 3910515334\n",
      " 3946645570 3262039238 1672510599 3202861106 2402092138 3349536270\n",
      " 1252736523 2162906169 1199086558  972922687 1810975827 1988813269]\n",
      "<class 'numpy.ndarray'>\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168]\n",
      "<class 'numpy.ndarray'>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Unique set of PLAN_HASH_VALUES\n",
    "np_sql_id, np_plan_hash_value, np_plan_instance = pd.unique(df['SQL_ID']),pd.unique(df['PLAN_HASH_VALUE']),pd.unique(df['PLAN_INSTANCE'])\n",
    "print(np_sql_id)\n",
    "print(type(np_sql_id))\n",
    "print(np_plan_hash_value)\n",
    "print(type(np_plan_hash_value))\n",
    "print(np_plan_instance)\n",
    "print(type(np_plan_instance))\n",
    "print('-'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
