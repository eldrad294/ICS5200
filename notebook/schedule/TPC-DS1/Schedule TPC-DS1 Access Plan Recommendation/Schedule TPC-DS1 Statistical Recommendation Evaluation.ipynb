{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule TPC-DS1 Statistical Recommendation Evaluation\n",
    "\n",
    "This experiment is intended at quantifying the statistical recommendation technique, through comparison of two query streams. The query streams are denoted as follows:\n",
    "\n",
    "* Expected Stream - Denotes a sequence of baseline query plans, against which comparison will be made.\n",
    "* Variation Stream - Denotes a sequence of upcoming query plans. Queries found within the upcoming stream mirror those established in the Expected Stream, with a number of exceptions. These exceptions are considered as query variants, and contain a degree of change from the original queries taken from the prior stream.\n",
    "\n",
    "Query variants are denoted below, and are therefore eligable to be flagged during the evaluation phase:\n",
    "\n",
    "* Query 5  \n",
    "* Query 10\n",
    "* Query 14\n",
    "* Query 18\n",
    "* Query 22\n",
    "* Query 27\n",
    "* Query 35\n",
    "* Query 36\n",
    "* Query 51\n",
    "* Query 67\n",
    "* Query 70\n",
    "* Query 77\n",
    "* Query 80\n",
    "* Query 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 0.23.4\n",
      "numpy: 1.15.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# sklearn\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "# AnyTree\n",
    "from anytree import Node, RenderTree, PostOrderIter\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "test_split=.2\n",
    "y_labels = ['COST',\n",
    "            'CARDINALITY',\n",
    "            'BYTES',\n",
    "            'CPU_COST',\n",
    "            'IO_COST',\n",
    "            'TEMP_SPACE',\n",
    "            'TIME']\n",
    "black_list = ['TIMESTAMP',\n",
    "              'SQL_ID',\n",
    "              'OPERATION',\n",
    "              'OPTIONS',\n",
    "              'OBJECT_NAME',\n",
    "              'OBJECT_OWNER',\n",
    "              'PARTITION_STOP',\n",
    "              'PARTITION_START'] # Columns which will be ignored during type conversion, and later used for aggregation\n",
    "nrows = 10000\n",
    "variant_ids = (5, 10, 14, 18, 22, 27, 35, 36, 51, 67, 70, 77, 80, 86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ('DBID',)    ('SQL_ID',)  ('PLAN_HASH_VALUE',)  ('ID',)    ('OPERATION',)  \\\n",
      "0  2634225673  dxv968j0352kb             103598129        0  SELECT STATEMENT   \n",
      "1  2634225673  dxv968j0352kb             103598129        1              SORT   \n",
      "2  2634225673  dxv968j0352kb             103598129        2    PX COORDINATOR   \n",
      "3  2634225673  dxv968j0352kb             103598129        3           PX SEND   \n",
      "4  2634225673  dxv968j0352kb             103598129        4              SORT   \n",
      "\n",
      "  ('OPTIONS',) ('OBJECT_NODE',)  ('OBJECT#',) ('OBJECT_OWNER',)  \\\n",
      "0          NaN              NaN           NaN               NaN   \n",
      "1     GROUP BY              NaN           NaN               NaN   \n",
      "2          NaN              NaN           NaN               NaN   \n",
      "3  QC (RANDOM)           :Q1001           NaN               SYS   \n",
      "4     GROUP BY           :Q1001           NaN               NaN   \n",
      "\n",
      "  ('OBJECT_NAME',)     ...     ('ACCESS_PREDICATES',) ('FILTER_PREDICATES',)  \\\n",
      "0              NaN     ...                        NaN                    NaN   \n",
      "1              NaN     ...                        NaN                    NaN   \n",
      "2              NaN     ...                        NaN                    NaN   \n",
      "3         :TQ10001     ...                        NaN                    NaN   \n",
      "4              NaN     ...                        NaN                    NaN   \n",
      "\n",
      "  ('PROJECTION',)  ('TIME',)  ('QBLOCK_NAME',)  ('REMARKS',)  \\\n",
      "0             NaN        NaN               NaN           NaN   \n",
      "1             NaN        NaN             SEL$1           NaN   \n",
      "2             NaN        NaN               NaN           NaN   \n",
      "3             NaN        NaN               NaN           NaN   \n",
      "4             NaN        NaN               NaN           NaN   \n",
      "\n",
      "        ('TIMESTAMP',)                                     ('OTHER_XML',)  \\\n",
      "0  2018-10-07 15:52:33                                                NaN   \n",
      "1  2018-10-07 15:52:33  <other_xml><info type=\"db_version\">12.1.0.2</i...   \n",
      "2  2018-10-07 15:52:33                                                NaN   \n",
      "3  2018-10-07 15:52:33                                                NaN   \n",
      "4  2018-10-07 15:52:33                                                NaN   \n",
      "\n",
      "   ('CON_DBID',) ('CON_ID',)  \n",
      "0     2634225673           0  \n",
      "1     2634225673           0  \n",
      "2     2634225673           0  \n",
      "3     2634225673           0  \n",
      "4     2634225673           0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "------------------------------------------\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Root path\n",
    "#base_dir = 'C:/Users/gabriel.sammut/University/'\n",
    "base_dir = 'D:/Projects/'\n",
    "root_dir = base_dir + 'Datagenerated_ICS5200/Schedule/' + tpcds\n",
    "src_dir = base_dir + 'ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/'\n",
    "\n",
    "rep_vsql_plan_path = root_dir + '/rep_vsql_plan.csv'\n",
    "#rep_vsql_plan_path = root_dir + '/rep_vsql_plan.csv'\n",
    "\n",
    "dtype={'COST':'int64',\n",
    "       'CARDINALITY':'int64',\n",
    "       'BYTES':'int64',\n",
    "       'CPU_COST':'int64',\n",
    "       'IO_COST':'int64',\n",
    "       'TEMP_SPACE':'int64',\n",
    "       'TIME':'int64',\n",
    "       'OPERATION':'str',\n",
    "       'OBJECT_NAME':'str'}\n",
    "rep_vsql_plan_df = pd.read_csv(rep_vsql_plan_path, nrows=nrows, dtype=dtype)\n",
    "print(rep_vsql_plan_df.head())\n",
    "\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "\n",
    "rep_vsql_plan_df.columns = prettify_header(rep_vsql_plan_df.columns.values)\n",
    "print('------------------------------------------')\n",
    "print(rep_vsql_plan_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read outlier data from file into pandas dataframes and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465, 35)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12354  11/20/2018 08:23:55     NaN  SELECT STATEMENT              NaN   \n",
      "1   12354  11/20/2018 08:23:55     NaN             COUNT          STOPKEY   \n",
      "2   12354  11/20/2018 08:23:55     NaN              VIEW              NaN   \n",
      "3   12354  11/20/2018 08:23:55     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12354  11/20/2018 08:23:55     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN       TPCDS1         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN       TPCDS1         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE     ...      \\\n",
      "0             NaN     ...       \n",
      "1             NaN     ...       \n",
      "2              18     ...       \n",
      "3             NaN     ...       \n",
      "4              19     ...       \n",
      "\n",
      "                                           OTHER_XML DISTRIBUTION    CPU_COST  \\\n",
      "0                                                NaN          NaN  1657360333   \n",
      "1  <other_xml><info type=\"db_version\">12.1.0.2</i...          NaN         NaN   \n",
      "2                                                NaN          NaN  1657360333   \n",
      "3                                                NaN          NaN  1657360333   \n",
      "4                                                NaN          NaN  1625075317   \n",
      "\n",
      "  IO_COST TEMP_SPACE ACCESS_PREDICATES FILTER_PREDICATES  \\\n",
      "0   13630        NaN               NaN               NaN   \n",
      "1     NaN        NaN               NaN       ROWNUM<=100   \n",
      "2   13630        NaN               NaN               NaN   \n",
      "3   13630        NaN               NaN               NaN   \n",
      "4   13630        NaN               NaN               NaN   \n",
      "\n",
      "                                          PROJECTION TIME QBLOCK_NAME  \n",
      "0                                                NaN    1         NaN  \n",
      "1  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN      SEL$11  \n",
      "2  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    1      SEL$12  \n",
      "3  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    1      SEL$12  \n",
      "4  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    1       SET$4  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "------------------------------------------\n",
      "(491, 35)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12372  11/20/2018 08:40:58     NaN  SELECT STATEMENT              NaN   \n",
      "1   12372  11/20/2018 08:40:58     NaN             COUNT          STOPKEY   \n",
      "2   12372  11/20/2018 08:40:58     NaN              VIEW              NaN   \n",
      "3   12372  11/20/2018 08:40:58     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12372  11/20/2018 08:40:58     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN       TPCDS1         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN       TPCDS1         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE     ...      \\\n",
      "0             NaN     ...       \n",
      "1             NaN     ...       \n",
      "2              18     ...       \n",
      "3             NaN     ...       \n",
      "4              19     ...       \n",
      "\n",
      "                                           OTHER_XML DISTRIBUTION   CPU_COST  \\\n",
      "0                                                NaN          NaN  242094911   \n",
      "1  <other_xml><info type=\"db_version\">12.1.0.2</i...          NaN        NaN   \n",
      "2                                                NaN          NaN  242094911   \n",
      "3                                                NaN          NaN  242094911   \n",
      "4                                                NaN          NaN  209150318   \n",
      "\n",
      "  IO_COST TEMP_SPACE ACCESS_PREDICATES FILTER_PREDICATES  \\\n",
      "0    2549        NaN               NaN               NaN   \n",
      "1     NaN        NaN               NaN       ROWNUM<=100   \n",
      "2    2549        NaN               NaN               NaN   \n",
      "3    2549        NaN               NaN               NaN   \n",
      "4    2549        NaN               NaN               NaN   \n",
      "\n",
      "                                          PROJECTION TIME QBLOCK_NAME  \n",
      "0                                                NaN    1         NaN  \n",
      "1  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN      SEL$11  \n",
      "2  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    1      SEL$12  \n",
      "3  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    1      SEL$12  \n",
      "4  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    1       SET$4  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "------------------------------------------\n",
      "(500, 35)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12386  11/20/2018 08:52:27     NaN  SELECT STATEMENT              NaN   \n",
      "1   12386  11/20/2018 08:52:27     NaN             COUNT          STOPKEY   \n",
      "2   12386  11/20/2018 08:52:27     NaN              VIEW              NaN   \n",
      "3   12386  11/20/2018 08:52:27     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12386  11/20/2018 08:52:27     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN       TPCDS1         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN       TPCDS1         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE     ...      \\\n",
      "0             NaN     ...       \n",
      "1             NaN     ...       \n",
      "2              18     ...       \n",
      "3             NaN     ...       \n",
      "4              19     ...       \n",
      "\n",
      "                                           OTHER_XML DISTRIBUTION   CPU_COST  \\\n",
      "0                                                NaN          NaN  226837615   \n",
      "1  <other_xml><info type=\"db_version\">12.1.0.2</i...          NaN        NaN   \n",
      "2                                                NaN          NaN  226837615   \n",
      "3                                                NaN          NaN  226837615   \n",
      "4                                                NaN          NaN  194552599   \n",
      "\n",
      "  IO_COST TEMP_SPACE ACCESS_PREDICATES FILTER_PREDICATES  \\\n",
      "0    1563        NaN               NaN               NaN   \n",
      "1     NaN        NaN               NaN     ROWNUM<=10000   \n",
      "2    1563        NaN               NaN               NaN   \n",
      "3    1563        NaN               NaN               NaN   \n",
      "4    1563        NaN               NaN               NaN   \n",
      "\n",
      "                                          PROJECTION TIME QBLOCK_NAME  \n",
      "0                                                NaN    1         NaN  \n",
      "1  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN      SEL$11  \n",
      "2  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...    1      SEL$12  \n",
      "3  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...    1      SEL$12  \n",
      "4  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...    1       SET$4  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# CSV Outlier Paths\n",
    "outlier_hints_q5_path = src_dir + 'hints/output/query_5.csv'\n",
    "outlier_hints_q10_path = src_dir + 'hints/output/query_10.csv'\n",
    "outlier_hints_q14_path = src_dir + 'hints/output/query_14.csv'\n",
    "outlier_hints_q18_path = src_dir + 'hints/output/query_18.csv'\n",
    "outlier_hints_q22_path = src_dir + 'hints/output/query_22.csv'\n",
    "outlier_hints_q27_path = src_dir + 'hints/output/query_27.csv'\n",
    "outlier_hints_q35_path = src_dir + 'hints/output/query_35.csv'\n",
    "outlier_hints_q36_path = src_dir + 'hints/output/query_36.csv'\n",
    "outlier_hints_q51_path = src_dir + 'hints/output/query_51.csv'\n",
    "outlier_hints_q67_path = src_dir + 'hints/output/query_67.csv'\n",
    "outlier_hints_q70_path = src_dir + 'hints/output/query_70.csv'\n",
    "outlier_hints_q77_path = src_dir + 'hints/output/query_77.csv'\n",
    "outlier_hints_q80_path = src_dir + 'hints/output/query_80.csv'\n",
    "outlier_hints_q86_path = src_dir + 'hints/output/query_86.csv'\n",
    "\n",
    "outlier_predicates_q5_path = src_dir + 'predicates/output/query_5.csv'\n",
    "outlier_predicates_q10_path = src_dir + 'predicates/output/query_10.csv'\n",
    "outlier_predicates_q14_path = src_dir + 'predicates/output/query_14.csv'\n",
    "outlier_predicates_q18_path = src_dir + 'predicates/output/query_18.csv'\n",
    "outlier_predicates_q22_path = src_dir + 'predicates/output/query_22.csv'\n",
    "outlier_predicates_q27_path = src_dir + 'predicates/output/query_27.csv'\n",
    "outlier_predicates_q35_path = src_dir + 'predicates/output/query_35.csv'\n",
    "outlier_predicates_q36_path = src_dir + 'predicates/output/query_36.csv'\n",
    "outlier_predicates_q51_path = src_dir + 'predicates/output/query_51.csv'\n",
    "outlier_predicates_q67_path = src_dir + 'predicates/output/query_67.csv'\n",
    "outlier_predicates_q70_path = src_dir + 'predicates/output/query_70.csv'\n",
    "outlier_predicates_q77_path = src_dir + 'predicates/output/query_77.csv'\n",
    "outlier_predicates_q80_path = src_dir + 'predicates/output/query_80.csv'\n",
    "outlier_predicates_q86_path = src_dir + 'predicates/output/query_86.csv'\n",
    "\n",
    "outlier_rownum_q5_path = src_dir + 'rownum/output/query_5.csv'\n",
    "outlier_rownum_q10_path = src_dir + 'rownum/output/query_10.csv'\n",
    "outlier_rownum_q14_path = src_dir + 'rownum/output/query_14.csv'\n",
    "outlier_rownum_q18_path = src_dir + 'rownum/output/query_18.csv'\n",
    "outlier_rownum_q22_path = src_dir + 'rownum/output/query_22.csv'\n",
    "outlier_rownum_q27_path = src_dir + 'rownum/output/query_27.csv'\n",
    "outlier_rownum_q35_path = src_dir + 'rownum/output/query_35.csv'\n",
    "outlier_rownum_q36_path = src_dir + 'rownum/output/query_36.csv'\n",
    "outlier_rownum_q51_path = src_dir + 'rownum/output/query_51.csv'\n",
    "outlier_rownum_q67_path = src_dir + 'rownum/output/query_67.csv'\n",
    "outlier_rownum_q70_path = src_dir + 'rownum/output/query_70.csv'\n",
    "outlier_rownum_q77_path = src_dir + 'rownum/output/query_77.csv'\n",
    "outlier_rownum_q80_path = src_dir + 'rownum/output/query_80.csv'\n",
    "outlier_rownum_q86_path = src_dir + 'rownum/output/query_86.csv'\n",
    "\n",
    "# Read CSV Paths\n",
    "outlier_hints_q5_df = pd.read_csv(outlier_hints_q5_path,dtype=str)\n",
    "outlier_hints_q10_df = pd.read_csv(outlier_hints_q10_path,dtype=str)\n",
    "outlier_hints_q14_df = pd.read_csv(outlier_hints_q14_path,dtype=str)\n",
    "outlier_hints_q18_df = pd.read_csv(outlier_hints_q18_path,dtype=str)\n",
    "outlier_hints_q22_df = pd.read_csv(outlier_hints_q22_path,dtype=str)\n",
    "outlier_hints_q27_df = pd.read_csv(outlier_hints_q27_path,dtype=str)\n",
    "outlier_hints_q35_df = pd.read_csv(outlier_hints_q35_path,dtype=str)\n",
    "outlier_hints_q36_df = pd.read_csv(outlier_hints_q36_path,dtype=str)\n",
    "outlier_hints_q51_df = pd.read_csv(outlier_hints_q51_path,dtype=str)\n",
    "outlier_hints_q67_df = pd.read_csv(outlier_hints_q67_path,dtype=str)\n",
    "outlier_hints_q70_df = pd.read_csv(outlier_hints_q70_path,dtype=str)\n",
    "outlier_hints_q77_df = pd.read_csv(outlier_hints_q77_path,dtype=str)\n",
    "outlier_hints_q80_df = pd.read_csv(outlier_hints_q80_path,dtype=str)\n",
    "outlier_hints_q86_df = pd.read_csv(outlier_hints_q86_path,dtype=str)\n",
    "\n",
    "outlier_predicates_q5_df = pd.read_csv(outlier_predicates_q5_path,dtype=str)\n",
    "outlier_predicates_q10_df = pd.read_csv(outlier_predicates_q10_path,dtype=str)\n",
    "outlier_predicates_q14_df = pd.read_csv(outlier_predicates_q14_path,dtype=str)\n",
    "outlier_predicates_q18_df = pd.read_csv(outlier_predicates_q18_path,dtype=str)\n",
    "outlier_predicates_q22_df = pd.read_csv(outlier_predicates_q22_path,dtype=str)\n",
    "outlier_predicates_q27_df = pd.read_csv(outlier_predicates_q27_path,dtype=str)\n",
    "outlier_predicates_q35_df = pd.read_csv(outlier_predicates_q35_path,dtype=str)\n",
    "outlier_predicates_q36_df = pd.read_csv(outlier_predicates_q36_path,dtype=str)\n",
    "outlier_predicates_q51_df = pd.read_csv(outlier_predicates_q51_path,dtype=str)\n",
    "outlier_predicates_q67_df = pd.read_csv(outlier_predicates_q67_path,dtype=str)\n",
    "outlier_predicates_q70_df = pd.read_csv(outlier_predicates_q70_path,dtype=str)\n",
    "outlier_predicates_q77_df = pd.read_csv(outlier_predicates_q77_path,dtype=str)\n",
    "outlier_predicates_q80_df = pd.read_csv(outlier_predicates_q80_path,dtype=str)\n",
    "outlier_predicates_q86_df = pd.read_csv(outlier_predicates_q86_path,dtype=str)\n",
    "\n",
    "outlier_rownum_q5_df = pd.read_csv(outlier_rownum_q5_path,dtype=str)\n",
    "outlier_rownum_q10_df = pd.read_csv(outlier_rownum_q10_path,dtype=str)\n",
    "outlier_rownum_q14_df = pd.read_csv(outlier_rownum_q14_path,dtype=str)\n",
    "outlier_rownum_q18_df = pd.read_csv(outlier_rownum_q18_path,dtype=str)\n",
    "outlier_rownum_q22_df = pd.read_csv(outlier_rownum_q22_path,dtype=str)\n",
    "outlier_rownum_q27_df = pd.read_csv(outlier_rownum_q27_path,dtype=str)\n",
    "outlier_rownum_q35_df = pd.read_csv(outlier_rownum_q35_path,dtype=str)\n",
    "outlier_rownum_q36_df = pd.read_csv(outlier_rownum_q36_path,dtype=str)\n",
    "outlier_rownum_q51_df = pd.read_csv(outlier_rownum_q51_path,dtype=str)\n",
    "outlier_rownum_q67_df = pd.read_csv(outlier_rownum_q67_path,dtype=str)\n",
    "outlier_rownum_q70_df = pd.read_csv(outlier_rownum_q70_path,dtype=str)\n",
    "outlier_rownum_q77_df = pd.read_csv(outlier_rownum_q77_path,dtype=str)\n",
    "outlier_rownum_q80_df = pd.read_csv(outlier_rownum_q80_path,dtype=str)\n",
    "outlier_rownum_q86_df = pd.read_csv(outlier_rownum_q86_path,dtype=str)\n",
    "\n",
    "# Merge dataframes into a single pandas matrix\n",
    "df_hints_outliers = pd.concat([outlier_hints_q5_df, outlier_hints_q10_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q14_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q18_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q22_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q27_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q35_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q36_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q51_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q67_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q70_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q77_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q80_df], sort=False)\n",
    "df_hints_outliers = pd.concat([df_hints_outliers, outlier_hints_q86_df], sort=False)\n",
    "\n",
    "df_predicate_outliers = pd.concat([outlier_predicates_q5_df, outlier_predicates_q10_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q14_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q18_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q22_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q27_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q35_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q36_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q51_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q67_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q70_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q77_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q80_df], sort=False)\n",
    "df_predicate_outliers = pd.concat([df_predicate_outliers, outlier_predicates_q86_df], sort=False)\n",
    "\n",
    "df_rownum_outliers = pd.concat([outlier_rownum_q5_df, outlier_rownum_q10_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q14_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q18_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q22_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q27_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q35_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q36_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q51_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q67_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q70_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q77_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q80_df], sort=False)\n",
    "df_rownum_outliers = pd.concat([df_rownum_outliers, outlier_rownum_q86_df], sort=False)\n",
    "\n",
    "print(df_hints_outliers.shape)\n",
    "print(df_hints_outliers.head())\n",
    "print('------------------------------------------')\n",
    "print(df_predicate_outliers.shape)\n",
    "print(df_predicate_outliers.head())\n",
    "print('------------------------------------------')\n",
    "print(df_rownum_outliers.shape)\n",
    "print(df_rownum_outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A Columns\n",
      "\n",
      "\n",
      "REP_VSQL_PLAN Features 39: ['OPTIONS', 'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'COST', 'CARDINALITY', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'IO_COST', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME', 'REMARKS', 'OTHER_XML']\n",
      "\n",
      "\n",
      "DF_HINT_OUTLIERS Features 35: ['REMARKS', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME']\n",
      "\n",
      "\n",
      "DF_PREDICATE_OUTLIERS Features 35: ['REMARKS', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME']\n",
      "\n",
      "\n",
      "DF_ROWNUM_OUTLIERS Features 35: ['REMARKS', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "\n",
    "print('N/A Columns\\n')\n",
    "print('\\nREP_VSQL_PLAN Features ' + str(len(rep_vsql_plan_df.columns)) + ': ' + str(get_na_columns(df=rep_vsql_plan_df,headers=rep_vsql_plan_df.columns)) + \"\\n\")\n",
    "print('\\nDF_HINT_OUTLIERS Features ' + str(len(df_hints_outliers.columns)) + ': ' + str(get_na_columns(df=df_hints_outliers,headers=df_hints_outliers.columns)) + \"\\n\")\n",
    "print('\\nDF_PREDICATE_OUTLIERS Features ' + str(len(df_predicate_outliers.columns)) + ': ' + str(get_na_columns(df=df_predicate_outliers,headers=df_predicate_outliers.columns)) + \"\\n\")\n",
    "print('\\nDF_ROWNUM_OUTLIERS Features ' + str(len(df_rownum_outliers.columns)) + ': ' + str(get_na_columns(df=df_rownum_outliers,headers=df_rownum_outliers.columns)) + \"\\n\")\n",
    "#\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Populating NaN values with amount '0'\n",
    "df = fill_na(df=rep_vsql_plan_df)\n",
    "df_hints_outliers = fill_na(df=df_hints_outliers)\n",
    "df_predicate_outliers = fill_na(df=df_predicate_outliers)\n",
    "df_rownum_outliers = fill_na(df=df_rownum_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversion\n",
    "\n",
    "Each column is converted into a column of type values which are Integer64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Dropped column [OBJECT_ALIAS]\n",
      "Dropped column [OBJECT_TYPE]\n",
      "Dropped column [OPTIMIZER]\n",
      "Dropped column [OTHER_XML]\n",
      "Dropped column [ACCESS_PREDICATES]\n",
      "Dropped column [FILTER_PREDICATES]\n",
      "Dropped column [PROJECTION]\n",
      "Dropped column [QBLOCK_NAME]\n",
      "-------------------------------------------------------------\n",
      "Dropped column [OBJECT_ALIAS]\n",
      "Dropped column [OBJECT_TYPE]\n",
      "Dropped column [OPTIMIZER]\n",
      "Dropped column [OTHER_XML]\n",
      "Dropped column [ACCESS_PREDICATES]\n",
      "Dropped column [FILTER_PREDICATES]\n",
      "Dropped column [PROJECTION]\n",
      "Dropped column [QBLOCK_NAME]\n",
      "-------------------------------------------------------------\n",
      "Dropped column [OBJECT_ALIAS]\n",
      "Dropped column [OBJECT_TYPE]\n",
      "Dropped column [OPTIMIZER]\n",
      "Dropped column [OTHER_XML]\n",
      "Dropped column [ACCESS_PREDICATES]\n",
      "Dropped column [FILTER_PREDICATES]\n",
      "Dropped column [PROJECTION]\n",
      "Dropped column [QBLOCK_NAME]\n",
      "-------------------------------------------------------------\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def handle_numeric_overflows(x):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe column, and \n",
    "    \"\"\"\n",
    "    try:\n",
    "        #df = df.astype('int64')\n",
    "        x1 = pd.DataFrame([x],dtype='int64')\n",
    "    except ValueError:\n",
    "        x = 9223372036854775807 # Max int size\n",
    "    return x\n",
    "\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df[col] = df[col].apply(handle_numeric_overflows)\n",
    "        df[col].astype('int64',inplace=True)\n",
    "    except:\n",
    "        df.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')\n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "for col in df_hints_outliers.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df_hints_outliers[col] = df_hints_outliers[col].astype('int64')\n",
    "    except OverflowError:\n",
    "        #\n",
    "        # Handles numeric overflow conversions by replacing such values with max value inside the dataset.\n",
    "        df_hints_outliers[col] = df_hints_outliers[col].apply(handle_numeric_overflows)\n",
    "        df_hints_outliers[col] = df_hints_outliers[col].astype('int64')\n",
    "    except Exception as e:\n",
    "        df_hints_outliers.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')\n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "for col in df_predicate_outliers.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df_predicate_outliers[col] = df_predicate_outliers[col].astype('int64')\n",
    "    except OverflowError:\n",
    "        \n",
    "        # Handles numeric overflow conversions by replacing such values with max value inside the dataset.\n",
    "        df_predicate_outliers[col] = df_predicate_outliers[col].apply(handle_numeric_overflows)\n",
    "        df_predicate_outliers[col] = df_predicate_outliers[col].astype('int64')\n",
    "    except Exception as e:\n",
    "        df_predicate_outliers.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')       \n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "for col in df_rownum_outliers.columns:\n",
    "    try:\n",
    "        if col in black_list:\n",
    "            continue\n",
    "        df_rownum_outliers[col] = df_rownum_outliers[col].astype('int64')\n",
    "    except OverflowError:\n",
    "        #\n",
    "        # Handles numeric overflow conversions by replacing such values with max value inside the dataset.\n",
    "        df_rownum_outliers[col] = df_rownum_outliers[col].apply(handle_numeric_overflows)\n",
    "        df_rownum_outliers[col] = df_rownum_outliers[col].astype('int64')\n",
    "    except Exception as e:\n",
    "        df_rownum_outliers.drop(columns=col, inplace=True)\n",
    "        print('Dropped column [' + col + ']')    \n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "      \n",
    "print(df.columns)\n",
    "print(df_hints_outliers.columns)\n",
    "print(df_predicate_outliers.columns)\n",
    "print(df_rownum_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Elimination\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_flatline_columns(df):\n",
    "#     columns = df.columns\n",
    "#     flatline_features = []\n",
    "#     for i in range(len(columns)):\n",
    "#         try:\n",
    "#             #\n",
    "#             if columns[i] in black_list:\n",
    "#                 continue\n",
    "#             #\n",
    "#             std = df[columns[i]].std()\n",
    "#             if std == 0:\n",
    "#                 flatline_features.append(columns[i])\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#     #print('Features which are considered flatline:\\n')\n",
    "#     #for col in flatline_features:\n",
    "#     #    print(col)\n",
    "#     print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "#     df = df.drop(columns=flatline_features)\n",
    "#     print('Shape after changes: [' + str(df.shape) + ']')\n",
    "#     print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "#     return df\n",
    "\n",
    "# df = drop_flatline_columns(df=df)\n",
    "# df_hints_outliers = drop_flatline_columns(df=df_hints_outliers)\n",
    "# df_predicate_outliers = drop_flatline_columns(df=df_predicate_outliers)\n",
    "# df_rownum_outliers = drop_flatline_columns(df=df_rownum_outliers)\n",
    "\n",
    "# print('\\nAfter flatline column drop:')\n",
    "# print(df.shape)\n",
    "# print(df.columns)\n",
    "\n",
    "# print('--------------------------------------------------------')\n",
    "# print('\\nAfter outlier flatline column drop [df_hints_outliers]:')\n",
    "# print(df_hints_outliers.shape)\n",
    "# print(df_hints_outliers.columns)\n",
    "\n",
    "# print('--------------------------------------------------------')\n",
    "# print('\\nAfter outlier flatline column drop [df_predicate_outliers]:')\n",
    "# print(df_predicate_outliers.shape)\n",
    "# print(df_predicate_outliers.columns)\n",
    "\n",
    "# print('--------------------------------------------------------')\n",
    "# print('\\nAfter outlier flatline column drop [df_rownum_outliers]:')\n",
    "# print(df_rownum_outliers.shape)\n",
    "# print(df_rownum_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling columns\n",
    "\n",
    "This section attempts to process a number of data columns through a MinMax Scaler. This is done, to normalize data on a similar scaler, particularly before comparing column measurements using a euclidean based measure. The following columns will be targetted:\n",
    "\n",
    "* CARDINALITY\n",
    "* BYTES\n",
    "* PARTITION_START\n",
    "* PARTITION_STOP\n",
    "* CPU_COST\n",
    "* IO_COST\n",
    "* TEMP_SPACE\n",
    "* TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "Minimal Vector Points: [0.000e+00 2.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.156e+06\n",
      " 0.000e+00]\n",
      "Maximal Vector Points: [2.85716704e+16 9.22337204e+18 0.00000000e+00 0.00000000e+00\n",
      " 9.22337204e+18 7.78425143e+09 9.22337204e+18 1.58287675e+08]\n",
      "\n",
      "After scaled column transformation:\n",
      "(10000, 39)\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier scaled column transformation [df_hints_outliers]:\n",
      "(465, 27)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier scaled column transformation [df_predicate_outliers]:\n",
      "(491, 27)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier scaled column transformation [df_rownum_outliers]:\n",
      "(500, 27)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_INSTANCE',\n",
      "       'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST',\n",
      "       'IO_COST', 'TEMP_SPACE', 'TIME'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_columns = ['CARDINALITY',\n",
    "                'BYTES',\n",
    "                'PARTITION_START',\n",
    "                'PARTITION_STOP',\n",
    "                'CPU_COST',\n",
    "                'IO_COST',\n",
    "                'TEMP_SPACE',\n",
    "                'TIME']\n",
    "print(df['PARTITION_START'].iloc[0])\n",
    "df[scaled_columns] = scaler.fit_transform(df[scaled_columns])\n",
    "print(df['PARTITION_START'].iloc[0])\n",
    "print(\"Minimal Vector Points: \" + str(scaler.data_min_))\n",
    "print(\"Maximal Vector Points: \" + str(scaler.data_max_))\n",
    "\n",
    "print('\\nAfter scaled column transformation:')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier scaled column transformation [df_hints_outliers]:')\n",
    "print(df_hints_outliers.shape)\n",
    "print(df_hints_outliers.columns)\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier scaled column transformation [df_predicate_outliers]:')\n",
    "print(df_predicate_outliers.shape)\n",
    "print(df_predicate_outliers.columns)\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier scaled column transformation [df_rownum_outliers]:')\n",
    "print(df_rownum_outliers.shape)\n",
    "print(df_rownum_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Grouping Column\n",
    "\n",
    "An extra column is added to allow access plans to be isolated per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before transformation: (10000, 39)\n",
      "Shape after transformation: (10000, 40)\n",
      "Shape before transformation: (465, 27)\n",
      "Shape after transformation: (465, 28)\n",
      "Shape before transformation: (491, 27)\n",
      "Shape after transformation: (491, 28)\n",
      "Shape before transformation: (500, 27)\n",
      "Shape after transformation: (500, 28)\n"
     ]
    }
   ],
   "source": [
    "# Adds a columns per SQL_ID, PLAN_HASH_VALUE grouping, which can be used to group instances together\n",
    "def add_grouping_column(df, column_identifier):\n",
    "    \"\"\"\n",
    "    Receives a pandas dataframe, and adds a new column which allows dataframe to be aggregated per \n",
    "    SQL_ID, PLAN_HASH_VALUE combination.\n",
    "    \n",
    "    :param: df                - Pandas Dataframe\n",
    "    :param: column_identifier - String denoting matrix column to group by\n",
    "    \n",
    "    :return: Pandas Dataframe, with added column    \n",
    "    \"\"\"\n",
    "    print('Shape before transformation: ' + str(df.shape))\n",
    "    new_grouping_col = []\n",
    "    counter = 0\n",
    "    last_sql_id = df[column_identifier].iloc(0) # Starts with first SQL_ID\n",
    "    for index, row in df.iterrows():\n",
    "        if column_identifier == 'SQL_ID':\n",
    "            if last_sql_id != row.SQL_ID:\n",
    "                last_sql_id = row.SQL_ID\n",
    "                counter += 1\n",
    "        elif column_identifier == 'PLAN_ID':\n",
    "            if last_sql_id != row.PLAN_ID:\n",
    "                last_sql_id = row.PLAN_ID\n",
    "                counter += 1\n",
    "        else:\n",
    "            raise ValueError('Column does not exist!')\n",
    "        new_grouping_col.append(counter)\n",
    "    \n",
    "    # Append list as new column\n",
    "    new_col = pd.Series(new_grouping_col)\n",
    "    df['PLAN_INSTANCE'] = new_col.values\n",
    "    print('Shape after transformation: ' + str(df.shape))\n",
    "    return df\n",
    "\n",
    "df = add_grouping_column(df=df,column_identifier='SQL_ID')\n",
    "df_hints_outliers = add_grouping_column(df=df_hints_outliers,column_identifier='PLAN_ID')\n",
    "df_predicate_outliers = add_grouping_column(df=df_predicate_outliers,column_identifier='PLAN_ID')\n",
    "df_rownum_outliers = add_grouping_column(df=df_rownum_outliers,column_identifier='PLAN_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan Matching Column\n",
    "\n",
    "This column is used to match plans between inliers and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DBID         SQL_ID  PLAN_HASH_VALUE  ID         OPERATION  \\\n",
      "0  2634225673  dxv968j0352kb        103598129   0  SELECT STATEMENT   \n",
      "1  2634225673  dxv968j0352kb        103598129   1              SORT   \n",
      "2  2634225673  dxv968j0352kb        103598129   2    PX COORDINATOR   \n",
      "3  2634225673  dxv968j0352kb        103598129   3           PX SEND   \n",
      "4  2634225673  dxv968j0352kb        103598129   4              SORT   \n",
      "\n",
      "       OPTIONS          OBJECT_NODE  OBJECT# OBJECT_OWNER OBJECT_NAME  \\\n",
      "0            0                    0      0.0            0           0   \n",
      "1     GROUP BY                    0      0.0            0           0   \n",
      "2            0                    0      0.0            0           0   \n",
      "3  QC (RANDOM)  9223372036854775807      0.0          SYS    :TQ10001   \n",
      "4     GROUP BY  9223372036854775807      0.0            0           0   \n",
      "\n",
      "                         ...                          PROJECTION  TIME  \\\n",
      "0                        ...                                 0.0   0.0   \n",
      "1                        ...                                 0.0   0.0   \n",
      "2                        ...                                 0.0   0.0   \n",
      "3                        ...                                 0.0   0.0   \n",
      "4                        ...                                 0.0   0.0   \n",
      "\n",
      "           QBLOCK_NAME  REMARKS            TIMESTAMP            OTHER_XML  \\\n",
      "0                    0      0.0  2018-10-07 15:52:33                    0   \n",
      "1  9223372036854775807      0.0  2018-10-07 15:52:33  9223372036854775807   \n",
      "2                    0      0.0  2018-10-07 15:52:33                    0   \n",
      "3                    0      0.0  2018-10-07 15:52:33                    0   \n",
      "4                    0      0.0  2018-10-07 15:52:33                    0   \n",
      "\n",
      "     CON_DBID  CON_ID  PLAN_INSTANCE  \\\n",
      "0  2634225673       0              1   \n",
      "1  2634225673       0              1   \n",
      "2  2634225673       0              1   \n",
      "3  2634225673       0              1   \n",
      "4  2634225673       0              1   \n",
      "\n",
      "                                           SQL_MATCH  \n",
      "0  SELECT STATEMENT > SORT > PX COORDINATOR > PX ...  \n",
      "1  SELECT STATEMENT > SORT > PX COORDINATOR > PX ...  \n",
      "2  SELECT STATEMENT > SORT > PX COORDINATOR > PX ...  \n",
      "3  SELECT STATEMENT > SORT > PX COORDINATOR > PX ...  \n",
      "4  SELECT STATEMENT > SORT > PX COORDINATOR > PX ...  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "def add_matching_column(df):\n",
    "    \n",
    "    # Create new empty column\n",
    "#     new_col = pd.Series([])\n",
    "#     df['SQL_MATCH'] = new_col.values\n",
    "    df = df.assign(SQL_MATCH = lambda x: 0)\n",
    "    \n",
    "    # Retrieve unique listing of SQL_IDs and iterate over them\n",
    "    unique_ids = pd.unique(df['SQL_ID'])\n",
    "    \n",
    "    for sql_id in unique_ids:\n",
    "        sql_plan = df[df['SQL_ID'] == sql_id]['OPERATION']\n",
    "        sql_match = sql_plan.str.cat(sep=' > ')        \n",
    "        df['SQL_MATCH'].loc[df['SQL_ID'] == sql_id] = sql_match \n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_matching_column(df=df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Formatting\n",
    "\n",
    "Constructs the tree plan structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanTreeModeller:\n",
    "    \"\"\"\n",
    "    This class simulates an access plan in the form of a tree structure\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def __create_node(node_name, parent=None):\n",
    "        \"\"\"\n",
    "        Builds a node which will be added to the tree. If the parent is 'None', it is assumed that this\n",
    "        node will be used as the root/parent Node.\n",
    "        \n",
    "        :param: node_name - String specifying node name.\n",
    "        :param: parent    - Parent node specifying parent node name.\n",
    "        \n",
    "        :return: anytree object\n",
    "        \"\"\"\n",
    "        if node_name is None:\n",
    "            raise ValueError('Node name was not specified!')\n",
    "        \n",
    "        if parent is None:\n",
    "            node = Node(node_name)\n",
    "        else:\n",
    "            node = Node(node_name, parent=parent)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_tree(df):\n",
    "        \"\"\"\n",
    "        This method receives a pandas dataframe, and converts it into a searchable python tree\n",
    "        \n",
    "        :param: df - Pandas Dataframe, pertaining to input access plan\n",
    "        \n",
    "        :return: Dictionary object, consisting of node objects (which are linked in a tree fashion)\n",
    "        \"\"\"\n",
    "        parent_node = None\n",
    "        node_dict = {}\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            # Build Node and add to parent\n",
    "            row_id = int(row['ID'])\n",
    "            parent_id = int(row['PARENT_ID'])\n",
    "            \n",
    "            if row_id == 0:\n",
    "                node = PlanTreeModeller.__create_node(node_name=row_id)\n",
    "            else:\n",
    "                parent_node = node_dict[parent_id]\n",
    "                node = PlanTreeModeller.__create_node(node_name=row_id, parent=parent_node)\n",
    "            node_dict[row_id] = node\n",
    "        \n",
    "        return node_dict # Dictionary consisting of tree nodes\n",
    "    \n",
    "    @staticmethod\n",
    "    def __retrieve_plan_details(df, node_name):\n",
    "        \"\"\"\n",
    "        Accepts a dataframe, and the node_name. Retrieves features pertaining to the row id in the access plan\n",
    "        \n",
    "        :param: df - Dataframe consisting of access plan features\n",
    "        :param: id - String id denoting which row to retrieve from the parameter dataframe\n",
    "        \n",
    "        :return: Dictionary consisting of access plan attributes\n",
    "        \"\"\"\n",
    "        operation = str(df[df['ID'] == node_name]['OPERATION'].iloc[0])\n",
    "        options = str(df[df['ID'] == node_name]['OPTIONS'].iloc[0])\n",
    "        object_name = str(df[df['ID'] == node_name]['OBJECT_NAME'].iloc[0])\n",
    "        cardinality = int(df[df['ID'] == node_name]['CARDINALITY'].iloc[0])\n",
    "        bytess = int(df[df['ID'] == node_name]['BYTES'].iloc[0])\n",
    "        partition_delta = int(df[df['ID'] == node_name]['PARTITION_STOP'].iloc[0]) - int(df[df['ID'] == node_name]['PARTITION_START'].iloc[0])\n",
    "        cpu_cost = int(df[df['ID'] == node_name]['CPU_COST'].iloc[0])\n",
    "        io_cost = int(df[df['ID'] == node_name]['IO_COST'].iloc[0])\n",
    "        temp_space = int(df[df['ID'] == node_name]['TEMP_SPACE'].iloc[0])\n",
    "        time = int(df[df['ID'] == node_name]['TIME'].iloc[0]) \n",
    "        \n",
    "        return {'OPERATION':operation,\n",
    "                'OPTIONS':options,\n",
    "                'OBJECT_NAME':object_name,\n",
    "                'CARDINALITY':cardinality,\n",
    "                'BYTES':bytess,\n",
    "                'PARTITION_DELTA':partition_delta,\n",
    "                'CPU_COST':cpu_cost,\n",
    "                'IO_COST':io_cost,\n",
    "                'TEMP_SPACE':temp_space,\n",
    "                'TIME':time}\n",
    "    \n",
    "    @staticmethod\n",
    "    def __tree_node_euclidean(tree_dict1, tree_dict2):\n",
    "        \"\"\"\n",
    "        This method calculates the eucldiean distance between two vectors.\n",
    "        \n",
    "        :param: tree_dict1 - Dictionary denoting a single node within plan / tree 1\n",
    "        :param: tree_dict2 - Dictionary denoting a single node within plan / tree 2\n",
    "        \n",
    "        :return: List denoting euclidean distance\n",
    "        \"\"\"\n",
    "        tree_vector_1 = [tree_dict1['CARDINALITY'],\n",
    "                         tree_dict1['BYTES'],\n",
    "                         tree_dict1['PARTITION_DELTA'],\n",
    "                         tree_dict1['CPU_COST'],\n",
    "                         tree_dict1['IO_COST'],\n",
    "                         tree_dict1['TEMP_SPACE'],\n",
    "                         tree_dict1['TIME']]\n",
    "        \n",
    "        tree_vector_2 = [tree_dict2['CARDINALITY'],\n",
    "                         tree_dict2['BYTES'],\n",
    "                         tree_dict2['PARTITION_DELTA'],\n",
    "                         tree_dict2['CPU_COST'],\n",
    "                         tree_dict2['IO_COST'],\n",
    "                         tree_dict2['TEMP_SPACE'],\n",
    "                         tree_dict2['TIME']]\n",
    "        \n",
    "        euc_distance = euclidean_distances([tree_vector_1],[tree_vector_2])\n",
    "        return euc_distance[0][0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def render_tree(tree, df):\n",
    "        \"\"\"\n",
    "        Renders Tree by printing to screen\n",
    "        \n",
    "        :param: tree - AnyTree object, representing tree modelled access plan\n",
    "        :param: df   - Pandas dataframe representatnt of the access plan about to be rendered\n",
    "        \n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for pre, fill, node in RenderTree(tree):\n",
    "            \n",
    "            access_plan_dict = PlanTreeModeller.__retrieve_plan_details(df=df,\n",
    "                                                                        node_name = node.name)\n",
    "            \n",
    "            if access_plan_dict['OBJECT_NAME'] == '0':\n",
    "                print(\"%s%s > %s\" % (pre, node.name, access_plan_dict['OPERATION']))\n",
    "            else:\n",
    "                if access_plan_dict['OPTIONS'] == '0': \n",
    "                    print(\"%s%s > %s (%s)\" % (pre, node.name, access_plan_dict['OPERATION'], access_plan_dict['OBJECT_NAME']))\n",
    "                else:\n",
    "                    print(\"%s%s > %s | %s (%s)\" % (pre, node.name, access_plan_dict['OPERATION'], access_plan_dict['OPTIONS'], access_plan_dict['OBJECT_NAME']))\n",
    "    \n",
    "    @staticmethod\n",
    "    def __postorder(tree):\n",
    "        \"\"\"\n",
    "        Accepts a tree, and iterates in post order fashion (left,right,root)\n",
    "        \n",
    "        :param: tree - Dictionary consisting of AnyTree Nodes\n",
    "        \n",
    "        :return: List consisting of tree traversal order\n",
    "        \"\"\"\n",
    "        post_order_traversal = [node.name for node in PostOrderIter(tree[0])]\n",
    "        return post_order_traversal\n",
    "    \n",
    "    @staticmethod\n",
    "    def tree_compare(tree1, tree2, df1, df2):\n",
    "        \"\"\"\n",
    "        Accepts two trees of type 'AnyTree', along with respective dataframe denoting each respective access\n",
    "        path.\n",
    "        \n",
    "        :param: tree1 - Dictionary consisting of 'AnyTree' nodes, belonging to tree 1\n",
    "        :param: tree2 - Dictionary consisting of 'AnyTree' nodes, belonging to tree 2\n",
    "        :param: df1   - Pandas dataframe consisting of access plan instructions opted for by tree 1\n",
    "        :param: df2   - Pandas dataframe consisting of access plan instructions opted for by tree 2\n",
    "        \n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieves traversal order for both trees\n",
    "        post_order_traversal1 = PlanTreeModeller.__postorder(tree1)\n",
    "        post_order_traversal2 = PlanTreeModeller.__postorder(tree2)\n",
    "        \n",
    "        # Iterates over traversal order, until a change is encountered\n",
    "        max_range = max(len(post_order_traversal1),len(post_order_traversal2))\n",
    "        delta_flag = True\n",
    "        euclidean_measure = []\n",
    "        for i in range(0,max_range):\n",
    "            \n",
    "            # This check avoids a list IndexError for scebarious when one plan is bigger than the others,\n",
    "            # and consequently the number of node traversals is bigger than the other tree.\n",
    "            if i >= len(post_order_traversal1) or i >= len(post_order_traversal2):\n",
    "                break\n",
    "            \n",
    "            # Retrive prior, current, and next nodes\n",
    "            try:\n",
    "                id_1_prev = post_order_traversal1[i-1]\n",
    "                id_2_prev = post_order_traversal2[i-1]\n",
    "            except IndexError:\n",
    "                id_1_prev = None\n",
    "                id_2_prev = None\n",
    "            try:\n",
    "                id_1 = post_order_traversal1[i]\n",
    "                id_2 = post_order_traversal2[i]\n",
    "            except IndexError:\n",
    "                id_1 = None\n",
    "                id_2 = None\n",
    "            try:\n",
    "                id_1_next = post_order_traversal1[i+1]\n",
    "                id_2_next = post_order_traversal2[i+1]\n",
    "            except IndexError:\n",
    "                id_1_next = None\n",
    "                id_2_next = None\n",
    "\n",
    "            if id_1_prev is not None and id_2_prev is not None:\n",
    "                pd_tree1_prev = PlanTreeModeller.__retrieve_plan_details(df=df1, node_name=id_1_prev)\n",
    "                pd_tree2_prev = PlanTreeModeller.__retrieve_plan_details(df=df2, node_name=id_2_prev)\n",
    "            if id_1 is not None and id_2 is not None:\n",
    "                pd_tree1 = PlanTreeModeller.__retrieve_plan_details(df=df1, node_name=id_1)\n",
    "                pd_tree2 = PlanTreeModeller.__retrieve_plan_details(df=df2, node_name=id_2)\n",
    "            if id_1_next is not None and id_2_next is not None:\n",
    "                pd_tree1_next = PlanTreeModeller.__retrieve_plan_details(df=df1, node_name=id_1_next)\n",
    "                pd_tree2_next = PlanTreeModeller.__retrieve_plan_details(df=df2, node_name=id_2_next)\n",
    "            \n",
    "            if (pd_tree1['OPERATION'] != pd_tree2['OPERATION'] or pd_tree1['OBJECT_NAME'] != pd_tree2['OBJECT_NAME'] or pd_tree1['OPTIONS'] != pd_tree2['OPTIONS']) and delta_flag:\n",
    "                print('Access Predicate Difference detected!')\n",
    "                print('Tree 1 difference at node [' + str(id_1) + '] operator > ' + pd_tree1['OPERATION'] + '(' + pd_tree1['OPTIONS'] + ') on object [' + pd_tree1['OBJECT_NAME'] + ']')\n",
    "                print('Tree 2 difference at node [' + str(id_2) + '] operator > ' + pd_tree2['OPERATION'] + '(' + pd_tree2['OPTIONS'] + ') on object [' + pd_tree2['OBJECT_NAME'] + ']')\n",
    "                PlanTreeModeller.render_tree(tree=tree1[0], df=df1) # Tree rendederer uses root node and traverses downwards\n",
    "                PlanTreeModeller.render_tree(tree=tree2[0], df=df2) # Tree rendederer uses root node and traverses downwards\n",
    "                \n",
    "                print('Stat Recommendation: ')\n",
    "                display_counter = 1\n",
    "                print(str(display_counter) + ') Collect [' + pd_tree1['OBJECT_TYPE'] + '] stats on [' + pd_tree1['OBJECT_NAME'] + ']')\n",
    "                display_counter += 1\n",
    "                if id_1_prev is not None:\n",
    "                    print(str(display_counter) + ') Collect [' + pd_tree1_prev['OBJECT_TYPE'] + '] stats on [' + pd_tree1_prev['OBJECT_NAME'] + ']')\n",
    "                    display_counter += 1\n",
    "                if id_1_next is not None:\n",
    "                    print(str(display_counter) + ') Collect [' + pd_tree1_next['OBJECT_TYPE'] + '] stats on [' + pd_tree1_next['OBJECT_NAME'] + ']')\n",
    "                    display_counter += 1\n",
    "                delta_flag = False\n",
    "            \n",
    "            # Calculate Node Euclidean Measure\n",
    "            euclidean_vector = PlanTreeModeller.__tree_node_euclidean(tree_dict1=pd_tree1,\n",
    "                                                                      tree_dict2=pd_tree2)\n",
    "            euclidean_measure.append(euclidean_vector)\n",
    "            \n",
    "        if delta_flag is not False and sum(euclidean_measure) > 10000:\n",
    "            print('Access Predicate Difference detected!')\n",
    "            print('Plan structure was the same, but a big operator difference was detected with delta score [' + str(sum(euclidean_measure))  + ']')\n",
    "            PlanTreeModeller.render_tree(tree=tree1[0], df=df1) # Tree rendederer uses root node and traverses downwards\n",
    "            PlanTreeModeller.render_tree(tree=tree2[0], df=df2) # Tree rendederer uses root node and traverses downwards\n",
    "        \n",
    "        if delta_flag:\n",
    "            print('No plan differences detected.')\n",
    "        \n",
    "        print('Total computed delta score [' + str(sum(euclidean_measure)) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Testing Streams\n",
    "\n",
    "This cell builds a total of 4 lists, composed as follows:\n",
    "\n",
    "* Expected Stream, composed of SQL queries with which comparison will be made.\n",
    "* Variant Stream, with intermingled hint outliers\n",
    "* Variant Stream, with intermingled predicate outliers\n",
    "* Variant Stream, with intermingled rownum outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL:\n",
      "['dxv968j0352kb', '0aq14dznn91rg', '4q1rzhn63sgpt', 'g0b4snpj74cv5', '03ggjrmy0wa1w', '93n8wp5a8xyxn', 'gdh0vpjkmbtjw', '1r7b985mxqj71', '1jhyrdp21f2q6', 'bwsf4tnh0gcgv', '29mjaymwt5p6d', 'aggcw7yk1a7s6', '8t26unxsrxj72', '71uursqtj1j2m', '59zh1b9759nf4', '0f60bzgt9127c', '9vmcsc3prvxpa', '76ds5wxsv7f5t', '20bqsr6btd9x9', '84ntdbh48ctu9', 'fx7sjdj48pn6z', '2wuhkcaz4uhs5', '2pz0tqbv91m11', '1u97hwfu7dcmz', '39nyc1pykjg41', '9ffht8tuysgx9', '7fbzhzg6ysu25', '14f5ngrj3cc5h', '4u268zn6r57tm', '6zcux9jb78w36', '2hnpu9m861609', '33vw5865cwyyn', 'gvcadr1hm4arv', '1pv23p59mjs0v', '7vtvbg7s3zcyp', '6fvfqaw68q59b', 'cqrk7q7f6nk3d', 'a6g97rawd3ggv', '85cmvvurya34f', '0ga8vk4nftz45', '13a9r2xkx1bxb', 'cjq93m442uprp', '7w116jy6ysqpm', 'au8ztarrm6vvs', '5g88vmdgd99f7', 'gkjkxbzzptg00', '4cgbvpjc134nu', 'a6fy23us0jz84', '64wct1dn3b771', 'fs2q92rb37cb0', '4vcvqy7vvy5zm', '8ymq1gb13rfab', 'b4j3z1g2nwfys', 'cbhzzp1d7dhkc', '9yw1cvrf3wmcz', '2046z7kdh823h', '8hb1p1z9z4wfb', '6zs29hb3gpcf5', '0jj0ct4x4gy27', '95y5r5ksf94b1', 'cfk18fw3fynvs', '7uzyw6t048658', '6nc5c1qczss8j', 'cwzcht2y54876', '7ts87fuvk3dhz']\n",
      "<class 'list'>\n",
      "65\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HINT_VARIANTS:\n",
      "[12354 12355 12358 12359 12360 12362 12363 12364 12366 12367 12368 12369\n",
      " 12370 12371]\n",
      "<class 'numpy.ndarray'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PREDICATE_VARIANTS:\n",
      "[12372 12373 12374 12375 12376 12377 12378 12379 12380 12381 12382 12383\n",
      " 12384 12385]\n",
      "<class 'numpy.ndarray'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ROWNUM_VARIANTS:\n",
      "[12386 12387 12388 12389 12390 12391 12392 12393 12394 12395 12396 12397\n",
      " 12398 12399]\n",
      "<class 'numpy.ndarray'>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Unique set of PLAN_HASH_VALUES\n",
    "np_sql_id = pd.unique(df['SQL_ID'])\n",
    "\n",
    "# Remove those which are not originating from TPC-DS\n",
    "filtered_sql = []\n",
    "for sql in np_sql_id:\n",
    "    \n",
    "    df_temp_plan = df[df['SQL_ID'] == sql]\n",
    "\n",
    "    # This step ensures that only TPC-DS related queries are displayed\n",
    "    tpc_check = df_temp_plan['OBJECT_OWNER'].tolist()\n",
    "    if tpcds not in tpc_check:\n",
    "        continue\n",
    "        \n",
    "    #\n",
    "    # Discards plans with double entries - Due to the parallel nature of the throughput test for \n",
    "    # TPC-DS, multiple threads may execute the same query at the same time, resulting in sql access\n",
    "    # plans with the same SQL_ID, same PLAN_HASH_VALUE, and same TIMESTAMP. Such occurances are skipped.\n",
    "    df_temp_count = df_temp_plan[df_temp_plan['ID'] == 0]\n",
    "    if df_temp_count.shape[0] != 1:\n",
    "        continue\n",
    "        \n",
    "    filtered_sql.append(sql)\n",
    "np_sql_id= filtered_sql \n",
    "\n",
    "print('ACTUAL:')\n",
    "print(np_sql_id)\n",
    "print(type(np_sql_id))\n",
    "print(len(np_sql_id))\n",
    "print('-'*100)\n",
    "\n",
    "# Retrieve Unique set of PLAN_IDs for hint outliers\n",
    "np_hint_outlier_plan_id = pd.unique(df_hints_outliers['PLAN_ID'])\n",
    "print('HINT_VARIANTS:')\n",
    "print(np_hint_outlier_plan_id)\n",
    "print(type(np_hint_outlier_plan_id))\n",
    "print('-'*100)\n",
    "\n",
    "# Retrieve Unique set of PLAN_IDs for predicate outliers\n",
    "np_predicate_outlier_plan_id = pd.unique(df_predicate_outliers['PLAN_ID'])\n",
    "print('PREDICATE_VARIANTS:')\n",
    "print(np_predicate_outlier_plan_id)\n",
    "print(type(np_predicate_outlier_plan_id))\n",
    "print('-'*100)\n",
    "\n",
    "# Retrieve Unique set of PLAN_IDs for rownum outliers\n",
    "np_rownum_outlier_plan_id = pd.unique(df_rownum_outliers['PLAN_ID'])\n",
    "print('ROWNUM_VARIANTS:')\n",
    "print(np_rownum_outlier_plan_id)\n",
    "print(type(np_rownum_outlier_plan_id))\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Comparison with Hint Based Outliers\n",
    "\n",
    "Compares the outlier queries with those in the inlier set. Uses fuzzy wuzzy library for Lehvensthein plan comparison to link the two sets together (since SQL_ID is different between two sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "Query variant [5] with plan_id [12354]\n",
      "Access Predicate Difference detected!\n",
      "Tree 1 difference at node [8] operator > TABLE ACCESS(FULL) on object [WAREHOUSE]\n",
      "Tree 2 difference at node [10] operator > TABLE ACCESS(FULL) on object [DATE_DIM]\n",
      "0 > SELECT STATEMENT\n",
      "└── 1 > COUNT\n",
      "    └── 2 > VIEW\n",
      "        └── 3 > SORT\n",
      "            └── 4 > VIEW\n",
      "                └── 5 > UNION-ALL\n",
      "                    ├── 6 > HASH\n",
      "                    │   └── 7 > HASH JOIN\n",
      "                    │       ├── 8 > TABLE ACCESS | FULL (WAREHOUSE)\n",
      "                    │       └── 9 > HASH JOIN\n",
      "                    │           ├── 10 > TABLE ACCESS | FULL (TIME_DIM)\n",
      "                    │           └── 11 > HASH JOIN\n",
      "                    │               ├── 12 > TABLE ACCESS | FULL (SHIP_MODE)\n",
      "                    │               └── 13 > HASH JOIN\n",
      "                    │                   ├── 14 > NESTED LOOPS\n",
      "                    │                   │   ├── 15 > NESTED LOOPS\n",
      "                    │                   │   │   ├── 16 > STATISTICS COLLECTOR\n",
      "                    │                   │   │   │   └── 17 > TABLE ACCESS | FULL (DATE_DIM)\n",
      "                    │                   │   │   └── 18 > INDEX | RANGE SCAN (WS_SOLD_DATE_SK_INDEX)\n",
      "                    │                   │   └── 19 > TABLE ACCESS | BY INDEX ROWID (WEB_SALES)\n",
      "                    │                   └── 20 > TABLE ACCESS | FULL (WEB_SALES)\n",
      "                    └── 21 > HASH\n",
      "                        └── 22 > HASH JOIN\n",
      "                            ├── 23 > TABLE ACCESS | FULL (WAREHOUSE)\n",
      "                            └── 24 > HASH JOIN\n",
      "                                ├── 25 > TABLE ACCESS | FULL (TIME_DIM)\n",
      "                                └── 26 > HASH JOIN\n",
      "                                    ├── 27 > TABLE ACCESS | FULL (SHIP_MODE)\n",
      "                                    └── 28 > HASH JOIN\n",
      "                                        ├── 29 > NESTED LOOPS\n",
      "                                        │   ├── 30 > NESTED LOOPS\n",
      "                                        │   │   ├── 31 > STATISTICS COLLECTOR\n",
      "                                        │   │   │   └── 32 > TABLE ACCESS | FULL (DATE_DIM)\n",
      "                                        │   │   └── 33 > INDEX | RANGE SCAN (CS_SOLD_DATE_SK_INDEX)\n",
      "                                        │   └── 34 > TABLE ACCESS | BY INDEX ROWID (CATALOG_SALES)\n",
      "                                        └── 35 > TABLE ACCESS | FULL (CATALOG_SALES)\n",
      "0 > SELECT STATEMENT\n",
      "└── 1 > COUNT\n",
      "    └── 2 > VIEW\n",
      "        └── 3 > SORT\n",
      "            └── 4 > VIEW\n",
      "                └── 5 > UNION-ALL\n",
      "                    ├── 6 > HASH\n",
      "                    │   └── 7 > NESTED LOOPS\n",
      "                    │       ├── 8 > NESTED LOOPS\n",
      "                    │       │   ├── 9 > HASH JOIN\n",
      "                    │       │   │   ├── 10 > TABLE ACCESS | FULL (DATE_DIM)\n",
      "                    │       │   │   └── 11 > VIEW\n",
      "                    │       │   │       └── 12 > UNION-ALL\n",
      "                    │       │   │           ├── 13 > TABLE ACCESS | FULL (STORE_SALES)\n",
      "                    │       │   │           └── 14 > TABLE ACCESS | FULL (STORE_RETURNS)\n",
      "                    │       │   └── 15 > INDEX | UNIQUE SCAN (SYS_C0021425)\n",
      "                    │       └── 16 > TABLE ACCESS | BY INDEX ROWID (STORE)\n",
      "                    ├── 17 > HASH\n",
      "                    │   └── 18 > HASH JOIN\n",
      "                    │       ├── 19 > NESTED LOOPS\n",
      "                    │       │   ├── 20 > TABLE ACCESS | FULL (DATE_DIM)\n",
      "                    │       │   └── 21 > VIEW\n",
      "                    │       │       └── 22 > UNION-ALL\n",
      "                    │       │           ├── 23 > TABLE ACCESS | BY INDEX ROWID BATCHED (CATALOG_SALES)\n",
      "                    │       │           │   └── 24 > INDEX | RANGE SCAN (CS_SOLD_DATE_SK_INDEX)\n",
      "                    │       │           └── 25 > TABLE ACCESS | BY INDEX ROWID BATCHED (CATALOG_RETURNS)\n",
      "                    │       │               └── 26 > INDEX | RANGE SCAN (CR_RETURNED_DATE_SK_INDEX)\n",
      "                    │       └── 27 > TABLE ACCESS | FULL (CATALOG_PAGE)\n",
      "                    └── 28 > HASH\n",
      "                        └── 29 > NESTED LOOPS\n",
      "                            ├── 30 > NESTED LOOPS\n",
      "                            │   ├── 31 > NESTED LOOPS\n",
      "                            │   │   ├── 32 > TABLE ACCESS | FULL (DATE_DIM)\n",
      "                            │   │   └── 33 > VIEW\n",
      "                            │   │       └── 34 > UNION-ALL\n",
      "                            │   │           ├── 35 > TABLE ACCESS | BY INDEX ROWID BATCHED (WEB_SALES)\n",
      "                            │   │           │   └── 36 > INDEX | RANGE SCAN (WS_SOLD_DATE_SK_INDEX)\n",
      "                            │   │           └── 37 > NESTED LOOPS\n",
      "                            │   │               ├── 38 > TABLE ACCESS | BY INDEX ROWID BATCHED (WEB_RETURNS)\n",
      "                            │   │               │   └── 39 > INDEX | RANGE SCAN (WR_RETURNED_DATE_SK_INDEX)\n",
      "                            │   │               └── 40 > TABLE ACCESS | BY INDEX ROWID (WEB_SALES)\n",
      "                            │   │                   └── 41 > INDEX | UNIQUE SCAN (SYS_C0021461)\n",
      "                            │   └── 42 > INDEX | UNIQUE SCAN (SYS_C0021434)\n",
      "                            └── 43 > TABLE ACCESS | BY INDEX ROWID (WEB_SITE)\n",
      "Stat Recommendation: \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'OBJECT_TYPE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8f5ecb24f139>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                                   \u001b[0mtree2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutlier_plan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                                   \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_inlier_plan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                                   df2=df_outlier_plan)\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-16f8aa1b4a49>\u001b[0m in \u001b[0;36mtree_compare\u001b[1;34m(tree1, tree2, df1, df2)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Stat Recommendation: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[0mdisplay_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_counter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m') Collect ['\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpd_tree1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'OBJECT_TYPE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'] stats on ['\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpd_tree1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'OBJECT_NAME'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m']'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m                 \u001b[0mdisplay_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mid_1_prev\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'OBJECT_TYPE'"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for plan in np_hint_outlier_plan_id:\n",
    "    #print(plan)\n",
    "    print('\\n\\n---------------------------------------------------\\nQuery variant [' + str(variant_ids[counter]) + '] with plan_id [' + str(plan) + ']')\n",
    "    sql_plan = df_hints_outliers[df_hints_outliers['PLAN_ID'] == plan]['OPERATION']\n",
    "    sql_match = sql_plan.str.cat(sep=' > ')\n",
    "    \n",
    "    sql_id_list, inlier_plans = [], []\n",
    "    for sql in np_sql_id:\n",
    "        sql_plan2 = df[df['SQL_ID'] == sql]['OPERATION']\n",
    "        sql_match2 = sql_plan2.str.cat(sep=' > ')\n",
    "        inlier_plans.append(sql_match2)\n",
    "        sql_id_list.append(sql)\n",
    "    \n",
    "    inlier_match = process.extractOne(sql_match, inlier_plans)\n",
    "    #print(inlier_match)\n",
    "    inlier_sql_id = None\n",
    "    for i in range(len(inlier_plans)):\n",
    "\n",
    "        if inlier_plans[i] == inlier_match[0]:\n",
    "            inlier_sql_id = sql_id_list[i]\n",
    "            break\n",
    "    \n",
    "    # Reads Inlier and Outlier plans into memory (Pandas Dataframes)\n",
    "    df_inlier_plan = df[df['SQL_ID'] == inlier_sql_id]\n",
    "    df_inlier_plan = df_inlier_plan.sort_values(by='ID', ascending=True)\n",
    "    df_outlier_plan = df_hints_outliers[df_hints_outliers['PLAN_ID'] == plan]\n",
    "    df_outlier_plan = df_outlier_plan.sort_values(by='ID', ascending=True)\n",
    "    \n",
    "    # Builds Trees\n",
    "    inlier_tree = PlanTreeModeller.build_tree(df=df_inlier_plan)\n",
    "    outlier_plan = PlanTreeModeller.build_tree(df=df_outlier_plan)\n",
    "    \n",
    "    # Compare Trees\n",
    "    PlanTreeModeller.tree_compare(tree1=inlier_tree, \n",
    "                                  tree2=outlier_plan, \n",
    "                                  df1=df_inlier_plan, \n",
    "                                  df2=df_outlier_plan)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Comparison with Predicate Based Outliers\n",
    "\n",
    "Compares the expected stream with variation stream. Variations found here will be composed of SQL optimizer hint injections to purposely skew the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for plan in np_predicate_outlier_plan_id:\n",
    "    #print(plan)\n",
    "    print('\\n\\n---------------------------------------------------\\nQuery variant [' + str(variant_ids[counter]) + '] with plan_id [' + str(plan) + ']')\n",
    "    sql_plan = df_predicate_outliers[df_predicate_outliers['PLAN_ID'] == plan]['OPERATION']\n",
    "    sql_match = sql_plan.str.cat(sep=' > ')\n",
    "    \n",
    "    sql_id_list, inlier_plans = [], []\n",
    "    for sql in np_sql_id:\n",
    "        sql_plan2 = df[df['SQL_ID'] == sql]['OPERATION']\n",
    "        sql_match2 = sql_plan2.str.cat(sep=' > ')\n",
    "        inlier_plans.append(sql_match2)\n",
    "        sql_id_list.append(sql)\n",
    "    \n",
    "    inlier_match = process.extractOne(sql_match, inlier_plans)\n",
    "    #print(inlier_match)\n",
    "    inlier_sql_id = None\n",
    "    for i in range(len(inlier_plans)):\n",
    "\n",
    "        if inlier_plans[i] == inlier_match[0]:\n",
    "            inlier_sql_id = sql_id_list[i]\n",
    "            break\n",
    "    \n",
    "    # Reads Inlier and Outlier plans into memory (Pandas Dataframes)\n",
    "    df_inlier_plan = df[df['SQL_ID'] == inlier_sql_id]\n",
    "    df_inlier_plan = df_inlier_plan.sort_values(by='ID', ascending=True)\n",
    "    df_outlier_plan = df_predicate_outliers[df_predicate_outliers['PLAN_ID'] == plan]\n",
    "    df_outlier_plan = df_outlier_plan.sort_values(by='ID', ascending=True)\n",
    "    \n",
    "    # Builds Trees\n",
    "    inlier_tree = PlanTreeModeller.build_tree(df=df_inlier_plan)\n",
    "    outlier_plan = PlanTreeModeller.build_tree(df=df_outlier_plan)\n",
    "    \n",
    "    # Compare Trees\n",
    "    PlanTreeModeller.tree_compare(tree1=inlier_tree, \n",
    "                                  tree2=outlier_plan, \n",
    "                                  df1=df_inlier_plan, \n",
    "                                  df2=df_outlier_plan)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Comparison with Rownum Based Outliers\n",
    "\n",
    "Compares the expected stream with variation stream. Variations found here will be composed of SQL optimizer hint injections to purposely skew the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for plan in np_rownum_outlier_plan_id:\n",
    "    #print(plan)\n",
    "    print('\\n\\n---------------------------------------------------\\nQuery variant [' + str(variant_ids[counter]) + '] with plan_id [' + str(plan) + ']')\n",
    "    sql_plan = df_rownum_outliers[df_rownum_outliers['PLAN_ID'] == plan]['OPERATION']\n",
    "    sql_match = sql_plan.str.cat(sep=' > ')\n",
    "    \n",
    "    sql_id_list, inlier_plans = [], []\n",
    "    for sql in np_sql_id:\n",
    "        sql_plan2 = df[df['SQL_ID'] == sql]['OPERATION']\n",
    "        sql_match2 = sql_plan2.str.cat(sep=' > ')\n",
    "        inlier_plans.append(sql_match2)\n",
    "        sql_id_list.append(sql)\n",
    "    \n",
    "    inlier_match = process.extractOne(sql_match, inlier_plans)\n",
    "    #print(inlier_match)\n",
    "    inlier_sql_id = None\n",
    "    for i in range(len(inlier_plans)):\n",
    "\n",
    "        if inlier_plans[i] == inlier_match[0]:\n",
    "            inlier_sql_id = sql_id_list[i]\n",
    "            break\n",
    "    \n",
    "    # Reads Inlier and Outlier plans into memory (Pandas Dataframes)\n",
    "    df_inlier_plan = df[df['SQL_ID'] == inlier_sql_id]\n",
    "    df_inlier_plan = df_inlier_plan.sort_values(by='ID', ascending=True)\n",
    "    df_outlier_plan = df_rownum_outliers[df_rownum_outliers['PLAN_ID'] == plan]\n",
    "    df_outlier_plan = df_outlier_plan.sort_values(by='ID', ascending=True)\n",
    "    \n",
    "    # Builds Trees\n",
    "    inlier_tree = PlanTreeModeller.build_tree(df=df_inlier_plan)\n",
    "    outlier_plan = PlanTreeModeller.build_tree(df=df_outlier_plan)\n",
    "    \n",
    "    # Compare Trees\n",
    "    PlanTreeModeller.tree_compare(tree1=inlier_tree, \n",
    "                                  tree2=outlier_plan, \n",
    "                                  df1=df_inlier_plan, \n",
    "                                  df2=df_outlier_plan)\n",
    "    \n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
