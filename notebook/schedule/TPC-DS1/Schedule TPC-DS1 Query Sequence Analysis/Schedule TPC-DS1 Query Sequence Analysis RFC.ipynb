{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis (RFC)\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.20.0\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=3 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "#\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "#\n",
    "# Forest Config\n",
    "parallel_degree = 4\n",
    "n_estimators = 500\n",
    "max_depth = 7\n",
    "criterion='gini'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3018: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "#rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "#\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (64912, 90)\n",
      "Shape After Aggregation: (820, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     28190  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     28191  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2     28192  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     28193  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4     28194  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5     28195  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6     28196  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7     28197  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     28198  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9     28199  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "10    28200  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "11    28201  [06g9mhm5ba7tt, 09vrdx888wvvb, 0kcbwucxmazcp, ...\n",
      "12    28202  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    28203  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "14    28204  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "15    28205  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "16    28206  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "17    28207  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "18    28208  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "19    28209  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "20    28210  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "21    28211  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "22    28212  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "23    28213  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "24    28214  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "25    28215  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "26    28216  [06dymzb481vnd, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "27    28217  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "28    28218  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "29    28219  [06g9mhm5ba7tt, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "..      ...                                                ...\n",
      "70    28260  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "71    28261  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "72    28262  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "73    28263  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "74    28264  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "75    28265  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "76    28266  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "77    28267  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "78    28268  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "79    28269  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "80    28270  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    28271  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "82    28272  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "83    28273  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "84    28274  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "85    28275  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "86    28276  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "87    28277  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "88    28278  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "89    28279  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1p5grz1gs7fjq, ...\n",
      "90    28280  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "91    28281  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "92    28282  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "93    28283  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "94    28284  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "95    28285  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "96    28286  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "97    28287  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "98    28288  [0aadu2kp270dv, 0gyb2wqdu4d9k, 0kkhhb2w93cx0, ...\n",
      "99    28289  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "df = rep_hist_snapshot_df.groupby(['SNAP_ID'])['SQL_ID'].apply(list).reset_index()\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(df.shape))\n",
    "print(type(df))\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 2)\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     28190  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     28191  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2     28192  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     28193  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4     28194  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5     28195  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6     28196  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7     28197  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     28198  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9     28199  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "10    28200  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "11    28201  [06g9mhm5ba7tt, 09vrdx888wvvb, 0kcbwucxmazcp, ...\n",
      "12    28202  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    28203  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "14    28204  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "15    28205  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "16    28206  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "17    28207  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "18    28208  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "19    28209  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "20    28210  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "21    28211  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "22    28212  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "23    28213  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "24    28214  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "25    28215  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "26    28216  [06dymzb481vnd, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "27    28217  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "28    28218  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "29    28219  [06g9mhm5ba7tt, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "..      ...                                                ...\n",
      "70    28260  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "71    28261  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "72    28262  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "73    28263  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "74    28264  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "75    28265  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "76    28266  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "77    28267  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "78    28268  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "79    28269  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "80    28270  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    28271  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "82    28272  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "83    28273  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "84    28274  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "85    28275  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "86    28276  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "87    28277  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "88    28278  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "89    28279  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1p5grz1gs7fjq, ...\n",
      "90    28280  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "91    28281  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "92    28282  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "93    28283  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "94    28284  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "95    28285  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "96    28286  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "97    28287  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "98    28288  [0aadu2kp270dv, 0gyb2wqdu4d9k, 0kkhhb2w93cx0, ...\n",
      "99    28289  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.sort_index(ascending=True,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection\n",
    "\n",
    "This sextion treats the dataset as a univariate dataset. Therefore the SNAP_ID pertaining to each set of SQL_IDs is removed, with the intent of future classifiers training solely on past SQL executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 2)\n",
      "(820, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "del df['SNAP_ID']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "\n",
    "NB: Since this experiment is solely focussed on Random Forest Training, One-Hot Encoding will not be used as recommended below:\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \"\"\"\n",
    "    Scikit Label Encoder was acting up with the following error whilst using the transform function, even though I tripled \n",
    "    checked that the passed data was exactly the same as the one used for training:\n",
    "    \n",
    "    * https://stackoverflow.com/questions/46288517/getting-valueerror-y-contains-new-labels-when-using-scikit-learns-labelencoder\n",
    "    \n",
    "    So I have rebuilt a similar functionality to categorize my data into numeric digits, as the LabelEncoder is supposed to do.\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__class_map = {}\n",
    "        self.__integer_counter = 0\n",
    "    #\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        :param - X: python list\n",
    "        \"\"\"\n",
    "        for val in X:\n",
    "            if val not in self.__class_map:\n",
    "                self.__class_map[val] = self.__integer_counter\n",
    "                self.__integer_counter += 1\n",
    "    #\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        param - X: python list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for val in X:\n",
    "            if val in self.__class_map:\n",
    "                value = self.__class_map[val]\n",
    "                encoded_map.append(value)\n",
    "            else:\n",
    "                raise ValueError('Label Mismatch - Encountered a label which was not trained on.')\n",
    "        return encoded_map\n",
    "    #\n",
    "    def get_class_map(self):\n",
    "        \"\"\"\n",
    "        Returns original classes as a list\n",
    "        \"\"\"\n",
    "        class_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            class_map.append(key)\n",
    "        return class_map\n",
    "    #\n",
    "    def get_encoded_map(self):\n",
    "        \"\"\"\n",
    "        Returns class encodings as a list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            encoded_map.append(value)\n",
    "        return encoded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 1)\n",
      "                                              SQL_ID\n",
      "0  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "Total SQL_ID Classes: 1046\n",
      "['03ggjrmy0wa1w', '06dymzb481vnd', '0aq14dznn91rg', '0f60bzgt9127c', '0ga8vk4nftz45', '13a9r2xkx1bxb', '13ys8ux8xvrbm', '14f5ngrj3cc5h', '1jhyrdp21f2q6', '1p5grz1gs7fjq']\n",
      "(820, 1)\n",
      "                                              SQL_ID\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
      "1  [71, 72, 72, 73, 73, 73, 74, 75, 76, 77, 78, 9...\n",
      "2  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "3  [134, 134, 134, 1, 72, 72, 73, 73, 73, 135, 75...\n",
      "4  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "5  [167, 168, 169, 170, 9, 171, 172, 173, 174, 17...\n",
      "6  [167, 168, 169, 170, 215, 9, 216, 217, 171, 21...\n",
      "7  [134, 134, 134, 1, 169, 243, 73, 73, 73, 135, ...\n",
      "8  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "9  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "le = LabelEncoder()\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    le.fit(sql_id_list)\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    transformed_list = le.transform(sql_id_list)\n",
    "    df['SQL_ID'].iloc[index] = transformed_list \n",
    "#\n",
    "print(\"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "print('Total SQL_ID Classes: ' + str(len(le.get_class_map())))\n",
    "print(le.get_class_map()[:10])\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "A note regarding normalization. Normalization for this experiment was purposely skipped, since value dimensionality & size is not as important for RandomForest based models. The purity split  does not benefit greatly from such a process:\n",
    "\n",
    "* https://stats.stackexchange.com/questions/57010/is-it-essential-to-do-normalization-for-svm-and-random-forest\n",
    "* https://stackoverflow.com/questions/8961586/do-i-need-to-normalize-or-scale-data-for-randomforest-r-package\n",
    "* https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-8-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Padding\n",
    "\n",
    "Since there isn't a fixed number of SQL_ID's per SNAP_ID, each set of SQL_IDs need to be padded so as to assume an equal number if SQL_IDs for the purpose of model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length at index 0: 80\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 52, 52, 52, 52, 53, 54, 55, 55, 55, 56, 57, 58, 59, 59, 59, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
      "Length at index 1: 81\n",
      "[71, 72, 72, 73, 73, 73, 74, 75, 76, 77, 78, 9, 79, 80, 81, 82, 83, 84, 85, 86, 24, 87, 88, 88, 89, 90, 91, 92, 93, 93, 94, 95, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 111, 53, 112, 113, 114, 115, 115, 116, 117, 118, 119, 119, 119, 119, 120, 120, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133]\n",
      "Length at index 2: 91\n",
      "[134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78, 9, 79, 136, 136, 83, 85, 24, 88, 88, 89, 91, 92, 93, 93, 94, 95, 95, 96, 97, 100, 101, 137, 138, 139, 139, 139, 102, 103, 140, 141, 104, 105, 142, 143, 144, 107, 108, 108, 109, 110, 52, 52, 52, 52, 52, 53, 112, 113, 55, 55, 55, 115, 115, 116, 145, 117, 59, 59, 59, 59, 118, 119, 119, 119, 119, 120, 120, 120, 121, 122, 63, 126, 126, 128, 129, 146, 130, 132, 132, 147]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-7c702ae534d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_datamatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n------------------------------------------\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-7c702ae534d2>\u001b[0m in \u001b[0;36mpad_datamatrix\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_row_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SQL_ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SQL_ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Appends -1 to padded values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;31m# print(\"Length at index \" + str(i) + \": \" + str(df['SQL_ID'].iloc[i].size))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    650\u001b[0m             self.obj._data = self.obj._data.setitem(indexer=indexer,\n\u001b[0;32m    651\u001b[0m                                                     value=value)\n\u001b[1;32m--> 652\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_align_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiindex_indexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_maybe_update_cacher\u001b[1;34m(self, clear, verify_is_copy)\u001b[0m\n\u001b[0;32m   2565\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2566\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2567\u001b[1;33m                     \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cache_changed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcacher\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2568\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2569\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_maybe_cache_changed\u001b[1;34m(self, item, value)\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \"\"\"The object has called back to us saying maybe it has changed.\n\u001b[0;32m   2523\u001b[0m         \"\"\"\n\u001b[1;32m-> 2524\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, item, value, check)\u001b[0m\n\u001b[0;32m   4251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4252\u001b[0m         \u001b[0mblknos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4253\u001b[1;33m         \u001b[0mblklocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4255\u001b[0m         \u001b[0munfit_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])\n",
    "#\n",
    "# Retrieve largest length\n",
    "def pad_datamatrix(df):\n",
    "    \"\"\"\n",
    "    Iterates over dataframe and pads SQL_ID lists accordingly with -1 values\n",
    "    \"\"\"\n",
    "    row_sizes = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_sizes.append(len(row['SQL_ID']))\n",
    "    max_row_size = max(row_sizes)\n",
    "    #\n",
    "    # Pad Dataframe Values\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        length = len(row['SQL_ID'])\n",
    "        diff = max_row_size - length\n",
    "        if diff != 0:\n",
    "            for j in range(length, max_row_size):\n",
    "                df['SQL_ID'].iloc[i] = np.append(df['SQL_ID'].iloc[i], -1) # Appends -1 to padded values\n",
    "        # print(\"Length at index \" + str(i) + \": \" + str(df['SQL_ID'].iloc[i].size))\n",
    "        i += 1\n",
    "    return df\n",
    "#\n",
    "df = pad_datamatrix(df)\n",
    "#\n",
    "print('\\n\\n------------------------------------------\\n\\n')\n",
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_row = []\n",
    "for i in range(lag+1,(lag*2)+2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns)*i:len(df.columns)*i + 1]\n",
    "    y_row.append(y_df_column_names)\n",
    "y_df_column_names = []   \n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "#\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "print('\\n-------------\\nLabels After Time Shift')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Feature Lists\n",
    "\n",
    "Expand Feature Lists, where in each list element is represented as it's own features. Total feature count here equates as follows:\n",
    "\n",
    "* Features = (lag * SQL_ID per SNAP_ID count) + SQL_ID per SNAP_ID count\n",
    "* Labels = lag * SQL_ID per SNAP_ID count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sequence2features(df):\n",
    "    \"\"\"\n",
    "    Converts pandas sequences into full fledged columns/features\n",
    "    \"\"\"\n",
    "    feature_count = len(df[df.columns[0]].iloc[0])\n",
    "    for column_name in df.columns:\n",
    "        data_matrix = []\n",
    "        new_values = df[column_name].values\n",
    "        #\n",
    "        new_values = np.stack(new_values, axis=0 )\n",
    "        #\n",
    "        for i in range(1,feature_count+1):\n",
    "            new_column_name = column_name + \"_\"+str(i)\n",
    "            df[new_column_name] = new_values[:,i-1]\n",
    "        #\n",
    "        # Drop original list columns\n",
    "        df.drop(column_name, inplace=True, axis=1)\n",
    "    return df\n",
    "#\n",
    "print('Features')\n",
    "print('Before: ' + str(X_df.shape))\n",
    "X_df = sequence2features(df=X_df)\n",
    "print('After: ' + str(X_df.shape))\n",
    "#\n",
    "print('Labels')\n",
    "print('Before: ' + str(y_df.shape))\n",
    "y_df = sequence2features(df=y_df)\n",
    "print('After: ' + str(y_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Removing Null Columns - this check is redundant, but it double checks that there are no useless/flatline columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before: ' + str(df.shape))\n",
    "#\n",
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "df = drop_flatline_columns(df=df)\n",
    "print('After: ' + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based Model (Many to Many Approach)\n",
    "\n",
    "### RandomForest Classification (Many To Many)\n",
    "\n",
    "Classification attemps using RFC - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Model Input - Takes training data in the form of past SQL_ID sequences, and trains on a number of past sequence histories, determined by the lag value\n",
    "\n",
    "Model Output - Outpus future SQL_ID sequences, determined by the lag output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random Forest\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest Class (Regression + Classification)\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, n_estimators, max_depth=None, criterion='gini', parallel_degree=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.parallel_degree=parallel_degree\n",
    "        self.criterion = criterion\n",
    "        self.model = RandomForestClassifier(max_depth=self.max_depth,\n",
    "                                            n_estimators=self.n_estimators,\n",
    "                                            criterion=self.criterion,\n",
    "                                            n_jobs=self.parallel_degree)\n",
    "    #\n",
    "    def fit_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits training data to target labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X,y)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, lag, plot=False):\n",
    "        \"\"\"\n",
    "        Runs test data through previously trained model, and evaluate differently depending if a regression of classification model\n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # F1-Score Evaluation\n",
    "        feature_count = int(y.shape[1] / lag)\n",
    "        acc_score, f_score = .0,.0\n",
    "        counter = 0\n",
    "        for i in range(y.shape[1]):\n",
    "            acc = accuracy_score(y[:,i], yhat[:,i])\n",
    "            acc_score += acc\n",
    "            f1 = f1_score(y[:,i], yhat[:,i], average='micro') \n",
    "            f_score += f1\n",
    "            counter += 1\n",
    "            if i % (feature_count-1) == 0 and i != 0:\n",
    "                acc_score = acc_score/feature_count # Averaging accuracy accross total sql_ids for that particular timestep\n",
    "                f_score = f_score/feature_count # Averaging f-score accross total sql_ids for that particular timestep\n",
    "                print('Test Accuracy ' + y_labels[i] + ' with LAG value [' + str(counter) + ']: ' +  str(acc_score))\n",
    "                print('Test FScore ' + y_labels[i] + ' with LAG value [' + str(counter) + ']: ' +  str(f_score) + \"\\n\")\n",
    "                counter, acc_score, f_score = 0,.0,.0\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, estimator, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'estimator', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'estimator': estimator,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_labels = y_df.columns\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "#\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "#\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "#\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)) + \"\\n------------------------------\")\n",
    "#\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate[0:5])\n",
    "print(y_validate[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "#\n",
    "# Deletes pandas frames to conserve RAM space.\n",
    "del X_df, y_df \n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "print('Training + Validation')\n",
    "model = RandomForest(n_estimators=n_estimators,\n",
    "                     max_depth=max_depth,\n",
    "                     criterion=criterion,\n",
    "                     parallel_degree=parallel_degree)\n",
    "model.fit_model(X=X_train,\n",
    "                y=y_train)\n",
    "model.predict_and_evaluate(X=X_validate,\n",
    "                           y=y_validate,\n",
    "                           y_labels=y_labels,\n",
    "                           lag=lag,\n",
    "                           plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "print('\\n\\nTraining + Testing')\n",
    "model.fit_model(X=X_validate,\n",
    "                y=y_validate)\n",
    "model.predict_and_evaluate(X=X_test,\n",
    "                           y=y_test,\n",
    "                           y_labels=y_labels,\n",
    "                           lag=lag,\n",
    "                           plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
