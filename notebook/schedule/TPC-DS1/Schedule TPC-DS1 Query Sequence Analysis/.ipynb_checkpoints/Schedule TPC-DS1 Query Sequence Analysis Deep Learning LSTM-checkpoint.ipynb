{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis (Deep Learning LSTM)\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database. In particular, the usage of LSTMs will be implemented, so as to take into account the temporal nature of the underlying data.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Module Installation and Importing Libraries\n",
    "\n",
    "* https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/\n",
    "* https://vertexai-plaidml.readthedocs-hosted.com/en/latest/installing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.4\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.20.2\n",
      "theano: 1.0.3\n",
      "tensorflow: 1.11.0\n",
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# theano\n",
    "import theano\n",
    "print('theano: %s' % theano.__version__)\n",
    "# tensorflow\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "# plaidml keras\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "# keras\n",
    "import keras as ke\n",
    "print('keras: %s' % ke.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=13 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "\n",
    "nrows=40000\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "\n",
    "# Net Config\n",
    "epochs=10\n",
    "batch_size=32\n",
    "activation='selu'\n",
    "dropout=.2\n",
    "layers=2\n",
    "initializer='zero'\n",
    "state=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    }
   ],
   "source": [
    "# Root path\n",
    "#root_dir = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds\n",
    "root_dir = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds\n",
    "\n",
    "# Open Data\n",
    "#rep_hist_snapshot_path = root_dir + '/rep_hist_snapshot.csv'\n",
    "rep_hist_snapshot_path = root_dir + '/rep_hist_snapshot.csv'\n",
    "\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path, nrows=nrows)\n",
    "\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (40000, 90)\n",
      "Shape After Aggregation: (477, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     43414  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     43415  [0w26sk6t6gq98, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "2     43416  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "3     43417  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "4     43418  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "5     43419  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "6     43420  [0a4un42k35fzy, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "7     43421  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "8     43422  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "9     43423  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "10    43424  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1hxfbnas8xr2j, ...\n",
      "11    43425  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "12    43426  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    43427  [01tp87bk1t2zv, 0w26sk6t6gq98, 130r442w3nfny, ...\n",
      "14    43428  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "15    43429  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "16    43430  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "17    43431  [0kcbwucxmazcp, 0w26sk6t6gq98, 130r442w3nfny, ...\n",
      "18    43432  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "19    43433  [01tp87bk1t2zv, 06dymzb481vnd, 0kkhhb2w93cx0, ...\n",
      "20    43434  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "21    43435  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "22    43436  [01tp87bk1t2zv, 0jj0ct4x4gy27, 0kcbwucxmazcp, ...\n",
      "23    43437  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0v3dvmc22qnam, ...\n",
      "24    43438  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "25    43439  [01tp87bk1t2zv, 06dymzb481vnd, 130r442w3nfny, ...\n",
      "26    43440  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "27    43441  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "28    43442  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "29    43443  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "..      ...                                                ...\n",
      "70    43484  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "71    43485  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "72    43486  [01tp87bk1t2zv, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "73    43487  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "74    43488  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "75    43489  [01tp87bk1t2zv, 06dymzb481vnd, 130r442w3nfny, ...\n",
      "76    43490  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "77    43491  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "78    43492  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "79    43493  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "80    43494  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    43495  [06dymzb481vnd, 0kkhhb2w93cx0, 130r442w3nfny, ...\n",
      "82    43496  [01tp87bk1t2zv, 06dymzb481vnd, 0v3dvmc22qnam, ...\n",
      "83    43497  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "84    43498  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "85    43499  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "86    43500  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "87    43501  [06dymzb481vnd, 0ga8vk4nftz45, 13a9r2xkx1bxb, ...\n",
      "88    43502  [062savj8zgzut, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "89    43503  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "90    43504  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "91    43505  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "92    43506  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "93    43507  [01fpx3aqyq6wp, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "94    43508  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "95    43509  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "96    43510  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "97    43511  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0v3dvmc22qnam, ...\n",
      "98    43512  [0a7q9v9nd2qc1, 0hhmdwwgxbw0r, 0kkhhb2w93cx0, ...\n",
      "99    43513  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "df = rep_hist_snapshot_df.groupby(['SNAP_ID'])['SQL_ID'].apply(list).reset_index()\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(df.shape))\n",
    "print(type(df))\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(477, 2)\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     43414  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     43415  [0w26sk6t6gq98, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "2     43416  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "3     43417  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "4     43418  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "5     43419  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "6     43420  [0a4un42k35fzy, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "7     43421  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "8     43422  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "9     43423  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "10    43424  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1hxfbnas8xr2j, ...\n",
      "11    43425  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "12    43426  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    43427  [01tp87bk1t2zv, 0w26sk6t6gq98, 130r442w3nfny, ...\n",
      "14    43428  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "15    43429  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "16    43430  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "17    43431  [0kcbwucxmazcp, 0w26sk6t6gq98, 130r442w3nfny, ...\n",
      "18    43432  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "19    43433  [01tp87bk1t2zv, 06dymzb481vnd, 0kkhhb2w93cx0, ...\n",
      "20    43434  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "21    43435  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "22    43436  [01tp87bk1t2zv, 0jj0ct4x4gy27, 0kcbwucxmazcp, ...\n",
      "23    43437  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0v3dvmc22qnam, ...\n",
      "24    43438  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "25    43439  [01tp87bk1t2zv, 06dymzb481vnd, 130r442w3nfny, ...\n",
      "26    43440  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "27    43441  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "28    43442  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "29    43443  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "..      ...                                                ...\n",
      "70    43484  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "71    43485  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "72    43486  [01tp87bk1t2zv, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "73    43487  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "74    43488  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "75    43489  [01tp87bk1t2zv, 06dymzb481vnd, 130r442w3nfny, ...\n",
      "76    43490  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "77    43491  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "78    43492  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "79    43493  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "80    43494  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    43495  [06dymzb481vnd, 0kkhhb2w93cx0, 130r442w3nfny, ...\n",
      "82    43496  [01tp87bk1t2zv, 06dymzb481vnd, 0v3dvmc22qnam, ...\n",
      "83    43497  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "84    43498  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "85    43499  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "86    43500  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "87    43501  [06dymzb481vnd, 0ga8vk4nftz45, 13a9r2xkx1bxb, ...\n",
      "88    43502  [062savj8zgzut, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "89    43503  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "90    43504  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "91    43505  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "92    43506  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "93    43507  [01fpx3aqyq6wp, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "94    43508  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "95    43509  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "96    43510  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "97    43511  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0v3dvmc22qnam, ...\n",
      "98    43512  [0a7q9v9nd2qc1, 0hhmdwwgxbw0r, 0kkhhb2w93cx0, ...\n",
      "99    43513  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.sort_index(ascending=True,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection\n",
    "\n",
    "This sextion treats the dataset as a univariate dataset. Therefore the SNAP_ID pertaining to each set of SQL_IDs is removed, with the intent of future classifiers training solely on past SQL executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(477, 2)\n",
      "(477, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "del df['SNAP_ID']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \"\"\"\n",
    "    Scikit Label Encoder was acting up with the following error whilst using the transform function, even though I tripled \n",
    "    checked that the passed data was exactly the same as the one used for training:\n",
    "    \n",
    "    * https://stackoverflow.com/questions/46288517/getting-valueerror-y-contains-new-labels-when-using-scikit-learns-labelencoder\n",
    "    \n",
    "    So I have rebuilt a similar functionality to categorize my data into numeric digits, as the LabelEncoder is supposed to do.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__class_map = {}\n",
    "        self.__integer_counter = 0\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        :param - X: python list\n",
    "        \"\"\"\n",
    "        for val in X:\n",
    "            if val not in self.__class_map:\n",
    "                self.__class_map[val] = self.__integer_counter\n",
    "                self.__integer_counter += 1\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        param - X: python list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for val in X:\n",
    "            if val in self.__class_map:\n",
    "                value = self.__class_map[val]\n",
    "                encoded_map.append(value)\n",
    "            else:\n",
    "                raise ValueError('Label Mismatch - Encountered a label which was not trained on.')\n",
    "        return encoded_map\n",
    "    \n",
    "    def get_class_map(self):\n",
    "        \"\"\"\n",
    "        Returns original classes as a list\n",
    "        \"\"\"\n",
    "        class_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            class_map.append(key)\n",
    "        return class_map\n",
    "    \n",
    "    def get_encoded_map(self):\n",
    "        \"\"\"\n",
    "        Returns class encodings as a list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            encoded_map.append(value)\n",
    "        return encoded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(477, 1)\n",
      "                                              SQL_ID\n",
      "0  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1  [0w26sk6t6gq98, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "2  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "3  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "4  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "5  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "6  [0a4un42k35fzy, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "7  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "8  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "9  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "Total SQL_ID Classes: 665\n",
      "['03ggjrmy0wa1w', '06dymzb481vnd', '0aq14dznn91rg', '0f60bzgt9127c', '0ga8vk4nftz45', '13a9r2xkx1bxb', '13ys8ux8xvrbm', '14f5ngrj3cc5h', '1jhyrdp21f2q6', '1pv23p59mjs0v']\n",
      "(477, 1)\n",
      "                                              SQL_ID\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
      "1  [67, 68, 68, 68, 69, 70, 71, 72, 73, 74, 74, 7...\n",
      "2  [127, 1, 128, 68, 68, 68, 69, 70, 71, 72, 74, ...\n",
      "3  [127, 1, 141, 141, 68, 68, 68, 69, 142, 70, 71...\n",
      "4  [127, 1, 68, 68, 68, 69, 70, 71, 148, 72, 149,...\n",
      "5  [164, 165, 67, 166, 148, 167, 168, 169, 170, 1...\n",
      "6  [209, 210, 211, 165, 67, 212, 166, 148, 168, 2...\n",
      "7  [127, 1, 128, 68, 68, 68, 69, 70, 71, 72, 74, ...\n",
      "8  [127, 1, 141, 141, 240, 68, 68, 68, 69, 70, 71...\n",
      "9  [127, 1, 68, 68, 68, 69, 70, 71, 72, 149, 74, ...\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "le = LabelEncoder()\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    le.fit(sql_id_list)\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    transformed_list = le.transform(sql_id_list)\n",
    "    df['SQL_ID'].iloc[index] = transformed_list \n",
    "\n",
    "print(\"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "print('Total SQL_ID Classes: ' + str(len(le.get_class_map())))\n",
    "print(le.get_class_map()[:10])\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "A note regarding normalization. Normalization for this experiment was purposely skipped, since training & testing data will be one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Padding\n",
    "\n",
    "Since there isn't a fixed number of SQL_ID's per SNAP_ID, each set of SQL_IDs need to be padded so as to assume an equal number if SQL_IDs for the purpose of model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length at index 0: 82\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 38, 38, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 50, 50, 51, 52, 53, 53, 53, 53, 54, 54, 54, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 63, 63, 63, 64, 65, 66]\n",
      "Length at index 1: 74\n",
      "[67, 68, 68, 68, 69, 70, 71, 72, 73, 74, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 83, 84, 85, 86, 87, 88, 88, 89, 90, 90, 91, 92, 93, 93, 94, 95, 32, 96, 97, 98, 99, 100, 39, 101, 102, 103, 104, 105, 105, 106, 107, 48, 108, 109, 110, 111, 111, 112, 113, 114, 55, 115, 116, 117, 118, 119, 120, 120, 121, 122, 123, 124, 125, 126]\n",
      "Length at index 2: 91\n",
      "[127, 1, 128, 68, 68, 68, 69, 70, 71, 72, 74, 74, 75, 79, 81, 83, 83, 84, 86, 87, 88, 88, 89, 90, 90, 91, 92, 94, 95, 129, 28, 28, 130, 131, 32, 97, 98, 132, 133, 99, 134, 38, 38, 38, 38, 39, 135, 136, 137, 104, 105, 105, 106, 48, 108, 109, 50, 50, 50, 138, 111, 111, 112, 139, 113, 53, 53, 53, 53, 114, 54, 54, 54, 54, 55, 140, 140, 140, 115, 116, 58, 119, 120, 120, 123, 124, 125, 63, 63, 63, 63]\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "Length at index 0: 114\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 28 29 30 31 32 33 34 35 36 37 38 38 38 38 39 40 41 42 43\n",
      " 44 45 46 47 48 49 50 50 50 51 52 53 53 53 53 54 54 54 54 55 56 57 58 59\n",
      " 60 61 62 63 63 63 63 64 65 66 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Length at index 1: 114\n",
      "[ 67  68  68  68  69  70  71  72  73  74  74  75  76  77  78  79  80  81\n",
      "  82  83  83  84  85  86  87  88  88  89  90  90  91  92  93  93  94  95\n",
      "  32  96  97  98  99 100  39 101 102 103 104 105 105 106 107  48 108 109\n",
      " 110 111 111 112 113 114  55 115 116 117 118 119 120 120 121 122 123 124\n",
      " 125 126  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1  -1  -1  -1  -1  -1]\n",
      "Length at index 2: 114\n",
      "[127   1 128  68  68  68  69  70  71  72  74  74  75  79  81  83  83  84\n",
      "  86  87  88  88  89  90  90  91  92  94  95 129  28  28 130 131  32  97\n",
      "  98 132 133  99 134  38  38  38  38  39 135 136 137 104 105 105 106  48\n",
      " 108 109  50  50  50 138 111 111 112 139 113  53  53  53  53 114  54  54\n",
      "  54  54  55 140 140 140 115 116  58 119 120 120 123 124 125  63  63  63\n",
      "  63  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1  -1  -1  -1  -1  -1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])\n",
    "\n",
    "# Retrieve largest length\n",
    "def pad_datamatrix(df):\n",
    "    \"\"\"\n",
    "    Iterates over dataframe and pads SQL_ID lists accordingly with -1 values, denoting empty SQL_ID slots.\n",
    "    \"\"\"\n",
    "    row_sizes = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_sizes.append(len(row['SQL_ID']))\n",
    "    max_row_size = max(row_sizes)\n",
    "    \n",
    "    # Pad Dataframe Values\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        length = len(row['SQL_ID'])\n",
    "        diff = max_row_size - length\n",
    "        if diff != 0:\n",
    "            for j in range(length, max_row_size):\n",
    "                df['SQL_ID'].iloc[i] = np.append(df['SQL_ID'].iloc[i], -1) # Appends -1 to padded values\n",
    "        # print(\"Length at index \" + str(i) + \": \" + str(df['SQL_ID'].iloc[i].size))\n",
    "        i += 1\n",
    "    return df\n",
    "\n",
    "df = pad_datamatrix(df)\n",
    "\n",
    "print('\\n\\n------------------------------------------\\n\\n')\n",
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Feature Lists\n",
    "\n",
    "Expand Feature Lists, where in each list element is represented as it's own features. Total feature count here equates as follows:\n",
    "\n",
    "* Features = (lag * SQL_ID per SNAP_ID count) + SQL_ID per SNAP_ID count\n",
    "* Labels = lag * SQL_ID per SNAP_ID count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "Before: (477, 1)\n",
      "After: (477, 114)\n"
     ]
    }
   ],
   "source": [
    "def sequence2features(df):\n",
    "    \"\"\"\n",
    "    Converts pandas sequences into full fledged columns/features\n",
    "    \"\"\"\n",
    "    feature_count = len(df[df.columns[0]].iloc[0])\n",
    "    for column_name in df.columns:\n",
    "        data_matrix = []\n",
    "        new_values = df[column_name].values\n",
    "        \n",
    "        new_values = np.stack(new_values, axis=0 )\n",
    "        \n",
    "        for i in range(1,feature_count+1):\n",
    "            new_column_name = column_name + \"_\"+str(i)\n",
    "            df[new_column_name] = new_values[:,i-1]\n",
    "        \n",
    "        # Drop original list columns\n",
    "        df.drop(column_name, inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "print('Features')\n",
    "print('Before: ' + str(df.shape))\n",
    "df = sequence2features(df=df)\n",
    "print('After: ' + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "One hot encoding target labels for deep learning application\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder:\n",
    "    \n",
    "    def __init__(self, classes):\n",
    "        self.__mapper = pd.DataFrame(columns=classes)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        class_types = self.__mapper.columns\n",
    "        for row in X:\n",
    "            temp_row = []\n",
    "            for i in range(len(class_types)):\n",
    "                if class_types[i] in row:\n",
    "                    temp_row.append(float(1))\n",
    "                else:\n",
    "                    temp_row.append(float(0))\n",
    "            self.__mapper.loc[len(self.__mapper)] = temp_row\n",
    "        return self.__mapper\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.__mapper.columns\n",
    "    \n",
    "    def get_unique_values(self):\n",
    "        return np.unique(self.__mapper.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Before One Hot Encoding: (477, 114)\n",
      "After One Hot Encoding: (477, 665)\n",
      "     0    1    2    3    4    5    6    7    8    9   ...   655  656  657  \\\n",
      "0    1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0 ...   0.0  0.0  0.0   \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "2    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "3    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "4    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "5    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "6    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "7    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "8    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "9    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "10   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "11   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "12   1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0 ...   0.0  0.0  0.0   \n",
      "13   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "14   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "15   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "16   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "17   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "18   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "19   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "20   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "21   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "22   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "23   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "24   1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0 ...   0.0  0.0  0.0   \n",
      "25   0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "26   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "27   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "28   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "29   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ... ...   ...  ...  ...   \n",
      "447  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0 ...   0.0  0.0  0.0   \n",
      "448  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "449  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "450  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "451  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "452  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "453  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "454  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "455  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "456  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "457  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "458  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "459  1.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0 ...   0.0  0.0  0.0   \n",
      "460  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "461  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "462  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "463  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "464  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "465  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "466  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "467  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "468  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "469  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "470  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "471  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0 ...   0.0  0.0  0.0   \n",
      "472  0.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "473  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   1.0  0.0  0.0   \n",
      "474  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "475  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "476  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "\n",
      "     658  659  660  661  662  663  664  \n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "5    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "6    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "7    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "8    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "9    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "10   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "12   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "13   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "14   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "15   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "16   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "17   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "18   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "19   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "20   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "21   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "22   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "23   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "24   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "25   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "26   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "27   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "28   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "29   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  \n",
      "447  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "448  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "449  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "450  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "451  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "452  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "453  0.0  1.0  1.0  0.0  0.0  0.0  0.0  \n",
      "454  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "455  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "456  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "457  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "458  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "459  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "460  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "461  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "462  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "463  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "464  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "465  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "466  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "467  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "468  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "469  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "470  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "471  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "472  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "473  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "474  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "475  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "476  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[477 rows x 665 columns]\n",
      "Value type: [0. 1.]\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding train data\n",
    "ohe = OneHotEncoder(classes=le.get_encoded_map())\n",
    "print('Training Data:')\n",
    "print(\"Before One Hot Encoding: \" + str(df.shape))\n",
    "df = ohe.fit_transform(X=df.values)\n",
    "print(\"After One Hot Encoding: \" + str(df.shape))\n",
    "print(df)\n",
    "print('Value type: ' + str(ohe.get_unique_values()))\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-13)', 'var2(t-13)', 'var3(t-13)', 'var4(t-13)', 'var5(t-13)',\n",
      "       'var6(t-13)', 'var7(t-13)', 'var8(t-13)', 'var9(t-13)', 'var10(t-13)',\n",
      "       ...\n",
      "       'var656(t)', 'var657(t)', 'var658(t)', 'var659(t)', 'var660(t)',\n",
      "       'var661(t)', 'var662(t)', 'var663(t)', 'var664(t)', 'var665(t)'],\n",
      "      dtype='object', length=9310)\n",
      "(451, 9310)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var2(t+1)', 'var3(t+1)', 'var4(t+1)', 'var5(t+1)',\n",
      "       'var6(t+1)', 'var7(t+1)', 'var8(t+1)', 'var9(t+1)', 'var10(t+1)',\n",
      "       ...\n",
      "       'var656(t+13)', 'var657(t+13)', 'var658(t+13)', 'var659(t+13)',\n",
      "       'var660(t+13)', 'var661(t+13)', 'var662(t+13)', 'var663(t+13)',\n",
      "       'var664(t+13)', 'var665(t+13)'],\n",
      "      dtype='object', length=8645)\n",
      "(451, 8645)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "\n",
    "# Seperate labels from features\n",
    "x_columns, y_columns = [], []\n",
    "for col in shifted_df.columns:\n",
    "    if '+' in col:\n",
    "        y_columns.append(col)\n",
    "    else:\n",
    "        x_columns.append(col)\n",
    "\n",
    "y_df = shifted_df[y_columns]\n",
    "X_df = shifted_df[x_columns]\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print(type(X_df))\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "print(type(y_df))\n",
    "\n",
    "# # Delete middle timesteps\n",
    "# X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "# print('\\n-------------\\nFeatures After Time Shift')\n",
    "# print(X_df.columns)\n",
    "# print(X_df.shape)\n",
    "# print(type(X_df))\n",
    "# # y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "# print('\\n-------------\\nLabels After Time Shift')\n",
    "# print(y_df.columns)\n",
    "# print(y_df.shape)\n",
    "# print(type(y_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model (LSTM)\n",
    "\n",
    "### Designing the network\n",
    "\n",
    "The Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.\n",
    "\n",
    "There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.\n",
    "\n",
    "The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our SQL_ID dataset, the output layer must create n output values, one for each SQL_ID class. The output value with the largest value will be taken as the class predicted by the model.\n",
    "\n",
    "The network topology of this simple one-layer neural network can be summarized as:\n",
    "\n",
    "* n_inputs = Number of input neurons dependent on SQL_ID Features (Vector Width)\n",
    "* n_hidden = Number of hidden layer neurons (This will almost be the same as n_inputs, pending testing)\n",
    "* n_output = Number of output neurons dependent on SQL_ID Classes.\n",
    "\n",
    "n_inputs -> [n_hidden] -> n_output\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8141873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Class\n",
    "class LSTM:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, lag, loss_func, activation, optimizer='sgd', lstm_layers=1, dropout=.0,\n",
    "                 stateful=False, initializer='uniform'):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X             - (Numpy 2D Array) Training data used to train the model (Features).\n",
    "        :param y             - (Numpy 2D Array) Test data used to test the model (Labels\n",
    "        :param lag           - (Integer) Denotes lag step value\n",
    "        :param loss_function - (String)  Denotes mode of measure fitting of model (Fitting function).\n",
    "        :param activation    - (String)  Neuron activation function used to activate/trigger neurons.\n",
    "        :param optimizer     - (String)  Denotes which function to us to optimize the model build (eg: Gradient Descent).\n",
    "        :param lstm_layers   - (Integer) Denotes the number of LSTM layers to be included in the model build.\n",
    "        :param dropout       - (Float)   Denotes amount of dropout for model. This parameter must be a value between 0 and 1.\n",
    "        :param stateful      - (Boolean) Denotes whether state is used as initial state for next training batch.\n",
    "        :param: initializer  - (String)  String initializer which denotes starting weights.\n",
    "        \"\"\"\n",
    "        self.__lag = lag\n",
    "        self.__model = ke.models.Sequential()\n",
    "\n",
    "        if dropout > 1 and dropout < 0:\n",
    "            raise ValueError('Dropout parameter exceeded! Must be a value between 0 and 1.')\n",
    "\n",
    "        for i in range(0, lstm_layers - 1):  # If lstm_layers == 1, this for loop logic is skipped.\n",
    "            if stateful:\n",
    "                if i == 0:\n",
    "                    self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                    batch_input_shape=(X.shape[0],\n",
    "                                                                       X.shape[1],\n",
    "                                                                       X.shape[2]),\n",
    "                                                    return_sequences=True,\n",
    "                                                    stateful=stateful))\n",
    "                else:\n",
    "                    self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                    input_shape=(X.shape[1],\n",
    "                                                                 X.shape[2]),\n",
    "                                                    return_sequences=True,\n",
    "                                                    stateful=stateful))\n",
    "            else:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                input_shape=(X.shape[1],\n",
    "                                                             X.shape[2]),\n",
    "                                                return_sequences=True,\n",
    "                                                stateful=stateful))\n",
    "            self.__model.add(ke.layers.Dropout(dropout))\n",
    "        if lstm_layers > 1:\n",
    "            self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                            input_shape=(X.shape[1],\n",
    "                                                         X.shape[2]),\n",
    "                                            stateful=stateful,\n",
    "                                            return_sequences=False))\n",
    "        else:\n",
    "            if stateful:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                batch_input_shape=(X.shape[0],\n",
    "                                                                   X.shape[1],\n",
    "                                                                   X.shape[2]),\n",
    "                                                stateful=stateful,\n",
    "                                                return_sequences=False))\n",
    "            else:\n",
    "                self.__model.add(ke.layers.LSTM(X.shape[2],\n",
    "                                                input_shape=(X.shape[1],\n",
    "                                                             X.shape[2]),\n",
    "                                                stateful=stateful,\n",
    "                                                return_sequences=False))\n",
    "        self.__model.add(ke.layers.Dropout(dropout))\n",
    "        self.__model.add(ke.layers.Dense(y.shape[1],\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation=activation))\n",
    "        self.__model.compile(loss=loss_func, optimizer=optimizer, metrics=['acc'])\n",
    "        print(self.__model.summary())\n",
    "\n",
    "    def fit_model(self, X_train=None, X_test=None, y_train=None, y_test=None, epochs=50, batch_size=50, verbose=2,\n",
    "                  shuffle=False, plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "\n",
    "        :param: X_train    - (Numpy 2D Array) Numpy matrix consisting of input training features\n",
    "        :param: X_test     - (Numpy 2D Array) Numpy matrix consisting of input validation/testing features\n",
    "        :param: y_train    - (Numpy 2D Array) Numpy matrix consisting of output training labels\n",
    "        :param: y_test     - (Numpy 2D Array) Numpy matrix consisting of output validation/testing labels\n",
    "        :param: epochs     - (Integer) Integer value denoting number of trained epochs\n",
    "        :param: batch_size - (Integer) Integer value denoting LSTM training batch_size\n",
    "        :param: verbose    - (Integer) Integer value denoting net verbosity (Amount of information shown to user during LSTM training)\n",
    "        :param: shuffle    - (Bool) Boolean value denoting whether or not to shuffle data. This parameter must always remain 'False' for time series datasets.\n",
    "        :param: plot       - (Bool) Boolean value denoting whether this function should plot out it's evaluation\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if X_test is not None and y_test is not None:\n",
    "            history = self.__model.fit(x=X_train,\n",
    "                                       y=y_train,\n",
    "                                       epochs=epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       validation_data=(X_test, y_test),\n",
    "                                       verbose=verbose,\n",
    "                                       shuffle=shuffle)\n",
    "        else:\n",
    "            history = self.__model.fit(x=X_train,\n",
    "                                       y=y_train,\n",
    "                                       epochs=epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       verbose=verbose,\n",
    "                                       shuffle=shuffle)\n",
    "\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(history.history['acc'], label='train')\n",
    "            plt.plot(history.history['val_acc'], label='validation')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, X, batch_size):\n",
    "        \"\"\"\n",
    "        Predicts label/s from input feature 'X'\n",
    "        :param: X - Numpy matrix consisting of a single feature vector\n",
    "        :param: batch_size - (Integer) Denotes prediction batch size\n",
    "        :return: Numpy matrix of predicted label output\n",
    "        \"\"\"\n",
    "        yhat = self.__model.predict(X, batch_size=batch_size)\n",
    "        return yhat\n",
    "\n",
    "    def evaluate(self, y, yhat, plot=False):\n",
    "        \"\"\"\n",
    "        Receives 2D matrix of input features and 2D matrix of output labels, and evaluates input data and target predictions.\n",
    "        :param: y    - Numpy array consisting of output label vectors (Test Set)\n",
    "        :param: yhat - Numpy array consisting of output label vectors (Prediction Set)\n",
    "        :param: plot     - (Bool) Boolean value denoting whether this function should plot out it's evaluation\n",
    "        :return: None\n",
    "        \"\"\"                \n",
    "        y = y.flatten()\n",
    "        yhat = yhat.flatten()\n",
    "\n",
    "        # F1-Score Evaluation\n",
    "        print(y)\n",
    "        print(yhat)\n",
    "        accuracy = accuracy_score(y, yhat)\n",
    "        f1 = f1_score(y,\n",
    "                      yhat,\n",
    "                      average='macro')  # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "        print('Accuracy [' + str(accuracy) + ']')\n",
    "        print('FScore [' + str(f1) + ']')\n",
    "\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(y, label='actual')\n",
    "            plt.plot(yhat, label='predicted')\n",
    "            plt.legend(['actual', 'predicted'], loc='upper left')\n",
    "            plt.title('Actual vs Predicted')\n",
    "            plt.show()\n",
    "        else:\n",
    "            return accuracy, f1\n",
    "\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, batch, dropout, epoch, layer, activation, initializer,\n",
    "                              stateful, rmse, accuracy, f_score, time_train):\n",
    "        \"\"\"\n",
    "        Static method which is used for test harness utilities. This method attempts a grid search across many\n",
    "        trained LSTM models, each denoted with different configurations.\n",
    "\n",
    "        Attempted configurations:\n",
    "        * Varied data test split\n",
    "        * Varied batch sizes\n",
    "        * Varied epoch counts\n",
    "\n",
    "        Each configuration is denoted with a score, and used to identify the most optimal configuration.\n",
    "\n",
    "        :param: path       - (String) String denoting result csv output.\n",
    "        :param: iteration  - (Integer) Integer denoting test iteration (Unique per test configuration).\n",
    "        :param: lag        - (Integer) Denotes lag time shift\n",
    "        :param: test_split - (Float) Float denoting data sample sizes.\n",
    "        :param: batch      - (Integer) Integer denoting LSTM batch size.\n",
    "        :param: epoch      - (Integer) Integer denoting number of LSTM training iterations.\n",
    "        :param: layer      - (Integer) Integer denoting number of LSTM layers\n",
    "        :param: activation - (String) String denoting activation for LSTM layers.\n",
    "        :param: initializer- (String) String denoting LSTM initializing weights.\n",
    "        :param: stateful   - (Bool) Boolean flag which denotes whether LSTM model is trained in stateful mode or not.\n",
    "        :param: dropout    - (Float) Float denoting model dropout layer.\n",
    "        :param: rmse       - (Float) Float denoting experiment configuration RSME score.\n",
    "        :param: accuracy   - (Float) Float denoting experiment accuracy score.\n",
    "        :param: fscore     - (Float) Float denoting experiment fscore score.\n",
    "        :param: time_train - (Integer) Integer denoting number of seconds taken by LSTM training iteration.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a+') as csvfile:\n",
    "            headers = ['iteration', 'test_split', 'batch', 'epoch', 'layer', 'stateful', 'dropout', 'activation',\n",
    "                       'initializer', 'rmse', 'accuracy', 'f_score', 'time_train', 'lag']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'test_split': test_split,\n",
    "                             'batch': batch,\n",
    "                             'epoch': epoch,\n",
    "                             'layer': layer,\n",
    "                             'stateful': stateful,\n",
    "                             'dropout': dropout,\n",
    "                             'activation': activation,\n",
    "                             'initializer': initializer,\n",
    "                             'rmse': rmse,\n",
    "                             'accuracy': accuracy,\n",
    "                             'f_score': f_score,\n",
    "                             'time_train': time_train,\n",
    "                             'lag': lag})\n",
    "\n",
    "    @staticmethod\n",
    "    def lag_multiple(X, lag):\n",
    "        \"\"\"\n",
    "        Divides the total number of rows by the lag value, until a perfect multiple amount is retrieved.\n",
    "        :param X: (Numpy) 2D array consisting of input.\n",
    "        :param lag: (Integer) Denotes time shift value.\n",
    "        :return: (Numpy) 2D array consisting of a perfect lag multiple rows.\n",
    "        \"\"\"\n",
    "        n_rows = X.shape[0]\n",
    "        multiple = int(n_rows/lag)\n",
    "        max_new_rows = multiple * lag\n",
    "        return X[0:max_new_rows, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reshaping Training Frames\n",
      "X_train shape [(27, 13, 9310)] Type - <class 'numpy.ndarray'>\n",
      "y_train shape [(27, 8645)] Type - <class 'numpy.ndarray'>\n",
      "X_validate shape [(3, 13, 9310)] Type - <class 'numpy.ndarray'>\n",
      "y_validate shape [(3, 8645)] Type - <class 'numpy.ndarray'>\n",
      "X_test shape [(3, 13, 9310)] Type - <class 'numpy.ndarray'>\n",
      "y_test shape [(3, 8645)] Type - <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_tonga.0\"\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-0fdb12f424d3>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X, y, lag, loss_func, activation, optimizer, lstm_layers, dropout, stateful, initializer)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                                          X.shape[2]),\n\u001b[0;32m     54\u001b[0m                                             \u001b[0mstateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                             return_sequences=False))\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    491\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;31m# set or validate state_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1872\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m             \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m             constraint=self.recurrent_constraint)\n\u001b[0m\u001b[0;32m   1875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[0;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[1;31m# Pick the one with the correct shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mflat_shape\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[0;32m   1560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m         \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->DdD'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->ddd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1562\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Lag Multiples\n",
    "X_train = LSTM.lag_multiple(X=X_train, lag=lag)\n",
    "y_train = LSTM.lag_multiple(X=y_train, lag=lag)\n",
    "X_validate = LSTM.lag_multiple(X=X_validate, lag=lag)\n",
    "y_validate = LSTM.lag_multiple(X=y_validate, lag=lag)\n",
    "X_test = LSTM.lag_multiple(X=X_test, lag=lag)\n",
    "y_test = LSTM.lag_multiple(X=y_test, lag=lag)\n",
    "\n",
    "# Reshape for fitting in LSTM\n",
    "X_train = X_train.reshape((int(X_train.shape[0] / lag), lag, X_train.shape[1]))\n",
    "y_train = y_train[0:int(y_train.shape[0] / lag),:]\n",
    "X_validate = X_validate.reshape((int(X_validate.shape[0] / lag), lag, X_validate.shape[1]))\n",
    "y_validate = y_validate[0:int(y_validate.shape[0] / lag),:]\n",
    "X_test = X_test.reshape((int(X_test.shape[0] / lag), lag, X_test.shape[1]))\n",
    "y_test = y_test[0:int(y_test.shape[0] / lag),:]                         \n",
    "\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)))\n",
    "\n",
    "# Train on discrete data (Train > Validation)\n",
    "model = LSTM(X=X_train,\n",
    "             y=y_train,\n",
    "             lag=lag,\n",
    "             loss_func='binary_crossentropy',\n",
    "             activation=activation,\n",
    "             optimizer='adam',\n",
    "             lstm_layers=layers,\n",
    "             dropout=dropout,\n",
    "             stateful=state,\n",
    "             initializer=initializer)\n",
    "model.fit_model(X_train=X_train,\n",
    "                X_test=X_validate,\n",
    "                y_train=y_train,\n",
    "                y_test=y_validate,\n",
    "                epochs=epochs, \n",
    "                batch_size=batch,\n",
    "                verbose=2, \n",
    "                shuffle=False,\n",
    "                plot=True)\n",
    "\n",
    "yhat = []\n",
    "for i in range(0, X_validate.shape[0]):\n",
    "    X = np.array(np.array(X_validate[i,:]))\n",
    "    X = X.reshape((int(X.shape[0] / lag), lag, X.shape[1]))\n",
    "    y = model.predict(X, batch_size=batch)\n",
    "    model.fit_model(X_train=X,\n",
    "                    y_train=y,\n",
    "                    epochs=2, \n",
    "                    batch_size=1,\n",
    "                    verbose=1, \n",
    "                    shuffle=False,\n",
    "                    plot=False) # Online Learning, Training on validation predictions. \n",
    "    yhat.extend(y)    \n",
    "model.evaluate(y=y_validate,\n",
    "               yhat=np.array(yhat),\n",
    "               plot=True)\n",
    "\n",
    "yhat = []\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    X = np.array(np.array(X_test[i,:]))\n",
    "    X = X.reshape((int(X.shape[0] / lag), lag, X.shape[1]))\n",
    "    y = model.predict(X, batch_size=batch)\n",
    "    model.fit_model(X_train=X,\n",
    "                    y_train=y,\n",
    "                    epochs=2, \n",
    "                    batch_size=1,\n",
    "                    verbose=1, \n",
    "                    shuffle=False,\n",
    "                    plot=False) # Online Learning, Training on validation predictions. \n",
    "    yhat.extend(y)    \n",
    "model.evaluate(y=y_test,\n",
    "               yhat=np.array(yhat),\n",
    "               plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
