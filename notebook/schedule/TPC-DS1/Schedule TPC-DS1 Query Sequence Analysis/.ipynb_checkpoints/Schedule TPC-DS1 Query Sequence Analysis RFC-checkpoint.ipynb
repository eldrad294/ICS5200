{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis (RFC)\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.20.0\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=3 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "#\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "#\n",
    "# Forest Config\n",
    "parallel_degree = 4\n",
    "n_estimators = 500\n",
    "max_depth = 7\n",
    "criterion='gini'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3018: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "#rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "#\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (64912, 90)\n",
      "Shape After Aggregation: (820, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     28190  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     28191  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2     28192  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     28193  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4     28194  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5     28195  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6     28196  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7     28197  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     28198  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9     28199  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "10    28200  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "11    28201  [06g9mhm5ba7tt, 09vrdx888wvvb, 0kcbwucxmazcp, ...\n",
      "12    28202  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    28203  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "14    28204  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "15    28205  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "16    28206  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "17    28207  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "18    28208  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "19    28209  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "20    28210  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "21    28211  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "22    28212  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "23    28213  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "24    28214  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "25    28215  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "26    28216  [06dymzb481vnd, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "27    28217  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "28    28218  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "29    28219  [06g9mhm5ba7tt, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "..      ...                                                ...\n",
      "70    28260  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "71    28261  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "72    28262  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "73    28263  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "74    28264  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "75    28265  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "76    28266  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "77    28267  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "78    28268  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "79    28269  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "80    28270  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    28271  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "82    28272  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "83    28273  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "84    28274  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "85    28275  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "86    28276  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "87    28277  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "88    28278  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "89    28279  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1p5grz1gs7fjq, ...\n",
      "90    28280  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "91    28281  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "92    28282  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "93    28283  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "94    28284  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "95    28285  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "96    28286  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "97    28287  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "98    28288  [0aadu2kp270dv, 0gyb2wqdu4d9k, 0kkhhb2w93cx0, ...\n",
      "99    28289  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "df = rep_hist_snapshot_df.groupby(['SNAP_ID'])['SQL_ID'].apply(list).reset_index()\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(df.shape))\n",
    "print(type(df))\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 2)\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     28190  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     28191  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2     28192  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     28193  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4     28194  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5     28195  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6     28196  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7     28197  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     28198  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9     28199  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "10    28200  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "11    28201  [06g9mhm5ba7tt, 09vrdx888wvvb, 0kcbwucxmazcp, ...\n",
      "12    28202  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    28203  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "14    28204  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "15    28205  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "16    28206  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "17    28207  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "18    28208  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "19    28209  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "20    28210  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "21    28211  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "22    28212  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "23    28213  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "24    28214  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "25    28215  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "26    28216  [06dymzb481vnd, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "27    28217  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "28    28218  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "29    28219  [06g9mhm5ba7tt, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "..      ...                                                ...\n",
      "70    28260  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "71    28261  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "72    28262  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "73    28263  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "74    28264  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "75    28265  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "76    28266  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "77    28267  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "78    28268  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "79    28269  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "80    28270  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    28271  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "82    28272  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "83    28273  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "84    28274  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "85    28275  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "86    28276  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "87    28277  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "88    28278  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "89    28279  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1p5grz1gs7fjq, ...\n",
      "90    28280  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "91    28281  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "92    28282  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "93    28283  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "94    28284  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "95    28285  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "96    28286  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "97    28287  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "98    28288  [0aadu2kp270dv, 0gyb2wqdu4d9k, 0kkhhb2w93cx0, ...\n",
      "99    28289  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.sort_index(ascending=True,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection\n",
    "\n",
    "This sextion treats the dataset as a univariate dataset. Therefore the SNAP_ID pertaining to each set of SQL_IDs is removed, with the intent of future classifiers training solely on past SQL executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 2)\n",
      "(820, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "del df['SNAP_ID']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "\n",
    "NB: Since this experiment is solely focussed on Random Forest Training, One-Hot Encoding will not be used as recommended below:\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \"\"\"\n",
    "    Scikit Label Encoder was acting up with the following error whilst using the transform function, even though I tripled \n",
    "    checked that the passed data was exactly the same as the one used for training:\n",
    "    \n",
    "    * https://stackoverflow.com/questions/46288517/getting-valueerror-y-contains-new-labels-when-using-scikit-learns-labelencoder\n",
    "    \n",
    "    So I have rebuilt a similar functionality to categorize my data into numeric digits, as the LabelEncoder is supposed to do.\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__class_map = {}\n",
    "        self.__integer_counter = 0\n",
    "    #\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        :param - X: python list\n",
    "        \"\"\"\n",
    "        for val in X:\n",
    "            if val not in self.__class_map:\n",
    "                self.__class_map[val] = self.__integer_counter\n",
    "                self.__integer_counter += 1\n",
    "    #\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        param - X: python list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for val in X:\n",
    "            if val in self.__class_map:\n",
    "                value = self.__class_map[val]\n",
    "                encoded_map.append(value)\n",
    "            else:\n",
    "                raise ValueError('Label Mismatch - Encountered a label which was not trained on.')\n",
    "        return encoded_map\n",
    "    #\n",
    "    def get_class_map(self):\n",
    "        \"\"\"\n",
    "        Returns original classes as a list\n",
    "        \"\"\"\n",
    "        class_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            class_map.append(key)\n",
    "        return class_map\n",
    "    #\n",
    "    def get_encoded_map(self):\n",
    "        \"\"\"\n",
    "        Returns class encodings as a list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            encoded_map.append(value)\n",
    "        return encoded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 1)\n",
      "                                              SQL_ID\n",
      "0  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "Total SQL_ID Classes: 1046\n",
      "['03ggjrmy0wa1w', '06dymzb481vnd', '0aq14dznn91rg', '0f60bzgt9127c', '0ga8vk4nftz45', '13a9r2xkx1bxb', '13ys8ux8xvrbm', '14f5ngrj3cc5h', '1jhyrdp21f2q6', '1p5grz1gs7fjq']\n",
      "(820, 1)\n",
      "                                              SQL_ID\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
      "1  [71, 72, 72, 73, 73, 73, 74, 75, 76, 77, 78, 9...\n",
      "2  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "3  [134, 134, 134, 1, 72, 72, 73, 73, 73, 135, 75...\n",
      "4  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "5  [167, 168, 169, 170, 9, 171, 172, 173, 174, 17...\n",
      "6  [167, 168, 169, 170, 215, 9, 216, 217, 171, 21...\n",
      "7  [134, 134, 134, 1, 169, 243, 73, 73, 73, 135, ...\n",
      "8  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "9  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "le = LabelEncoder()\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    le.fit(sql_id_list)\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    transformed_list = le.transform(sql_id_list)\n",
    "    df['SQL_ID'].iloc[index] = transformed_list \n",
    "#\n",
    "print(\"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "print('Total SQL_ID Classes: ' + str(len(le.get_class_map())))\n",
    "print(le.get_class_map()[:10])\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "A note regarding normalization. Normalization for this experiment was purposely skipped, since value dimensionality & size is not as important for RandomForest based models. The purity split  does not benefit greatly from such a process:\n",
    "\n",
    "* https://stats.stackexchange.com/questions/57010/is-it-essential-to-do-normalization-for-svm-and-random-forest\n",
    "* https://stackoverflow.com/questions/8961586/do-i-need-to-normalize-or-scale-data-for-randomforest-r-package\n",
    "* https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-8-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Padding\n",
    "\n",
    "Since there isn't a fixed number of SQL_ID's per SNAP_ID, each set of SQL_IDs need to be padded so as to assume an equal number if SQL_IDs for the purpose of model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length at index 0: 80\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 52, 52, 52, 52, 53, 54, 55, 55, 55, 56, 57, 58, 59, 59, 59, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
      "Length at index 1: 81\n",
      "[71, 72, 72, 73, 73, 73, 74, 75, 76, 77, 78, 9, 79, 80, 81, 82, 83, 84, 85, 86, 24, 87, 88, 88, 89, 90, 91, 92, 93, 93, 94, 95, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 111, 53, 112, 113, 114, 115, 115, 116, 117, 118, 119, 119, 119, 119, 120, 120, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133]\n",
      "Length at index 2: 91\n",
      "[134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78, 9, 79, 136, 136, 83, 85, 24, 88, 88, 89, 91, 92, 93, 93, 94, 95, 95, 96, 97, 100, 101, 137, 138, 139, 139, 139, 102, 103, 140, 141, 104, 105, 142, 143, 144, 107, 108, 108, 109, 110, 52, 52, 52, 52, 52, 53, 112, 113, 55, 55, 55, 115, 115, 116, 145, 117, 59, 59, 59, 59, 118, 119, 119, 119, 119, 120, 120, 120, 121, 122, 63, 126, 126, 128, 129, 146, 130, 132, 132, 147]\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "Length at index 0: 109\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 52 52 52 52 53 54 55 55 55 56 57 58 59 59 59 59 60 61 62\n",
      " 63 64 65 66 67 68 69 70 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Length at index 1: 109\n",
      "[ 71  72  72  73  73  73  74  75  76  77  78   9  79  80  81  82  83  84\n",
      "  85  86  24  87  88  88  89  90  91  92  93  93  94  95  95  96  97  98\n",
      "  98  99 100 101 102 103 104 105 106 107 108 108 109 110 111  53 112 113\n",
      " 114 115 115 116 117 118 119 119 119 119 120 120 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 132 133  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1]\n",
      "Length at index 2: 109\n",
      "[134 134 134   1  73  73  73 135  75  76  78   9  79 136 136  83  85  24\n",
      "  88  88  89  91  92  93  93  94  95  95  96  97 100 101 137 138 139 139\n",
      " 139 102 103 140 141 104 105 142 143 144 107 108 108 109 110  52  52  52\n",
      "  52  52  53 112 113  55  55  55 115 115 116 145 117  59  59  59  59 118\n",
      " 119 119 119 119 120 120 120 121 122  63 126 126 128 129 146 130 132 132\n",
      " 147  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])\n",
    "#\n",
    "# Retrieve largest length\n",
    "def pad_datamatrix(df):\n",
    "    \"\"\"\n",
    "    Iterates over dataframe and pads SQL_ID lists accordingly with -1 values\n",
    "    \"\"\"\n",
    "    row_sizes = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_sizes.append(len(row['SQL_ID']))\n",
    "    max_row_size = max(row_sizes)\n",
    "    #\n",
    "    # Pad Dataframe Values\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        length = len(row['SQL_ID'])\n",
    "        diff = max_row_size - length\n",
    "        if diff != 0:\n",
    "            for j in range(length, max_row_size):\n",
    "                df['SQL_ID'].iloc[i] = np.append(df['SQL_ID'].iloc[i], -1) # Appends -1 to padded values\n",
    "        # print(\"Length at index \" + str(i) + \": \" + str(df['SQL_ID'].iloc[i].size))\n",
    "        i += 1\n",
    "    return df\n",
    "#\n",
    "df = pad_datamatrix(df)\n",
    "#\n",
    "print('\\n\\n------------------------------------------\\n\\n')\n",
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(814, 4)\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(814, 3)\n",
      "\n",
      "-------------\n",
      "Features After Time Shift\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(814, 4)\n",
      "\n",
      "-------------\n",
      "Labels After Time Shift\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(814, 3)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_row = []\n",
    "for i in range(lag+1,(lag*2)+2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns)*i:len(df.columns)*i + 1]\n",
    "    y_row.append(y_df_column_names)\n",
    "y_df_column_names = []   \n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "#\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "print('\\n-------------\\nLabels After Time Shift')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Feature Lists\n",
    "\n",
    "Expand Feature Lists, where in each list element is represented as it's own features. Total feature count here equates as follows:\n",
    "\n",
    "* Features = (lag * SQL_ID per SNAP_ID count) + SQL_ID per SNAP_ID count\n",
    "* Labels = lag * SQL_ID per SNAP_ID count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "Before: (814, 4)\n",
      "After: (814, 436)\n",
      "Labels\n",
      "Before: (814, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: (814, 327)\n"
     ]
    }
   ],
   "source": [
    "def sequence2features(df):\n",
    "    \"\"\"\n",
    "    Converts pandas sequences into full fledged columns/features\n",
    "    \"\"\"\n",
    "    feature_count = len(df[df.columns[0]].iloc[0])\n",
    "    for column_name in df.columns:\n",
    "        data_matrix = []\n",
    "        new_values = df[column_name].values\n",
    "        #\n",
    "        new_values = np.stack(new_values, axis=0 )\n",
    "        #\n",
    "        for i in range(1,feature_count+1):\n",
    "            new_column_name = column_name + \"_\"+str(i)\n",
    "            df[new_column_name] = new_values[:,i-1]\n",
    "        #\n",
    "        # Drop original list columns\n",
    "        df.drop(column_name, inplace=True, axis=1)\n",
    "    return df\n",
    "#\n",
    "print('Features')\n",
    "print('Before: ' + str(X_df.shape))\n",
    "X_df = sequence2features(df=X_df)\n",
    "print('After: ' + str(X_df.shape))\n",
    "#\n",
    "print('Labels')\n",
    "print('Before: ' + str(y_df.shape))\n",
    "y_df = sequence2features(df=y_df)\n",
    "print('After: ' + str(y_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Removing Null Columns - this check is redundant, but it double checks that there are no useless/flatline columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (820, 1)\n",
      "\n",
      "Shape before changes: [(820, 1)]\n",
      "Shape after changes: [(820, 1)]\n",
      "Dropped a total [0]\n",
      "After: (820, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Before: ' + str(df.shape))\n",
    "#\n",
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "df = drop_flatline_columns(df=df)\n",
    "print('After: ' + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based Model (Many to Many Approach)\n",
    "\n",
    "### RandomForest Classification (Many To Many)\n",
    "\n",
    "Classification attemps using RFC - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Model Input - Takes training data in the form of past SQL_ID sequences, and trains on a number of past sequence histories, determined by the lag value\n",
    "\n",
    "Model Output - Outpus future SQL_ID sequences, determined by the lag output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random Forest\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest Class (Regression + Classification)\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, n_estimators, max_depth=None, criterion='gini', parallel_degree=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.parallel_degree=parallel_degree\n",
    "        self.criterion = criterion\n",
    "        self.model = RandomForestClassifier(max_depth=self.max_depth,\n",
    "                                            n_estimators=self.n_estimators,\n",
    "                                            criterion=self.criterion,\n",
    "                                            n_jobs=self.parallel_degree)\n",
    "    #\n",
    "    def fit_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits training data to target labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X,y)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, lag, plot=False):\n",
    "        \"\"\"\n",
    "        Runs test data through previously trained model, and evaluate differently depending if a regression of classification model\n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # F1-Score Evaluation\n",
    "        feature_count = int(y.shape[1] / lag)\n",
    "        acc_score, f_score = .0,.0\n",
    "        counter = 0\n",
    "        for i in range(y.shape[1]):\n",
    "            acc = accuracy_score(y[:,i], yhat[:,i])\n",
    "            acc_score += acc\n",
    "            f1 = f1_score(y[:,i], yhat[:,i], average='micro') \n",
    "            f_score += f1\n",
    "            if i % (feature_count-1) == 0 and i != 0:\n",
    "                counter += 1\n",
    "                acc_score = acc_score/feature_count # Averaging accuracy accross total sql_ids for that particular timestep\n",
    "                f_score = f_score/feature_count # Averaging f-score accross total sql_ids for that particular timestep\n",
    "                print('Test Accuracy ' + y_labels[i] + ' with LAG value [' + str(counter) + ']: ' +  str(acc_score))\n",
    "                print('Test FScore ' + y_labels[i] + ' with LAG value [' + str(counter) + ']: ' +  str(f_score) + \"\\n\")\n",
    "                acc_score, f_score = .0,.0\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, estimator, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'estimator', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'estimator': estimator,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape [(651, 436)] Type - <class 'numpy.ndarray'>\n",
      "y_train shape [(651, 327)] Type - <class 'numpy.ndarray'>\n",
      "X_validate shape [(81, 436)] Type - <class 'numpy.ndarray'>\n",
      "y_validate shape [(81, 327)] Type - <class 'numpy.ndarray'>\n",
      "X_test shape [(82, 436)] Type - <class 'numpy.ndarray'>\n",
      "y_test shape [(82, 327)] Type - <class 'numpy.ndarray'>\n",
      "------------------------------\n",
      "[[ 134    1  510  170   75   76   77  511    9   79   80  283   82   83\n",
      "    84  285  157   85   86   24   87   88   88   89   90   92   94   95\n",
      "    95   96  546   97   98   98   99  101  102  140  104  151  151  151\n",
      "   193  107  108  108  109  111   53  112  113   55   55   55  288  114\n",
      "   115  115  116  117   59   59   59   59  118  119  119  119  119   60\n",
      "   120  120  120  121  122  123  124   63  213  125  127  128  129  130\n",
      "   132  132  132  132   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1  348  170   73\n",
      "    73   73  266    9   79  385  257 1006  285   85  357  258   91  361\n",
      "    94   96  183  100  101  854  102  103  189  365  366  104   42  161\n",
      "   193  980  106  287  275  367  368  107  236  199  369   53  113  202\n",
      "   116  118   60  371  121  122  211  213  125  126  126  129  375  336\n",
      "   130  377   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1  348  169  170  355   77  356\n",
      "     9   80   82  257  285   85  357  342 1015   87  386  258  359  361\n",
      "   962  363  183  101  506  271  854  389  103  189  365  366   42  193\n",
      "   287  275  367  964  368  236  199  307  369  202  203 1016  264   60\n",
      "   371  211  213  125  374  128  129  375  336  376  377   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1  348  170   78    9  322  148 1017  136  257\n",
      "   285   85   24  318   93   93   94   96  465  183  294  101  333  441\n",
      "   139  139  102  103  189  104  105  160   42  193  287  275  367  153\n",
      "   368  107  236 1018  982   53  113  202  335  116  118  400  400   60\n",
      "   371  121  122  211  213  125  129  146  336  130  251   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "    -1   -1]]\n",
      "[[134   1  73  73  73  75  76   9  79 136  83 285  85  24  88  88  89  91\n",
      "   92  93  93  94  95  95  96  97 100 101 297 150 137 138 102 103 140 141\n",
      "  104 292 151 151 151  42 142 143 144 107 108 108 236 109  53 112 113  55\n",
      "   55  55 114 115 115 116 145 117  59  59  59  59 118 119 119 119 119  60\n",
      "  120 120 120 121 122  63 126 126 129 146 130 132 132 132 132 147  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1 134   1  72  73  73  73  75 291  76  78   9  79 136  83  84 156 285\n",
      "   85  24  88  88  89  91 149  92  93  93  94  95  95  96  97 100 101 137\n",
      "  102 103 140 104 151 151 151 152  42 106 143 107 108 108 236 109  53 112\n",
      "  113  55  55  55 114 115 115 116 145 117 118 119 119 119 119  60 120 120\n",
      "  120 121 122 124  63 213 129 146 130 132 132 132 132 251  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1 134   1 170  73  73  73  75  76  78   9  79 351 284 136 352  83\n",
      "   84 156 285 157  85  24 402  89  90  92  93  93  94  95  95  96  97  99\n",
      "  101 137 286 102 103 104  42 106 143 144 107 108 108 236 109  53 113  55\n",
      "   55  55 115 115 116 117  59  59  59  59 118  60 120 120 120 121 122 124\n",
      "   63 213 125 126 126 129 130 289 132 132 132 132  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1]]\n",
      "------------------------------------------------------------\n",
      "[[  1 135  75  76  78   9  79 875 136 136  83 285  85  24  88  88  89  92\n",
      "   93  93  94  95  95  96  97  99 101 150 137 102 103 103 141 104 105  42\n",
      "  143 144 107 108 108 527 236 109 111 877  53 112 113 114 337 876 115 115\n",
      "  116 145 117 118 119 119 119 119  60 120 120 120 121 122  63 213 125 126\n",
      "  128 129 146 130 147  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1 134   1  73  73  73 135  75 291  76  78   9 322  79 136 136  83 285\n",
      "   85  24  88  88  89  91  92  93  93  94  95  95  96  97 100 101 137 102\n",
      "  103 103 140 104 292 151 151 151  42 106 382 143 144 107 108 108 236 109\n",
      "  111  53 112 113 114 337 115 115 116 145 117 118 119 119 119 119  60 120\n",
      "  120 120 121 122  63 128 129 146 130 132 132 132 251  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1 134   1  72  73  73  73 135  75  76  78   9  79 136 136  83  84\n",
      "  156 157  85  88  88 293  91  92  93  93  94  95  95  96  97  99 294 100\n",
      "  101 137 102 103 103 104 160  42 161 106 143 144 253 107 108 108 236 109\n",
      "  111  53 112 113 337 115 115 116 145 117 118 119 119 119 119  60 120 120\n",
      "  120 121 122 124  63 126 146 130 132 132 132  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1 134 168 169 170  73  73  73 135  75  76  78   9  79 154 136\n",
      "  136  83 175  84 177  85  24  88  88 179  93  93  94  95  95  96 546  97\n",
      "   98  99 150 137 184 185 102 187 103 103 159 104  42 143 196 197 144 162\n",
      "  107 108 108 198 111  53 112 113 337 204 205 115 115 116 145 163 118  60\n",
      "  120 120 120 164 121 122 124  63 213 126 146 130 165 132 132 132 166  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1]]\n",
      "[[168 169 170   9 216 217 171 267 218 219 220 257 223 176  85  24  87 179\n",
      "   96 181 546 182 183 101 271 185 187 189 190 231 104 191 105  42 192 193\n",
      "  233 234 194 195 107 236 198 261 201  53 113 239 202 203 206 207  60 952\n",
      "  209 122 211 212 213 128 214 242  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1 134   1 300 169  73  73  73 135  75 290  76 254   9  79  83 245  85\n",
      "   24  88  88  89  91 301  92  93  93  94  95  95  96 546 182  97  99 100\n",
      "  137 227 102 272 228 103 103 141 104 191 143 248 195 144 162 107 108 108\n",
      "  109 111  53 112 113 302 114 337 115 115 207 116 145 117 163 118 119 119\n",
      "  119 119 120 120 120 164 121 122  63 126 146 130 165 132 132 132 166  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1 134   1  73  73  73 135  75 291  76  78   9 148  79 136 136  83\n",
      "   84  85  88  88  89  91  92  93  93  94  95  95  96 546  97 100 101 150\n",
      "  137 102 103 103 140 104 292 151 151 151 152  42 106 143 107 108 108 109\n",
      "  111  53 112 113 114 337 115 115 116 145 117 118 119 119 119 119  60 120\n",
      "  120 120 121 122 124  63 129 146 130 132 132 132 251  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1]]\n",
      "------------------------------------------------------------\n",
      "[[128  53   9  71 250  84  42  60  76  85  19 207 106 124   6 557 563  22\n",
      "  561  69 609 614 104 118 102 130 121 116  94   1 615  24 616 617 618 115\n",
      "  115 343 343 103 619  83 120 120 120  92 139 139  63  52  52  52  55  55\n",
      "   55 146 101 143 109  95  95 107 113 122  96 445  88  88 620 117 621  89\n",
      "   79 112  75  91 100 140 111  73  73  73  72  72  78 136 136  93  93 108\n",
      "  108  97 137 134 119 119 119 119 622  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1 162 166  53   9  71 133 250  84  42  60 213 170  76  85 165 207 124\n",
      "  557 563 623 623  22 561  69 164 614 163 104 118 102 130 121 116  94 624\n",
      "    1 156 615  24 616 625 115 115 343 343 103 619  83 120 120 120 139 139\n",
      "   63  52  52  52 626  55  55  55 101 143 109  95  95 107 113 122  96 445\n",
      "  117  79 112 144  75  91 100 111  73  73  73  72  72  78 136 136  93  93\n",
      "  108 108  59  59  59  59  97 137 134 119 119 119 119  99  -1  -1  -1  -1\n",
      "   -1  -1 609  71   1  72  72 170 627 624   9  79 352  83 628  22  84 629\n",
      "  630 285  85  24  88  88 631  89  91 632  94  96  98  98  99 633 100 101\n",
      "  102 103 140 141 104 634 635  42 636 162 107 236 111 561  52  52  52  53\n",
      "  113  55  55  55 637 638 639 115 115 563 116 117  59  59  59  59 118 119\n",
      "  119 119 119  60 640 121 122 641 123 124 571  63 213 127 129 146 130 250\n",
      "  642 643 133  69  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1 609 134  71   1  72  72  73  73  73  75 644  76  78   9  79\n",
      "  136 136  83 483  84 555  85  24 645  88  88  89  91  92  93  93  94  95\n",
      "   95  96  98  98 100 101 150 343 343 137 139 139 102 103 104  42 161 559\n",
      "  143 107 108 108 109 111 561  52  52  52  53 112 113 646  55  55  55 486\n",
      "  638 114 337 115 115 563 116 145 117 118 119 119 119 119  60 647 120 120\n",
      "  120 640 121 122 124  63 128 129 648 146 130 250 649 133  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1]]\n",
      "[[134  71   1  72  72  73  73  73  75  76 650  78   9  79 252 136 136 651\n",
      "   83  84 157  85  24  88  88  92  93  93  94  95  95  96  97  99 294 100\n",
      "  101 343 343 137 139 139 102 103 104 160  42 106 143 144 253 107 108 108\n",
      "  109 110 111 561  52  52  52  53 112 113  55  55  55 638 480 337 115 115\n",
      "  563 116 145 117  59  59  59  59 118 119 119 119 119 120 120 120 640 121\n",
      "  122 124  63 126 128 129 146 130 250 652 652 652 649  -1  -1  -1  -1  -1\n",
      "   -1 327  71 168 653 654 135 655  78   9 171 154 172 651 174 656 175 155\n",
      "   22  84 177 177  85  24 318 657 179  96 297 413 343 343 658 137 184 185\n",
      "  659 187 159 104 191 151 151 151 151  42 193 194 196 197 162 107 198 200\n",
      "  561  53 112 113 288 337 204 205 563 207 163 164 122 660 124  63 250 165\n",
      "  652 652 652 652 652 147  69 166  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  71 167 661 662 168 663 170   6   9 216 664 217 171 218 665 219\n",
      "  220 666 656  19  21 667 223  22 555  85 668  24 669 657  96 670  31 413\n",
      "  671 672 673 185 674 675 659 187 231 104  42 559 193 194 107 198 573 561\n",
      "  201 676 677  53 678 113 206 563 597  60 679 680 681 682 209 122 213 214\n",
      "  242 250 652 652 652 652 652  69  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1]]\n",
      "Training + Validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=4,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Test Accuracy var1(t+1)_109 with LAG value [1]: 0.3696907917091402\n",
      "Test FScore var1(t+1)_109 with LAG value [1]: 0.3696907917091402\n",
      "\n",
      "Test Accuracy var1(t+2)_108 with LAG value [2]: 0.32676407294144294\n",
      "Test FScore var1(t+2)_108 with LAG value [2]: 0.32676407294144294\n",
      "\n",
      "Test Accuracy var1(t+3)_107 with LAG value [3]: 0.31203986861479205\n",
      "Test FScore var1(t+3)_107 with LAG value [3]: 0.31203986861479205\n",
      "\n",
      "\n",
      "\n",
      "Training + Testing\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=4,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Test Accuracy var1(t+1)_109 with LAG value [1]: 0.3326247482658312\n",
      "Test FScore var1(t+1)_109 with LAG value [1]: 0.3326247482658312\n",
      "\n",
      "Test Accuracy var1(t+2)_108 with LAG value [2]: 0.2982770194674423\n",
      "Test FScore var1(t+2)_108 with LAG value [2]: 0.2982770194674423\n",
      "\n",
      "Test Accuracy var1(t+3)_107 with LAG value [3]: 0.3021928843141643\n",
      "Test FScore var1(t+3)_107 with LAG value [3]: 0.3021928843141643\n",
      "\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_labels = y_df.columns\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "#\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "#\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "#\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)) + \"\\n------------------------------\")\n",
    "#\n",
    "print(X_train[0:1])\n",
    "print(y_train[0:1])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate[0:1])\n",
    "print(y_validate[0:1])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test[0:1])\n",
    "print(y_test[0:1])\n",
    "#\n",
    "# Deletes pandas frames to conserve RAM space.\n",
    "del X_df, y_df \n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "print('Training + Validation')\n",
    "model = RandomForest(n_estimators=n_estimators,\n",
    "                     max_depth=max_depth,\n",
    "                     criterion=criterion,\n",
    "                     parallel_degree=parallel_degree)\n",
    "model.fit_model(X=X_train,\n",
    "                y=y_train)\n",
    "model.predict_and_evaluate(X=X_validate,\n",
    "                           y=y_validate,\n",
    "                           y_labels=y_labels,\n",
    "                           lag=lag,\n",
    "                           plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "print('\\n\\nTraining + Testing')\n",
    "model.fit_model(X=X_validate,\n",
    "                y=y_validate)\n",
    "model.predict_and_evaluate(X=X_test,\n",
    "                           y=y_test,\n",
    "                           y_labels=y_labels,\n",
    "                           lag=lag,\n",
    "                           plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
