{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis (RFC)\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=3 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "#\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "#\n",
    "# Forest Config\n",
    "parallel_degree = 4\n",
    "n_estimators = 500\n",
    "max_depth = 7\n",
    "criterion='gini'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "#\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (2230, 90)\n",
      "Shape After Aggregation: (27, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     37137  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "1     37138  [06dymzb481vnd, 13a9r2xkx1bxb, 14f5ngrj3cc5h, ...\n",
      "2     37139  [01d5n1nm17r2h, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     37140  [04kug40zbu4dm, 06g9mhm5ba7tt, 0y080mnfaqk3u, ...\n",
      "4     37141  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "5     37142  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "6     37143  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "7     37144  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     37145  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "9     37146  [06g9mhm5ba7tt, 0hhmdwwgxbw0r, 0kcbwucxmazcp, ...\n",
      "10    37147  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "11    37148  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "12    37149  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "13    37150  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "14    37151  [06g9mhm5ba7tt, 09vrdx888wvvb, 0a7q9v9nd2qc1, ...\n",
      "15    37152  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "16    37153  [01tp87bk1t2zv, 06dymzb481vnd, 130r442w3nfny, ...\n",
      "17    37154  [0qbzfjt00pbsx, 0w26sk6t6gq98, 0y080mnfaqk3u, ...\n",
      "18    37155  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "19    37156  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "20    37157  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "21    37158  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0v3dvmc22qnam, ...\n",
      "22    37159  [01d5n1nm17r2h, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "23    37160  [01tp87bk1t2zv, 04kug40zbu4dm, 06dymzb481vnd, ...\n",
      "24    37161  [01d5n1nm17r2h, 01tp87bk1t2zv, 04kug40zbu4dm, ...\n",
      "25    37162  [01tp87bk1t2zv, 04kug40zbu4dm, 06dymzb481vnd, ...\n",
      "26    37163  [04kug40zbu4dm, 06dymzb481vnd, 0w26sk6t6gq98, ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "df = rep_hist_snapshot_df.groupby(['SNAP_ID'])['SQL_ID'].apply(list).reset_index()\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(df.shape))\n",
    "print(type(df))\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 2)\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     37137  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "1     37138  [06dymzb481vnd, 13a9r2xkx1bxb, 14f5ngrj3cc5h, ...\n",
      "2     37139  [01d5n1nm17r2h, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     37140  [04kug40zbu4dm, 06g9mhm5ba7tt, 0y080mnfaqk3u, ...\n",
      "4     37141  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "5     37142  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "6     37143  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "7     37144  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     37145  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "9     37146  [06g9mhm5ba7tt, 0hhmdwwgxbw0r, 0kcbwucxmazcp, ...\n",
      "10    37147  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "11    37148  [01tp87bk1t2zv, 06dymzb481vnd, 0a08ug2qc1j82, ...\n",
      "12    37149  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "13    37150  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "14    37151  [06g9mhm5ba7tt, 09vrdx888wvvb, 0a7q9v9nd2qc1, ...\n",
      "15    37152  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "16    37153  [01tp87bk1t2zv, 06dymzb481vnd, 130r442w3nfny, ...\n",
      "17    37154  [0qbzfjt00pbsx, 0w26sk6t6gq98, 0y080mnfaqk3u, ...\n",
      "18    37155  [01tp87bk1t2zv, 06dymzb481vnd, 0jj0ct4x4gy27, ...\n",
      "19    37156  [01tp87bk1t2zv, 06dymzb481vnd, 0y080mnfaqk3u, ...\n",
      "20    37157  [01tp87bk1t2zv, 06dymzb481vnd, 0kcbwucxmazcp, ...\n",
      "21    37158  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0v3dvmc22qnam, ...\n",
      "22    37159  [01d5n1nm17r2h, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "23    37160  [01tp87bk1t2zv, 04kug40zbu4dm, 06dymzb481vnd, ...\n",
      "24    37161  [01d5n1nm17r2h, 01tp87bk1t2zv, 04kug40zbu4dm, ...\n",
      "25    37162  [01tp87bk1t2zv, 04kug40zbu4dm, 06dymzb481vnd, ...\n",
      "26    37163  [04kug40zbu4dm, 06dymzb481vnd, 0w26sk6t6gq98, ...\n"
     ]
    }
   ],
   "source": [
    "df.sort_index(ascending=True,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection\n",
    "\n",
    "This sextion treats the dataset as a univariate dataset. Therefore the SNAP_ID pertaining to each set of SQL_IDs is removed, with the intent of future classifiers training solely on past SQL executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 2)\n",
      "(27, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "del df['SNAP_ID']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "\n",
    "NB: Since this experiment is solely focussed on Random Forest Training, One-Hot Encoding will not be used as recommended below:\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \"\"\"\n",
    "    Scikit Label Encoder was acting up with the following error whilst using the transform function, even though I tripled \n",
    "    checked that the passed data was exactly the same as the one used for training:\n",
    "    \n",
    "    * https://stackoverflow.com/questions/46288517/getting-valueerror-y-contains-new-labels-when-using-scikit-learns-labelencoder\n",
    "    \n",
    "    So I have rebuilt a similar functionality to categorize my data into numeric digits, as the LabelEncoder is supposed to do.\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__class_map = {}\n",
    "        self.__integer_counter = 0\n",
    "    #\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        :param - X: python list\n",
    "        \"\"\"\n",
    "        for val in X:\n",
    "            if val not in self.__class_map:\n",
    "                self.__class_map[val] = self.__integer_counter\n",
    "                self.__integer_counter += 1\n",
    "    #\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        param - X: python list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for val in X:\n",
    "            if val in self.__class_map:\n",
    "                value = self.__class_map[val]\n",
    "                encoded_map.append(value)\n",
    "            else:\n",
    "                raise ValueError('Label Mismatch - Encountered a label which was not trained on.')\n",
    "        return encoded_map\n",
    "    #\n",
    "    def get_class_map(self):\n",
    "        \"\"\"\n",
    "        Returns original classes as a list\n",
    "        \"\"\"\n",
    "        class_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            class_map.append(key)\n",
    "        return class_map\n",
    "    #\n",
    "    def get_encoded_map(self):\n",
    "        \"\"\"\n",
    "        Returns class encodings as a list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            encoded_map.append(value)\n",
    "        return encoded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 1)\n",
      "                                              SQL_ID\n",
      "0  [03ggjrmy0wa1w, 0aq14dznn91rg, 0f60bzgt9127c, ...\n",
      "1  [06dymzb481vnd, 13a9r2xkx1bxb, 14f5ngrj3cc5h, ...\n",
      "2  [01d5n1nm17r2h, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3  [04kug40zbu4dm, 06g9mhm5ba7tt, 0y080mnfaqk3u, ...\n",
      "4  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "5  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "6  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "7  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "9  [06g9mhm5ba7tt, 0hhmdwwgxbw0r, 0kcbwucxmazcp, ...\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "Total SQL_ID Classes: 379\n",
      "['03ggjrmy0wa1w', '0aq14dznn91rg', '0f60bzgt9127c', '0ga8vk4nftz45', '0hdquu87pydzk', '1jhyrdp21f2q6', '1p5grz1gs7fjq', '1r7b985mxqj71', '29mjaymwt5p6d', '2hnpu9m861609']\n",
      "(27, 1)\n",
      "                                              SQL_ID\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
      "1  [47, 48, 49, 6, 50, 51, 52, 53, 54, 55, 56, 57...\n",
      "2  [91, 92, 92, 92, 93, 94, 95, 96, 97, 6, 98, 99...\n",
      "3  [93, 94, 161, 161, 161, 162, 6, 163, 164, 98, ...\n",
      "4  [47, 161, 161, 161, 96, 97, 162, 6, 98, 166, 1...\n",
      "5  [92, 92, 92, 47, 161, 161, 161, 96, 213, 97, 1...\n",
      "6  [92, 92, 92, 47, 161, 161, 161, 96, 97, 162, 6...\n",
      "7  [92, 92, 92, 220, 221, 222, 161, 161, 161, 223...\n",
      "8  [94, 245, 220, 222, 246, 247, 6, 248, 249, 167...\n",
      "9  [94, 280, 245, 220, 222, 281, 6, 282, 249, 283...\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "le = LabelEncoder()\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    le.fit(sql_id_list)\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    transformed_list = le.transform(sql_id_list)\n",
    "    df['SQL_ID'].iloc[index] = transformed_list \n",
    "#\n",
    "print(\"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "print('Total SQL_ID Classes: ' + str(len(le.get_class_map())))\n",
    "print(le.get_class_map()[:10])\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "A note regarding normalization. Normalization for this experiment was purposely skipped, since value dimensionality & size is not as important for RandomForest based models. The purity split  does not benefit greatly from such a process:\n",
    "\n",
    "* https://stats.stackexchange.com/questions/57010/is-it-essential-to-do-normalization-for-svm-and-random-forest\n",
    "* https://stackoverflow.com/questions/8961586/do-i-need-to-normalize-or-scale-data-for-randomforest-r-package\n",
    "* https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-8-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Padding\n",
    "\n",
    "Since there isn't a fixed number of SQL_ID's per SNAP_ID, each set of SQL_IDs need to be padded so as to assume an equal number if SQL_IDs for the purpose of model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length at index 0: 47\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "Length at index 1: 55\n",
      "[47, 48, 49, 6, 50, 51, 52, 53, 54, 55, 56, 57, 11, 58, 59, 13, 60, 61, 62, 19, 63, 64, 65, 22, 66, 67, 24, 68, 69, 70, 71, 26, 72, 73, 29, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 39, 86, 87, 88, 42, 43, 89, 90]\n",
      "Length at index 2: 98\n",
      "[91, 92, 92, 92, 93, 94, 95, 96, 97, 6, 98, 99, 99, 99, 100, 101, 102, 103, 104, 105, 13, 106, 107, 108, 108, 109, 110, 111, 112, 113, 114, 115, 116, 19, 117, 118, 118, 119, 120, 121, 122, 22, 123, 124, 24, 125, 126, 26, 127, 128, 29, 129, 130, 130, 131, 131, 132, 133, 134, 135, 136, 136, 137, 138, 139, 140, 141, 142, 143, 144, 144, 145, 146, 147, 148, 149, 149, 149, 149, 150, 151, 151, 151, 152, 39, 153, 154, 155, 40, 156, 157, 158, 43, 159, 159, 159, 159, 160]\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "Length at index 0: 104\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Length at index 1: 104\n",
      "[47 48 49  6 50 51 52 53 54 55 56 57 11 58 59 13 60 61 62 19 63 64 65 22\n",
      " 66 67 24 68 69 70 71 26 72 73 29 74 75 76 77 78 79 80 81 82 83 84 85 39\n",
      " 86 87 88 42 43 89 90 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Length at index 2: 104\n",
      "[ 91  92  92  92  93  94  95  96  97   6  98  99  99  99 100 101 102 103\n",
      " 104 105  13 106 107 108 108 109 110 111 112 113 114 115 116  19 117 118\n",
      " 118 119 120 121 122  22 123 124  24 125 126  26 127 128  29 129 130 130\n",
      " 131 131 132 133 134 135 136 136 137 138 139 140 141 142 143 144 144 145\n",
      " 146 147 148 149 149 149 149 150 151 151 151 152  39 153 154 155  40 156\n",
      " 157 158  43 159 159 159 159 160  -1  -1  -1  -1  -1  -1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])\n",
    "#\n",
    "# Retrieve largest length\n",
    "def pad_datamatrix(df):\n",
    "    \"\"\"\n",
    "    Iterates over dataframe and pads SQL_ID lists accordingly with -1 values\n",
    "    \"\"\"\n",
    "    row_sizes = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_sizes.append(len(row['SQL_ID']))\n",
    "    max_row_size = max(row_sizes)\n",
    "    #\n",
    "    # Pad Dataframe Values\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        length = len(row['SQL_ID'])\n",
    "        diff = max_row_size - length\n",
    "        if diff != 0:\n",
    "            for j in range(length, max_row_size):\n",
    "                df['SQL_ID'].iloc[i] = np.append(df['SQL_ID'].iloc[i], -1) # Appends -1 to padded values\n",
    "        # print(\"Length at index \" + str(i) + \": \" + str(df['SQL_ID'].iloc[i].size))\n",
    "        i += 1\n",
    "    return df\n",
    "#\n",
    "df = pad_datamatrix(df)\n",
    "#\n",
    "print('\\n\\n------------------------------------------\\n\\n')\n",
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(21, 4)\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(21, 3)\n",
      "\n",
      "-------------\n",
      "Features After Time Shift\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(21, 4)\n",
      "\n",
      "-------------\n",
      "Labels After Time Shift\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(21, 3)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_row = []\n",
    "for i in range(lag+1,(lag*2)+2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns)*i:len(df.columns)*i + 1]\n",
    "    y_row.append(y_df_column_names)\n",
    "y_df_column_names = []   \n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "#\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "print('\\n-------------\\nLabels After Time Shift')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Feature Lists\n",
    "\n",
    "Expand Feature Lists, where in each list element is represented as it's own features. Total feature count here equates as follows:\n",
    "\n",
    "* Features = (lag * SQL_ID per SNAP_ID count) + SQL_ID per SNAP_ID count\n",
    "* Labels = lag * SQL_ID per SNAP_ID count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "Before: (21, 4)\n",
      "After: (21, 416)\n",
      "Labels\n",
      "Before: (21, 3)\n",
      "After: (21, 312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def sequence2features(df):\n",
    "    \"\"\"\n",
    "    Converts pandas sequences into full fledged columns/features\n",
    "    \"\"\"\n",
    "    feature_count = len(df[df.columns[0]].iloc[0])\n",
    "    for column_name in df.columns:\n",
    "        data_matrix = []\n",
    "        new_values = df[column_name].values\n",
    "        #\n",
    "        new_values = np.stack(new_values, axis=0 )\n",
    "        #\n",
    "        for i in range(1,feature_count+1):\n",
    "            new_column_name = column_name + \"_\"+str(i)\n",
    "            df[new_column_name] = new_values[:,i-1]\n",
    "        #\n",
    "        # Drop original list columns\n",
    "        df.drop(column_name, inplace=True, axis=1)\n",
    "    return df\n",
    "#\n",
    "print('Features')\n",
    "print('Before: ' + str(X_df.shape))\n",
    "X_df = sequence2features(df=X_df)\n",
    "print('After: ' + str(X_df.shape))\n",
    "#\n",
    "print('Labels')\n",
    "print('Before: ' + str(y_df.shape))\n",
    "y_df = sequence2features(df=y_df)\n",
    "print('After: ' + str(y_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Removing Null Columns - this check is redundant, but it double checks that there are no useless/flatline columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (27, 1)\n",
      "\n",
      "Shape before changes: [(27, 1)]\n",
      "Shape after changes: [(27, 1)]\n",
      "Dropped a total [0]\n",
      "After: (27, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Before: ' + str(df.shape))\n",
    "#\n",
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "df = drop_flatline_columns(df=df)\n",
    "print('After: ' + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based Model (Many to Many Approach)\n",
    "\n",
    "### RandomForest Classification (Many To Many)\n",
    "\n",
    "Classification attemps using RFC - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Model Input - Takes training data in the form of past SQL_ID sequences, and trains on a number of past sequence histories, determined by the lag value\n",
    "\n",
    "Model Output - Outpus future SQL_ID sequences, determined by the lag output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random Forest\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest Class (Regression + Classification)\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, n_estimators, max_depth=None, criterion='gini', parallel_degree=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.parallel_degree=parallel_degree\n",
    "        self.criterion = criterion\n",
    "        self.model = RandomForestClassifier(max_depth=self.max_depth,\n",
    "                                            n_estimators=self.n_estimators,\n",
    "                                            criterion=self.criterion,\n",
    "                                            n_jobs=self.parallel_degree)\n",
    "    #\n",
    "    def fit_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits training data to target labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X,y)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, lag, plot=False):\n",
    "        \"\"\"\n",
    "        Runs test data through previously trained model, and evaluate differently depending if a regression of classification model\n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # F1-Score Evaluation\n",
    "        feature_count = int(y.shape[1] / lag)\n",
    "        acc_score, f_score = .0,.0\n",
    "        counter = 1\n",
    "        for i in range(y.shape[1]):\n",
    "            acc = accuracy_score(y[:,i], yhat[:,i], normalize=True)\n",
    "            acc_score += acc\n",
    "            f1 = f1_score(y[:,i], yhat[:,i], average='micro') \n",
    "            f_score += f1\n",
    "            if i % (feature_count-1) == 0 and i != 0:\n",
    "                acc_score = acc_score/feature_count # Averaging accuracy accross total sql_ids for that particular timestep\n",
    "                f_score = f_score/feature_count # Averaging f-score accross total sql_ids for that particular timestep\n",
    "                print('Test Accuracy LAG value [' + str(counter) + ']: ' +  str(acc_score))\n",
    "                print('Test FScore LAG value [' + str(counter) + ']: ' +  str(f_score) + \"\\n\")\n",
    "                acc_score, f_score = .0,.0\n",
    "                counter += 1\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, estimator, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'estimator', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'estimator': estimator,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape [(16, 416)] Type - <class 'numpy.ndarray'>\n",
      "y_train shape [(16, 312)] Type - <class 'numpy.ndarray'>\n",
      "X_validate shape [(2, 416)] Type - <class 'numpy.ndarray'>\n",
      "y_validate shape [(2, 312)] Type - <class 'numpy.ndarray'>\n",
      "X_test shape [(3, 416)] Type - <class 'numpy.ndarray'>\n",
      "y_test shape [(3, 312)] Type - <class 'numpy.ndarray'>\n",
      "------------------------------\n",
      "[[ 93  94 161 ...  -1  -1  -1]\n",
      " [ 92  47 319 ...  -1  -1  -1]\n",
      " [ 92  92  92 ...  -1  -1  -1]\n",
      " [344 222 161 ...  -1  -1  -1]\n",
      " [ 47 161 161 ...  -1  -1  -1]]\n",
      "[[ 92  92  92 ...  -1  -1  -1]\n",
      " [  0   1   2 ...  -1  -1  -1]\n",
      " [ 92  47 319 ...  -1  -1  -1]\n",
      " [245 220 221 ...  -1  -1  -1]\n",
      " [ 94 245 220 ...  -1  -1  -1]]\n",
      "------------------------------------------------------------\n",
      "[[ 94 280 245 220 222 281   6 282 249 283 284 285 286 287 288 169 289 290\n",
      "  105  13 106 291 292 293 116 294 295  19 175 296 297 298  22 299 300 301\n",
      "  258 302 259  24 125 261 303  26 128 304 266  29 234 129 305 306 137 139\n",
      "  307 308 309 273 274 310 311 275 238 312  37 313 239  39 153 314 242  43\n",
      "  315 243 244  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  92  92  92  47\n",
      "  316 316 161 161 161  96  97 162   6  98 166 166 100 105  13 107 108 108\n",
      "  109 173 111 114 174 174 115 116  19 117 177 120 121 201  65 317  22 123\n",
      "  180 124 202  24 125 126  26 182 204 205 318  29 206 206 129 130 130 132\n",
      "  137 138 139 209 144 144 145 210 146 211 211 211 211 148 149 149 149 149\n",
      "   37 151 151 151 152  39 153  87 242 195 195  40 196 197 158  43 159 159\n",
      "  159 159  -1  -1  -1  -1  -1  -1  -1  -1  92  47 319 319 161 161 161  96\n",
      "   97 162   6 164  98 166 166 100 102 217 105  13 107 108 108 109 173 111\n",
      "  114 174 174 115 116  19 117 177 120 201  65  22 123 180 124  24 125 214\n",
      "  214 214 215  26 182 183 205  29 129 130 130 132 137 138 139 143 209 144\n",
      "  144 145 210 146 320 148 149 149 149 149  37 151 151 151 152  39 153 154\n",
      "   87 196 197 158  43 159 159 159 159  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  92  47 245 220 161 161 161  96  97 162   6  98\n",
      "  225 166 166 100 102 217 105  13 107 108 108 111 254 114 174 174 115 116\n",
      "   19 117 119 201  65 232 257  22 123 180  24 125 261  26 182 183 205  29\n",
      "  206 206 234 129 130 130 132 137 138 139 143 209 236 144 144 275 145 210\n",
      "  146 238 148 149 149 149 149 151 151 151 239 152  39 153 154  87 195 195\n",
      "  197 158  43 243 159 159 159 159 244  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ 92  47 161 161 161  96  97 162   6 164  98  99  99  99 166 166 100 102\n",
      "  217 105  13 107 108 108 109 173 111 114 174 174 115 116  19 117 177 120\n",
      "  201  65  22 123 180 180 124  24 125 214 214 214  26 182 204 183 205 185\n",
      "   29 129 130 130 132 137 138 139 143 209 144 144 145 210 146 148 149 149\n",
      "  149 149  37 151 151 151 152  39 153 154  87 195 195 196 197 158  43 159\n",
      "  159 159 159  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  92  47 245 220\n",
      "  161 161 161 223  96  97 162   6  98 225 226 166 166 100 102 230 105  13\n",
      "  107 108 108 111 254 114 174 174 115 116  19 117 119 201  65 232 257  22\n",
      "  123 233 180 180  24 125  26 182 204 183 205  29 206 206 234 129 130 130\n",
      "  268 132 137 138 139 143 209 236 144 144 275 145 146 238 148  37 151 151\n",
      "  151 239 152  39 153 154  87 195 195 197 158  43 243 159 159 159 159 244\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1 245 220 221 222 356   6 224 248\n",
      "  249 332 321 284 169 228 229 289 102 250 105  13 231 106 323 357 254 116\n",
      "  256  19 175 232 257  22 233 258 259  24 125 261  26 262 128 266  29 358\n",
      "  129 268 271 235 137 139 273 142 359 237 339 275 360  37  39 153 154 314\n",
      "  276 242 277  43  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  91 220 222 345 162   6 225  99  99  99 285 100\n",
      "  361 287 216 104 105 362  13 172 110 291 292 115 363 116  19 117 119 122\n",
      "  298  22 364 123 300 301 351 259  24 125 261 303  26 182 128 204 205 318\n",
      "  266  29 234 129 130 130 132 135 137 139 309 209 275 145 146 238 148  37\n",
      "  151 151 151 239 152  39 153 242 195 195 340 158  43 243 244  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]]\n",
      "[[ 94 245 220   6 248 249 321 322 226 286 167 227 169 229 102 250 217 230\n",
      "  105  13 106 251 252 323 254 255 116 256 324 294  19 175 232 257  22 325\n",
      "  233 259  24 125 261 326  26 262 128 327 266  29 129 268 269 328 270 271\n",
      "  137 139 142 237 275  39 153 154 276 277 329  43  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  94 330 331 280\n",
      "  220 281   6 249 283 332 333 284 334 286 287 335 288 169 289 290 250 105\n",
      "  336  13 106 252 291 292 293 337 116  19 175 298  22 299 300 301 302 259\n",
      "   24 125 261 303  26 128 327 265 304 318 266  29 234 129 270 305 306 137\n",
      "  338 139 307 309 274 339 310 311 275 238 239  39 153 276 340  43 243 244\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1   0   1   2   3  48 341   5   6\n",
      "   50   7  52   8   9  54  55  56  57  11  12  13  14  15  16  61  17  18\n",
      "   19  20  21  22  23  24  69  70  25  71  26  27  28  29  30  75  76  31\n",
      "   32 137  33  79  34  82  83  36  85  38  39  88  41  42  43  44  45  46\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1]\n",
      " [ 92  93  47 161 161 161 223  96  97 162   6  98 166 166 100 103 105  13\n",
      "  107 108 108 109 173 111 114 174 174 115 116  19 117 118 118 177 120 201\n",
      "  365 365  65 123 180 180 124  24 125 126 203 182 127 205 206 206 129 130\n",
      "  130 366 366 132 367 207 135 368 137 138 139 143 209 144 144 145 210 146\n",
      "  148 149 149 149 149 151 151 151 152 153  87 195 195  40 197 369 158 159\n",
      "  159 159 159  29  22  26  39  43  -1  -1  -1  -1  -1  -1  91  92  93  47\n",
      "  319 319 161 161 161 223  96  97 162   6 164  98 166 166 100 102 217 105\n",
      "   13 107 108 108 109 173 111 114 174 174 115 116  19 117 118 118 177 120\n",
      "  201 365 365  65  22 123 180 180  24 125 214 214 214 215  26 182 183 205\n",
      "   29 129 130 130 366 366 132 135 137 138 139 143 209 144 144 145 210 146\n",
      "  148 149 149 149 149 151 151 151 152  39 153 154  87 195 195  40 196 197\n",
      "  158  43 159 159 159 159 160  -1  -1  -1  92  93  47 370 222 161 161 161\n",
      "  223  96  97 162   6  98 225 166 166 100 102 217 103 104 105  13 107 108\n",
      "  108 109 110 111 114 174 174 115 116  19 117 118 118 119 176 120 201 365\n",
      "  365  65  22 123 180 180  24 125  26 182 127 183 205  29 206 219 129 130\n",
      "  130 366 366 132 367 135 137 138 139 143 209 144 144 145 146 148 149 149\n",
      "  149 149  37 151 151 151 152  39 153 154  87 242 195 195 196 371 197 158\n",
      "   43 159 159 159 159 160]]\n",
      "------------------------------------------------------------\n",
      "[[ 92  47  96 ...  -1  -1  -1]\n",
      " [ 94 245 220 ...  -1  -1  -1]\n",
      " [ 94 245 220 ...  -1  -1  -1]]\n",
      "[[ 92  47 245 220 161 161 161 223  96  97 162   6  98 225 226 166 166 100\n",
      "  102 230 105  13 107 108 108 111 254 114 174 174 115 116  19 117 119 201\n",
      "   65 232 257  22 123 233 180 180  24 125  26 182 204 183 205  29 206 206\n",
      "  234 129 130 130 268 132 137 138 139 143 209 236 144 144 275 145 146 238\n",
      "  148  37 151 151 151 239 152  39 153 154  87 195 195 197 158  43 243 159\n",
      "  159 159 159 244  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1 245 220 221 222\n",
      "  356   6 224 248 249 332 321 284 169 228 229 289 102 250 105  13 231 106\n",
      "  323 357 254 116 256  19 175 232 257  22 233 258 259  24 125 261  26 262\n",
      "  128 266  29 358 129 268 271 235 137 139 273 142 359 237 339 275 360  37\n",
      "   39 153 154 314 276 242 277  43  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  91 220 222 345 162   6 225  99\n",
      "   99  99 285 100 361 287 216 104 105 362  13 172 110 291 292 115 363 116\n",
      "   19 117 119 122 298  22 364 123 300 301 351 259  24 125 261 303  26 182\n",
      "  128 204 205 318 266  29 234 129 130 130 132 135 137 139 309 209 275 145\n",
      "  146 238 148  37 151 151 151 239 152  39 153 242 195 195 340 158  43 243\n",
      "  244  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1]\n",
      " [ 92  47 245 220 161 161 161  96  97 162   6  98 225 166 166 100 102 217\n",
      "  105  13 107 108 108 111 254 114 174 174 115 116  19 117 119 201  65 232\n",
      "  257  22 123 180  24 125 261  26 182 183 205  29 206 206 234 129 130 130\n",
      "  132 137 138 139 143 209 236 144 144 275 145 210 146 238 148 149 149 149\n",
      "  149 151 151 151 239 152  39 153 154  87 195 195 197 158  43 243 159 159\n",
      "  159 159 244  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  94 245 220   6\n",
      "  248 249 321 322 226 286 167 227 169 229 102 250 217 230 105  13 106 251\n",
      "  252 323 254 255 116 256 324 294  19 175 232 257  22 325 233 259  24 125\n",
      "  261 326  26 262 128 327 266  29 129 268 269 328 270 271 137 139 142 237\n",
      "  275  39 153 154 276 277 329  43  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  94 330 331 280 220 281   6 249\n",
      "  283 332 333 284 334 286 287 335 288 169 289 290 250 105 336  13 106 252\n",
      "  291 292 293 337 116  19 175 298  22 299 300 301 302 259  24 125 261 303\n",
      "   26 128 327 265 304 318 266  29 234 129 270 305 306 137 338 139 307 309\n",
      "  274 339 310 311 275 238 239  39 153 276 340  43 243 244  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1]\n",
      " [344 222 161 161 161 223 345 346 347 162   6  98 348 225  99  99  99 349\n",
      "  166 166 350 104 105  13 106 107 110 173 111 114 174 174 115 116  19 177\n",
      "  120 201  22 123 180 180 351  24 125  26 182 128 183 264 205  29 206 206\n",
      "  129 130 130 352 135 353 137 138 139 209 145 146 148  37 151 151 151 152\n",
      "   39 153 242 194 195 195  40 196 197 354 158  43  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  92  47 316 316\n",
      "  161 161 161  96  97 162   6  98 166 166 100 105  13 107 108 108 109 173\n",
      "  111 114 174 174 115 116  19 117 177 120 121 201  65  22 123 180 180 124\n",
      "  202  24 125 126  26 182 204 355 205  29 206 206 129 130 130 132 137 138\n",
      "  139 209 144 144 145 210 146 148 149 149 149 149  37 151 151 151 152  39\n",
      "  153  87 196 197 158  43 159 159 159 159  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  92  47 161 161 161  96  97 162\n",
      "    6 164  98  99  99  99 166 166 100 102 217 105  13 107 108 108 109 173\n",
      "  111 114 174 174 115 116  19 117 177 120 201  65  22 123 180 180 124  24\n",
      "  125 214 214 214  26 182 204 183 205 185  29 129 130 130 132 137 138 139\n",
      "  143 209 144 144 145 210 146 148 149 149 149 149  37 151 151 151 152  39\n",
      "  153 154  87 195 195 196 197 158  43 159 159 159 159  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1]]\n",
      "Training + Validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=4,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Test Accuracy LAG value [1]: 0.28365384615384615\n",
      "Test FScore LAG value [1]: 0.28365384615384615\n",
      "\n",
      "Test Accuracy LAG value [2]: 0.14423076923076922\n",
      "Test FScore LAG value [2]: 0.14423076923076922\n",
      "\n",
      "Test Accuracy LAG value [3]: 0.17307692307692307\n",
      "Test FScore LAG value [3]: 0.17307692307692307\n",
      "\n",
      "\n",
      "\n",
      "Training + Testing\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=4,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Test Accuracy LAG value [1]: 0.1474358974358974\n",
      "Test FScore LAG value [1]: 0.1474358974358974\n",
      "\n",
      "Test Accuracy LAG value [2]: 0.2435897435897436\n",
      "Test FScore LAG value [2]: 0.2435897435897436\n",
      "\n",
      "Test Accuracy LAG value [3]: 0.19230769230769232\n",
      "Test FScore LAG value [3]: 0.19230769230769232\n",
      "\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "#\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - \" + str(type(X_train)))\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - \" + str(type(y_train)))\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "#\n",
    "X_validate = X_validate.values\n",
    "y_validate = y_validate.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - \" + str(type(X_validate)))\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - \" + str(type(y_validate)))\n",
    "#\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - \" + str(type(X_test)))\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - \" + str(type(y_test)) + \"\\n------------------------------\")\n",
    "#\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate[0:5])\n",
    "print(y_validate[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "#\n",
    "# Deletes pandas frames to conserve RAM space.\n",
    "del X_df, y_df \n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "print('Training + Validation')\n",
    "model = RandomForest(n_estimators=n_estimators,\n",
    "                     max_depth=max_depth,\n",
    "                     criterion=criterion,\n",
    "                     parallel_degree=parallel_degree)\n",
    "model.fit_model(X=X_train,\n",
    "                y=y_train)\n",
    "model.predict_and_evaluate(X=X_validate,\n",
    "                           y=y_validate,\n",
    "                           lag=lag,\n",
    "                           plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "print('\\n\\nTraining + Testing')\n",
    "model.fit_model(X=X_validate,\n",
    "                y=y_validate)\n",
    "model.predict_and_evaluate(X=X_test,\n",
    "                           y=y_test,\n",
    "                           lag=lag,\n",
    "                           plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
