{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database.\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# keras\n",
    "import keras as ke\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.utils import np_utils\n",
    "print('keras: %s' % ke.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=3 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "#\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "y_label = ['SQL_ID'] # Denotes which label to use for time series experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3018: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "#rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "#\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (64912, 90)\n",
      "Shape After Aggregation: (53902, 78)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID','DBID','INSTANCE_NUMBER','SQL_ID']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "df = rep_hist_snapshot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53902, 78)\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=['SNAP_ID'], ascending=True, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging Labels\n",
    "\n",
    "Removes the label column, and adds it at the beginning of the matrix for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Column Switch: (53902, 78)\n",
      "After Column Switch: (53902, 1)\n",
      "           SQL_ID\n",
      "0   03ggjrmy0wa1w\n",
      "52  bj5v9w48937nu\n",
      "50  au8ztarrm6vvs\n",
      "49  agm13xcgfst01\n",
      "48  aggcw7yk1a7s6\n"
     ]
    }
   ],
   "source": [
    "print('Before Column Switch: ' + str(df.shape))\n",
    "df = df[y_label]\n",
    "print('After Column Switch: ' + str(df.shape))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before label encoding: (53902, 1)\n",
      "After label encoding: (53902,)\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "Total SQL_IDs: [53902]\n",
      "Total Vocab: [1046]\n",
      "['00ajuu5902429' '014qrkkyv7j56' '01d5n1nm17r2h' ... 'gyb8bxyuwym37'\n",
      " 'gypw3juu7w5bc' 'gyqkrz4r79a2t']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "### Label Encoding\n",
    "#\n",
    "# Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "print(\"Before label encoding: \" + str(df.shape))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[y_label])\n",
    "df = le.transform(df[y_label])\n",
    "print(\"After label encoding: \" + str(df.shape) + \"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "df = pd.DataFrame(df, columns=y_label)\n",
    "print(\"Total SQL_IDs: [\" + str(df.shape[0]) + \"]\")\n",
    "print(\"Total Vocab: [\" + str(len(le.classes_)) + \"]\")\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(53896, 4)\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(53896, 3)\n",
      "\n",
      "-------------\n",
      "Features After Time Shift\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(53896, 4)\n",
      "\n",
      "-------------\n",
      "Labels After Time Shift\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(53896, 3)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #     for header in headers:\n",
    "    #         if \"(t)\" in header:\n",
    "    #             dropped_headers.append(header)\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_row = []\n",
    "for i in range(lag+1,(lag*2)+2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns)*i:len(df.columns)*i + len(y_label)]\n",
    "    y_row.append(y_df_column_names)\n",
    "y_df_column_names = []   \n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "#\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "print('\\n-------------\\nLabels After Time Shift')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Features (X)\n",
    "\n",
    "Normalization through MinMax Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53896, 4)\n",
      "   var1(t-3)  var1(t-2)  var1(t-1)   var1(t)\n",
      "0   0.004785   0.722488   0.686124  0.670813\n",
      "1   0.722488   0.686124   0.670813  0.669856\n",
      "2   0.686124   0.670813   0.669856  0.647847\n",
      "3   0.670813   0.669856   0.647847  0.646890\n",
      "4   0.669856   0.647847   0.646890  0.723445\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_normalized_values = scaler.fit_transform(X_df.values)\n",
    "#\n",
    "X_df = pd.DataFrame(data=df_normalized_values, columns=X_df.columns)\n",
    "del df_normalized_values\n",
    "print(str(X_df.shape))\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding (y)\n",
    "\n",
    "One hot encoding target labels for net application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # One Hot Encoding\n",
    "# print(\"Before One Hot Encoding: \" + str(y_df.shape))\n",
    "# y_df = np_utils.to_categorical(y_df)\n",
    "# print(\"After One Hot Encoding: \" + str(y_df.shape))\n",
    "# print(y_df[:,0])\n",
    "# print(len(y_df[:,0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Regression (Many to Many Approach)\n",
    "### Designing the network\n",
    "\n",
    "- The first step is to define your network.\n",
    "- Neural networks are defined in Keras as a sequence of layers. The container for these layers is the **Sequential class**.\n",
    "- The first step is to create an instance of the Sequential class. Then you can create your layers and add them in the order that they should be connected.\n",
    "- The LSTM recurrent layer comprised of memory units is called LSTM().\n",
    "- A fully connected layer that often follows LSTM layers and is used for outputting a prediction is called Dense().\n",
    "- The first layer in the network must define the number of inputs to expect.\n",
    "- Input must be three-dimensional, comprised of samples, timesteps, and features.\n",
    "    - **Samples:** These are the rows in your data.\n",
    "    - **Timesteps:** These are the past observations for a feature, such as lag variables.\n",
    "    - **Features:** These are columns in your data.\n",
    "- Assuming your data is loaded as a NumPy array, you can convert a 2D dataset to a 3D dataset using the reshape() function in NumPy.\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "* https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "* https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/\n",
    "* https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "* https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "* https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8141873\n",
    "* https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# KerasModel Class\n",
    "class KerasModel:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    def __init__(self, X_train, y_train, optimizer, loss):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X_train - Training data used to train the model\n",
    "        :param y_train - Test data used to test the model\n",
    "        :param layers - A list of values, where in each value denotes a layer, and the number of neurons for that layer\n",
    "        :param loss_function - Function used to measure fitting of model (predicted from actual)\n",
    "        :param optimizer - Function used to optimize the model (eg: Gradient Descent)\n",
    "        \"\"\"\n",
    "        self.model = ke.models.Sequential()\n",
    "        self.model.add(ke.layers.LSTM(X_train[2], input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        self.model.add(ke.layers.Dropout(0.2))\n",
    "        self.model.add(ke.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    #\n",
    "    def fit_model(self, X_train, X_test, y_train, y_test, epochs=50, batch_size=50, verbose=2, shuffle=False,\n",
    "                  plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "        \"\"\"\n",
    "        history = self.model.fit(x=X_train,\n",
    "                                 y=y_train,\n",
    "                                 epochs=epochs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 validation_data=(X_test, y_test),\n",
    "                                 verbose=verbose,\n",
    "                                 shuffle=shuffle)\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(history.history['acc'], label='train')\n",
    "            plt.plot(history.history['val_acc'], label='validation')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, plot=False):\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # F1-Score Evaluation\n",
    "        for i in range(y.shape[1]):\n",
    "            f1 = f1_score(y[:, i], \n",
    "                          yhat[:, i],\n",
    "                          average='micro')  # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "            print('Test FScore ' + y_labels[0] + ' with LAG value [' + str(i) + ']: ' +  str(f1))\n",
    "        #\n",
    "        if plot:\n",
    "            for i in range(0, y[0]):\n",
    "                plt.rcParams['figure.figsize'] = [20, 15]\n",
    "                plt.plot(y[:, i], label='actual')\n",
    "                plt.plot(yhat[:, i], label='predicted')\n",
    "                plt.legend(['actual', 'predicted'], loc='upper left')\n",
    "                plt.title(y_labels[i%len(y_labels)] + \" +\" + str(math.ceil((i+1)/len(y_label))))\n",
    "                plt.show()\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, batch, depth, epoch, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'batch', 'depth', 'epoch', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'batch': batch,\n",
    "                             'depth':depth,\n",
    "                             'epoch':epoch,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape [(43116, 4)] Type - [<class 'numpy.ndarray'>]\n",
      "y_train shape [(43116, 3)] Type - [<class 'pandas.core.frame.DataFrame'>]\n",
      "X_validate shape [(5390, 4)] Type - [<class 'numpy.ndarray'>]\n",
      "y_validate shape [(5390, 3)] Type - [<class 'pandas.core.frame.DataFrame'>]\n",
      "X_test shape [(5390, 4)] Type - [<class 'numpy.ndarray'>]\n",
      "y_test shape [(5390, 3)] Type - [<class 'pandas.core.frame.DataFrame'>]\n",
      "\n",
      "\n",
      "[[0.09186603 0.06028708 0.03636364 0.01244019]\n",
      " [0.36650718 0.36076555 0.35885167 0.35598086]\n",
      " [0.76650718 0.74832536 0.73684211 0.72344498]\n",
      " [0.5291866  0.52631579 0.63732057 0.74258373]\n",
      " [0.44019139 0.40574163 0.40382775 0.48325359]]\n",
      "       var1(t+1)  var1(t+2)  var1(t+3)\n",
      "30300        3.0      138.0      303.0\n",
      "21205      370.0      399.0      770.0\n",
      "40141      745.0      715.0      689.0\n",
      "35058      829.0      801.0      803.0\n",
      "36871      399.0      383.0      377.0\n",
      "------------------------------------------------------------\n",
      "[[0.58373206 0.56076555 0.54736842 0.52631579]\n",
      " [0.88229665 0.87751196 0.85263158 0.84497608]\n",
      " [0.48421053 0.4708134  0.13875598 0.45645933]\n",
      " [0.94832536 0.79425837 0.49186603 0.93110048]\n",
      " [0.26889952 0.49760766 0.48708134 0.48133971]]\n",
      "       var1(t+1)  var1(t+2)  var1(t+3)\n",
      "39310      715.0      639.0      743.0\n",
      "12505      871.0      869.0      941.0\n",
      "45387       19.0       21.0       22.0\n",
      "31310      968.0      956.0      945.0\n",
      "50620      472.0      428.0      480.0\n",
      "------------------------------------------------------------\n",
      "[[0.91674641 0.93110048 0.94736842 0.94832536]\n",
      " [0.15502392 0.15406699 0.13205742 0.24210526]\n",
      " [0.72248804 0.76746411 0.74736842 0.73684211]\n",
      " [0.50717703 0.5062201  0.46889952 0.53014354]\n",
      " [0.6277512  0.60574163 0.79425837 0.68229665]]\n",
      "       var1(t+1)  var1(t+2)  var1(t+3)\n",
      "24354      992.0      808.0      514.0\n",
      "7478       113.0       96.0       71.0\n",
      "4412       756.0      713.0      570.0\n",
      "25955      756.0      467.0      823.0\n",
      "53283      841.0     1009.0      852.0\n",
      "\n",
      "Reshaping Training Frames\n",
      "X_train shape [(43116, 1, 4)] Type - [<class 'numpy.ndarray'>]\n",
      "X_validate shape [(5390, 1, 4)] Type - [<class 'numpy.ndarray'>]\n",
      "X_test shape [(5390, 1, 4)] Type - [<class 'numpy.ndarray'>]\n",
      "       var1(t+1)  var1(t+2)  var1(t+3)\n",
      "30300        3.0      138.0      303.0\n",
      "21205      370.0      399.0      770.0\n",
      "40141      745.0      715.0      689.0\n",
      "35058      829.0      801.0      803.0\n",
      "36871      399.0      383.0      377.0\n",
      "45800       71.0       63.0       60.0\n",
      "37267      597.0      713.0      666.0\n",
      "23388      666.0      664.0      633.0\n",
      "44243      370.0      321.0      313.0\n",
      "13908      719.0      713.0      671.0\n",
      "22741      281.0      288.0      497.0\n",
      "31143      256.0      498.0      520.0\n",
      "21119      173.0      183.0      239.0\n",
      "16327      922.0      550.0      995.0\n",
      "39017      384.0      383.0      472.0\n",
      "47044      922.0      945.0      956.0\n",
      "9040       548.0      377.0      505.0\n",
      "12707      949.0      972.0      995.0\n",
      "29401       71.0      481.0      505.0\n",
      "47729      138.0      183.0      239.0\n",
      "38111      770.0      734.0      814.0\n",
      "13810      239.0      110.0      254.0\n",
      "38706      991.0      855.0      868.0\n",
      "5113       149.0      154.0      160.0\n",
      "52509      355.0      391.0      213.0\n",
      "19087      460.0      428.0      424.0\n",
      "2048       656.0      633.0      614.0\n",
      "28260      586.0      572.0      756.0\n",
      "29913      263.0      239.0      236.0\n",
      "19696      570.0       38.0      548.0\n",
      "...          ...        ...        ...\n",
      "24563      144.0      140.0      138.0\n",
      "26446      138.0      113.0      110.0\n",
      "10541      922.0      948.0      950.0\n",
      "35680      303.0      113.0      324.0\n",
      "23132      855.0      917.0      973.0\n",
      "23568      404.0      399.0      383.0\n",
      "16299      731.0      724.0      695.0\n",
      "3402       263.0      303.0      321.0\n",
      "18617      471.0      460.0      431.0\n",
      "3684       521.0      507.0      506.0\n",
      "29321      869.0      871.0      883.0\n",
      "3706       570.0      554.0      781.0\n",
      "47673      263.0       63.0      303.0\n",
      "31720      431.0      429.0      428.0\n",
      "18128      883.0      917.0      922.0\n",
      "1442        13.0      263.0      260.0\n",
      "29666      855.0      852.0      830.0\n",
      "3720       830.0      941.0      891.0\n",
      "9356       990.0      991.0     1045.0\n",
      "7366       759.0      756.0      755.0\n",
      "45909      530.0      983.0      504.0\n",
      "7745        60.0      113.0      136.0\n",
      "39621      891.0      917.0      922.0\n",
      "19056      956.0      968.0      973.0\n",
      "11123      521.0      271.0      263.0\n",
      "52570      801.0      792.0      770.0\n",
      "37429      538.0      509.0      520.0\n",
      "18452      886.0      966.0      983.0\n",
      "8201       973.0      968.0      950.0\n",
      "27428      481.0      480.0      472.0\n",
      "\n",
      "[43116 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-32b6bccce869>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m                             \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                             \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                             loss='categorical_crossentropy')\n\u001b[0m\u001b[0;32m     46\u001b[0m discrete_model.fit_model(X_train=X_train,\n\u001b[0;32m     47\u001b[0m                          \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-9d6deb7e5bb5>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X_train, y_train, optimizer, loss)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \"\"\"\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    491\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;31m# set or validate state_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1866\u001b[0m                                       \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1868\u001b[1;33m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[0;32m   1869\u001b[0m         self.recurrent_kernel = self.add_weight(\n\u001b[0;32m   1870\u001b[0m             \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[0;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mscale\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mscale\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'normal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;31m# 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "X_train = X_train.values\n",
    "X_validate = X_validate.values\n",
    "#\n",
    "# X_train = X_train.values\n",
    "# y_train = y_train.values\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - [\" + str(type(y_train)) + \"]\")\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "#\n",
    "# X_validate = X_validate.values\n",
    "# y_validate = y_validate.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - [\" + str(type(y_validate)) + \"]\")\n",
    "#\n",
    "# X_test = X_test.values\n",
    "# y_test = y_test.values\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - [\" + str(type(y_test)) + \"]\")\n",
    "print(\"\\n\")\n",
    "#\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate[0:5])\n",
    "print(y_validate[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "#\n",
    "X_train = np.reshape(X_train,(X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_validate = np.reshape(X_validate,(X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "X_test = np.reshape(X_test,(X_test.shape[0], 1, X_test.shape[1]))\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "print(y_train)\n",
    "discrete_model = KerasModel(X_train=X_train,\n",
    "                            y_train=y_train,\n",
    "                            optimizer='adam',\n",
    "                            loss='categorical_crossentropy')\n",
    "discrete_model.fit_model(X_train=X_train,\n",
    "                         X_test=X_validate,\n",
    "                         y_train=y_train,\n",
    "                         y_test=y_validate,\n",
    "                         epochs=10, \n",
    "                         batch_size=128,\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_validate,\n",
    "                                    y=y_validate,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "discrete_model.fit_model(X_train=X_validate,\n",
    "                         X_test=X_test,\n",
    "                         y_train=y_validate,\n",
    "                         y_test=y_test,\n",
    "                         epochs=10, \n",
    "                         batch_size=1, # Incremental batch size fitting\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_test,\n",
    "                                    y=y_test,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
