{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis (Deep Learning)\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# keras\n",
    "import keras as ke\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.utils import np_utils\n",
    "print('keras: %s' % ke.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=3 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "#\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "#\n",
    "# Net Config\n",
    "epochs=10\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3018: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "#rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "#\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (64912, 90)\n",
      "Shape After Aggregation: (820, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     28190  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     28191  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2     28192  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     28193  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4     28194  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5     28195  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6     28196  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7     28197  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     28198  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9     28199  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "10    28200  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "11    28201  [06g9mhm5ba7tt, 09vrdx888wvvb, 0kcbwucxmazcp, ...\n",
      "12    28202  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    28203  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "14    28204  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "15    28205  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "16    28206  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "17    28207  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "18    28208  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "19    28209  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "20    28210  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "21    28211  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "22    28212  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "23    28213  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "24    28214  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "25    28215  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "26    28216  [06dymzb481vnd, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "27    28217  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "28    28218  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "29    28219  [06g9mhm5ba7tt, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "..      ...                                                ...\n",
      "70    28260  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "71    28261  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "72    28262  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "73    28263  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "74    28264  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "75    28265  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "76    28266  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "77    28267  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "78    28268  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "79    28269  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "80    28270  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    28271  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "82    28272  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "83    28273  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "84    28274  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "85    28275  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "86    28276  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "87    28277  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "88    28278  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "89    28279  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1p5grz1gs7fjq, ...\n",
      "90    28280  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "91    28281  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "92    28282  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "93    28283  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "94    28284  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "95    28285  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "96    28286  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "97    28287  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "98    28288  [0aadu2kp270dv, 0gyb2wqdu4d9k, 0kkhhb2w93cx0, ...\n",
      "99    28289  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "df = rep_hist_snapshot_df.groupby(['SNAP_ID'])['SQL_ID'].apply(list).reset_index()\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(df.shape))\n",
    "print(type(df))\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 2)\n",
      "    SNAP_ID                                             SQL_ID\n",
      "0     28190  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1     28191  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2     28192  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3     28193  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4     28194  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5     28195  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6     28196  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7     28197  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8     28198  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9     28199  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "10    28200  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "11    28201  [06g9mhm5ba7tt, 09vrdx888wvvb, 0kcbwucxmazcp, ...\n",
      "12    28202  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "13    28203  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "14    28204  [06dymzb481vnd, 0y080mnfaqk3u, 0y080mnfaqk3u, ...\n",
      "15    28205  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "16    28206  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "17    28207  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "18    28208  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "19    28209  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "20    28210  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "21    28211  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "22    28212  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "23    28213  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "24    28214  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "25    28215  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "26    28216  [06dymzb481vnd, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "27    28217  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "28    28218  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "29    28219  [06g9mhm5ba7tt, 0jj0ct4x4gy27, 0jj0ct4x4gy27, ...\n",
      "..      ...                                                ...\n",
      "70    28260  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "71    28261  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "72    28262  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "73    28263  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "74    28264  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "75    28265  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "76    28266  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0t9xqgg9n3yws, ...\n",
      "77    28267  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "78    28268  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "79    28269  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "80    28270  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "81    28271  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "82    28272  [062savj8zgzut, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "83    28273  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "84    28274  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "85    28275  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "86    28276  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "87    28277  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "88    28278  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "89    28279  [0kcbwucxmazcp, 0kkhhb2w93cx0, 1p5grz1gs7fjq, ...\n",
      "90    28280  [09vrdx888wvvb, 0a7q9v9nd2qc1, 0hhmdwwgxbw0r, ...\n",
      "91    28281  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "92    28282  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "93    28283  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "94    28284  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "95    28285  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "96    28286  [01tp87bk1t2zv, 01tp87bk1t2zv, 06dymzb481vnd, ...\n",
      "97    28287  [0kcbwucxmazcp, 0kkhhb2w93cx0, 0w26sk6t6gq98, ...\n",
      "98    28288  [0aadu2kp270dv, 0gyb2wqdu4d9k, 0kkhhb2w93cx0, ...\n",
      "99    28289  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.sort_index(ascending=True,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection\n",
    "\n",
    "This sextion treats the dataset as a univariate dataset. Therefore the SNAP_ID pertaining to each set of SQL_IDs is removed, with the intent of future classifiers training solely on past SQL executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 2)\n",
      "(820, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "del df['SNAP_ID']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \"\"\"\n",
    "    Scikit Label Encoder was acting up with the following error whilst using the transform function, even though I tripled \n",
    "    checked that the passed data was exactly the same as the one used for training:\n",
    "    \n",
    "    * https://stackoverflow.com/questions/46288517/getting-valueerror-y-contains-new-labels-when-using-scikit-learns-labelencoder\n",
    "    \n",
    "    So I have rebuilt a similar functionality to categorize my data into numeric digits, as the LabelEncoder is supposed to do.\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__class_map = {}\n",
    "        self.__integer_counter = 0\n",
    "    #\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        :param - X: python list\n",
    "        \"\"\"\n",
    "        for val in X:\n",
    "            if val not in self.__class_map:\n",
    "                self.__class_map[val] = self.__integer_counter\n",
    "                self.__integer_counter += 1\n",
    "    #\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        param - X: python list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for val in X:\n",
    "            if val in self.__class_map:\n",
    "                value = self.__class_map[val]\n",
    "                encoded_map.append(value)\n",
    "            else:\n",
    "                raise ValueError('Label Mismatch - Encountered a label which was not trained on.')\n",
    "        return encoded_map\n",
    "    #\n",
    "    def get_class_map(self):\n",
    "        \"\"\"\n",
    "        Returns original classes as a list\n",
    "        \"\"\"\n",
    "        class_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            class_map.append(key)\n",
    "        return class_map\n",
    "    #\n",
    "    def get_encoded_map(self):\n",
    "        \"\"\"\n",
    "        Returns class encodings as a list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            encoded_map.append(value)\n",
    "        return encoded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 1)\n",
      "                                              SQL_ID\n",
      "0  [03ggjrmy0wa1w, 06dymzb481vnd, 0aq14dznn91rg, ...\n",
      "1  [04kug40zbu4dm, 0a08ug2qc1j82, 0a08ug2qc1j82, ...\n",
      "2  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "3  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "4  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "5  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "6  [06g9mhm5ba7tt, 0kcbwucxmazcp, 0kkhhb2w93cx0, ...\n",
      "7  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "8  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "9  [01tp87bk1t2zv, 01tp87bk1t2zv, 01tp87bk1t2zv, ...\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "Total SQL_ID Classes: 1046\n",
      "['03ggjrmy0wa1w', '06dymzb481vnd', '0aq14dznn91rg', '0f60bzgt9127c', '0ga8vk4nftz45', '13a9r2xkx1bxb', '13ys8ux8xvrbm', '14f5ngrj3cc5h', '1jhyrdp21f2q6', '1p5grz1gs7fjq']\n",
      "(820, 1)\n",
      "                                              SQL_ID\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
      "1  [71, 72, 72, 73, 73, 73, 74, 75, 76, 77, 78, 9...\n",
      "2  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "3  [134, 134, 134, 1, 72, 72, 73, 73, 73, 135, 75...\n",
      "4  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "5  [167, 168, 169, 170, 9, 171, 172, 173, 174, 17...\n",
      "6  [167, 168, 169, 170, 215, 9, 216, 217, 171, 21...\n",
      "7  [134, 134, 134, 1, 169, 243, 73, 73, 73, 135, ...\n",
      "8  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n",
      "9  [134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78...\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "le = LabelEncoder()\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    le.fit(sql_id_list)\n",
    "for index, row in df.iterrows():\n",
    "    sql_id_list = row['SQL_ID']\n",
    "    transformed_list = le.transform(sql_id_list)\n",
    "    df['SQL_ID'].iloc[index] = transformed_list \n",
    "#\n",
    "print(\"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "print('Total SQL_ID Classes: ' + str(len(le.get_class_map())))\n",
    "print(le.get_class_map()[:10])\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "A note regarding normalization. Normalization for this experiment was purposely skipped, since training & testing data will be one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Padding\n",
    "\n",
    "Since there isn't a fixed number of SQL_ID's per SNAP_ID, each set of SQL_IDs need to be padded so as to assume an equal number if SQL_IDs for the purpose of model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length at index 0: 80\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 52, 52, 52, 52, 53, 54, 55, 55, 55, 56, 57, 58, 59, 59, 59, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
      "Length at index 1: 81\n",
      "[71, 72, 72, 73, 73, 73, 74, 75, 76, 77, 78, 9, 79, 80, 81, 82, 83, 84, 85, 86, 24, 87, 88, 88, 89, 90, 91, 92, 93, 93, 94, 95, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 111, 53, 112, 113, 114, 115, 115, 116, 117, 118, 119, 119, 119, 119, 120, 120, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133]\n",
      "Length at index 2: 91\n",
      "[134, 134, 134, 1, 73, 73, 73, 135, 75, 76, 78, 9, 79, 136, 136, 83, 85, 24, 88, 88, 89, 91, 92, 93, 93, 94, 95, 95, 96, 97, 100, 101, 137, 138, 139, 139, 139, 102, 103, 140, 141, 104, 105, 142, 143, 144, 107, 108, 108, 109, 110, 52, 52, 52, 52, 52, 53, 112, 113, 55, 55, 55, 115, 115, 116, 145, 117, 59, 59, 59, 59, 118, 119, 119, 119, 119, 120, 120, 120, 121, 122, 63, 126, 126, 128, 129, 146, 130, 132, 132, 147]\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "Length at index 0: 109\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 52 52 52 52 53 54 55 55 55 56 57 58 59 59 59 59 60 61 62\n",
      " 63 64 65 66 67 68 69 70 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Length at index 1: 109\n",
      "[ 71  72  72  73  73  73  74  75  76  77  78   9  79  80  81  82  83  84\n",
      "  85  86  24  87  88  88  89  90  91  92  93  93  94  95  95  96  97  98\n",
      "  98  99 100 101 102 103 104 105 106 107 108 108 109 110 111  53 112 113\n",
      " 114 115 115 116 117 118 119 119 119 119 120 120 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 132 133  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1]\n",
      "Length at index 2: 109\n",
      "[134 134 134   1  73  73  73 135  75  76  78   9  79 136 136  83  85  24\n",
      "  88  88  89  91  92  93  93  94  95  95  96  97 100 101 137 138 139 139\n",
      " 139 102 103 140 141 104 105 142 143 144 107 108 108 109 110  52  52  52\n",
      "  52  52  53 112 113  55  55  55 115 115 116 145 117  59  59  59  59 118\n",
      " 119 119 119 119 120 120 120 121 122  63 126 126 128 129 146 130 132 132\n",
      " 147  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "  -1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])\n",
    "#\n",
    "# Retrieve largest length\n",
    "def pad_datamatrix(df):\n",
    "    \"\"\"\n",
    "    Iterates over dataframe and pads SQL_ID lists accordingly with -1 values\n",
    "    \"\"\"\n",
    "    row_sizes = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_sizes.append(len(row['SQL_ID']))\n",
    "    max_row_size = max(row_sizes)\n",
    "    #\n",
    "    # Pad Dataframe Values\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        length = len(row['SQL_ID'])\n",
    "        diff = max_row_size - length\n",
    "        if diff != 0:\n",
    "            for j in range(length, max_row_size):\n",
    "                df['SQL_ID'].iloc[i] = np.append(df['SQL_ID'].iloc[i], -1) # Appends -1 to padded values\n",
    "        # print(\"Length at index \" + str(i) + \": \" + str(df['SQL_ID'].iloc[i].size))\n",
    "        i += 1\n",
    "    return df\n",
    "#\n",
    "df = pad_datamatrix(df)\n",
    "#\n",
    "print('\\n\\n------------------------------------------\\n\\n')\n",
    "print(\"Length at index 0: \" + str(len(df['SQL_ID'].iloc[0])))\n",
    "print(df['SQL_ID'].iloc[0])\n",
    "print(\"Length at index 1: \" + str(len(df['SQL_ID'].iloc[1])))\n",
    "print(df['SQL_ID'].iloc[1])\n",
    "print(\"Length at index 2: \" + str(len(df['SQL_ID'].iloc[2])))\n",
    "print(df['SQL_ID'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(814, 4)\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(814, 3)\n",
      "\n",
      "-------------\n",
      "Features After Time Shift\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(814, 4)\n",
      "\n",
      "-------------\n",
      "Labels After Time Shift\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(814, 3)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_row = []\n",
    "for i in range(lag+1,(lag*2)+2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns)*i:len(df.columns)*i + 1]\n",
    "    y_row.append(y_df_column_names)\n",
    "y_df_column_names = []   \n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "#\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "print('\\n-------------\\nLabels After Time Shift')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Feature Lists\n",
    "\n",
    "Expand Feature Lists, where in each list element is represented as it's own features. Total feature count here equates as follows:\n",
    "\n",
    "* Features = (lag * SQL_ID per SNAP_ID count) + SQL_ID per SNAP_ID count\n",
    "* Labels = lag * SQL_ID per SNAP_ID count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "Before: (814, 4)\n",
      "After: (814, 436)\n",
      "Labels\n",
      "Before: (814, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: (814, 327)\n"
     ]
    }
   ],
   "source": [
    "def sequence2features(df):\n",
    "    \"\"\"\n",
    "    Converts pandas sequences into full fledged columns/features\n",
    "    \"\"\"\n",
    "    feature_count = len(df[df.columns[0]].iloc[0])\n",
    "    for column_name in df.columns:\n",
    "        data_matrix = []\n",
    "        new_values = df[column_name].values\n",
    "        #\n",
    "        new_values = np.stack(new_values, axis=0 )\n",
    "        #\n",
    "        for i in range(1,feature_count+1):\n",
    "            new_column_name = column_name + \"_\"+str(i)\n",
    "            df[new_column_name] = new_values[:,i-1]\n",
    "        #\n",
    "        # Drop original list columns\n",
    "        df.drop(column_name, inplace=True, axis=1)\n",
    "    return df\n",
    "#\n",
    "print('Features')\n",
    "print('Before: ' + str(X_df.shape))\n",
    "X_df = sequence2features(df=X_df)\n",
    "print('After: ' + str(X_df.shape))\n",
    "#\n",
    "print('Labels')\n",
    "print('Before: ' + str(y_df.shape))\n",
    "y_df = sequence2features(df=y_df)\n",
    "print('After: ' + str(y_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "One hot encoding target labels for deep learning application\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Before One Hot Encoding: (814, 436)\n",
      "After One Hot Encoding: (814, 436, 1044)\n",
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "\n",
      "Target Data:\n",
      "Before One Hot Encoding: (814, 327)\n",
      "After One Hot Encoding: (814, 327, 1046)\n",
      "\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "y_labels = y_df.columns\n",
    "#\n",
    "# One Hot Encoding train data\n",
    "print('Training Data:')\n",
    "print(\"Before One Hot Encoding: \" + str(X_df.shape))\n",
    "X_df = np_utils.to_categorical(X_df.values)\n",
    "print(\"After One Hot Encoding: \" + str(X_df.shape))\n",
    "print(X_df)\n",
    "#\n",
    "# One Hot Encoding target labels\n",
    "print('\\nTarget Data:')\n",
    "print(\"Before One Hot Encoding: \" + str(y_df.shape))\n",
    "y_df = np_utils.to_categorical(y_df.values)\n",
    "print(\"After One Hot Encoding: \" + str(y_df.shape) + \"\\n\")\n",
    "print(y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model (Many to Many Approach)\n",
    "\n",
    "### Designing the network\n",
    "\n",
    "The Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.\n",
    "\n",
    "There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.\n",
    "\n",
    "The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our SQL_ID dataset, the output layer must create n output values, one for each SQL_ID class. The output value with the largest value will be taken as the class predicted by the model.\n",
    "\n",
    "The network topology of this simple one-layer neural network can be summarized as:\n",
    "\n",
    "* n_inputs = Number of input neurons dependent on SQL_ID Features (Vector Width)\n",
    "* n_hidden = Number of hidden layer neurons (This will almost be the same as n_inputs, pending testing)\n",
    "* n_output = Number of output neurons dependent on SQL_ID Classes.\n",
    "\n",
    "n_inputs -> [n_hidden] -> n_output\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8141873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# KerasModel Class\n",
    "class KerasModel:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    def __init__(self, X_train, y_train, optimizer, loss, vector_width, output_classes):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X_train - Training data used to train the model\n",
    "        :param y_train - Test data used to test the model\n",
    "        :param layers - A list of values, where in each value denotes a layer, and the number of neurons for that layer\n",
    "        :param loss_function - Function used to measure fitting of model (predicted from actual)\n",
    "        :param optimizer - Function used to optimize the model (eg: Gradient Descent)\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        #self.model.add(Dense(vector_width, input_dim=vector_width, activation='relu'))\n",
    "        self.model.add(Dense(vector_width, input_shape=(vector_width,1), activation='relu'))\n",
    "        self.model.add(Dense(output_classes, activation='softmax'))\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    #\n",
    "    def fit_model(self, X_train, X_test, y_train, y_test, epochs=50, batch_size=50, verbose=2, shuffle=False,\n",
    "                  plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "        \"\"\"\n",
    "        history = self.model.fit(x=X_train,\n",
    "                                 y=y_train,\n",
    "                                 epochs=epochs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 validation_data=(X_test, y_test),\n",
    "                                 verbose=verbose,\n",
    "                                 shuffle=shuffle)\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(history.history['acc'], label='train')\n",
    "            plt.plot(history.history['val_acc'], label='validation')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, lag, plot=False):\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # F1-Score Evaluation\n",
    "        feature_count = int(y.shape[1] / lag)\n",
    "        acc_score, f_score = .0,.0\n",
    "        counter = 0\n",
    "        for i in range(y.shape[1]):\n",
    "            acc = accuracy_score(y[:,i], yhat[:,i])\n",
    "            acc_score += acc\n",
    "            f1 = f1_score(y[:,i], yhat[:,i], average='micro') \n",
    "            f_score += f1\n",
    "            if i % (feature_count-1) == 0 and i != 0:\n",
    "                counter += 1\n",
    "                acc_score = acc_score/feature_count # Averaging accuracy accross total sql_ids for that particular timestep\n",
    "                f_score = f_score/feature_count # Averaging f-score accross total sql_ids for that particular timestep\n",
    "                print('Test Accuracy ' + y_labels[i] + ' with LAG value [' + str(counter) + ']: ' +  str(acc_score))\n",
    "                print('Test FScore ' + y_labels[i] + ' with LAG value [' + str(counter) + ']: ' +  str(f_score) + \"\\n\")\n",
    "                acc_score, f_score = .0,.0\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, batch, depth, epoch, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'batch', 'depth', 'epoch', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'batch': batch,\n",
    "                             'depth':depth,\n",
    "                             'epoch':epoch,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape [(651, 436, 1044)] Type - [<class 'numpy.ndarray'>]\n",
      "y_train shape [(651, 327, 1046)] Type - [<class 'numpy.ndarray'>]\n",
      "X_validate shape [(81, 436, 1044)] Type - [<class 'numpy.ndarray'>]\n",
      "y_validate shape [(81, 327, 1046)] Type - [<class 'numpy.ndarray'>]\n",
      "X_test shape [(82, 436, 1044)] Type - [<class 'numpy.ndarray'>]\n",
      "y_test shape [(82, 327, 1046)] Type - [<class 'numpy.ndarray'>]\n",
      "\n",
      "\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "------------------------------------------------------------\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "------------------------------------------------------------\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 296324784 into shape (651,1,436)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    277\u001b[0m            [5, 6]])\n\u001b[0;32m    278\u001b[0m     \"\"\"\n\u001b[1;32m--> 279\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 296324784 into shape (651,1,436)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "#X_train = X_train.values\n",
    "#X_validate = X_validate.values\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - [\" + str(type(y_train)) + \"]\")\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "#\n",
    "# X_validate = X_validate.values\n",
    "# y_validate = y_validate.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - [\" + str(type(y_validate)) + \"]\")\n",
    "#\n",
    "# X_test = X_test.values\n",
    "# y_test = y_test.values\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - [\" + str(type(y_test)) + \"]\")\n",
    "print(\"\\n\")\n",
    "#\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate[0:5])\n",
    "print(y_validate[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "#\n",
    "X_train = np.reshape(X_train,(X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_validate = np.reshape(X_validate,(X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "X_test = np.reshape(X_test,(X_test.shape[0], 1, X_test.shape[1]))\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "print(y_train)\n",
    "discrete_model = KerasModel(X_train=X_train,\n",
    "                            y_train=y_train,\n",
    "                            optimizer='adam',\n",
    "                            loss='categorical_crossentropy',\n",
    "                            vector_width=y_df.shape[1],\n",
    "                            output_classes=len(le.get_class_map()))\n",
    "discrete_model.fit_model(X_train=X_train,\n",
    "                         X_test=X_validate,\n",
    "                         y_train=y_train,\n",
    "                         y_test=y_validate,\n",
    "                         epochs=10, \n",
    "                         batch_size=128,\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_validate,\n",
    "                                    y=y_validate,\n",
    "                                    y_labels=y_label,\n",
    "                                    lag=lag,\n",
    "                                    plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "discrete_model.fit_model(X_train=X_validate,\n",
    "                         X_test=X_test,\n",
    "                         y_train=y_validate,\n",
    "                         y_test=y_test,\n",
    "                         epochs=10, \n",
    "                         batch_size=1, # Incremental batch size fitting\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_test,\n",
    "                                    y=y_test,\n",
    "                                    y_labels=y_label,\n",
    "                                    lag=lag,\n",
    "                                    plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
