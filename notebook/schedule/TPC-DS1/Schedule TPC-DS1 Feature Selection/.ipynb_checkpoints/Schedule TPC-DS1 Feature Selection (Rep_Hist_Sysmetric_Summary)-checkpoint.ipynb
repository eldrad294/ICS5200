{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule TPC-DS 1 Feature Selection (REP_HIST_SYSMETRIC_SUMMARY)\n",
    "\n",
    "This notebook is dedicated to dataset profiling. In this notebook, feature selection techniques will be implemented so as to categorize which features belay the most information to address the problem at hand - Workload Prediction. Due to the vast feature space which have been gathered during a workload's execution, manual techniques at determining which are most detrimental is not sufficient. \n",
    "\n",
    "Therefore the following work puts emphasis on automated techniques so as to determine out of the vast feature space which are most important to base future models upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, RFE\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment\n",
    "* tpcds - Schema upon which to operate test\n",
    "* debug_mode - Determines whether to plot graphs or not, useful for development purposes\n",
    "* low_quartile_limit - Lower Quartile threshold to detect outliers\n",
    "* upper_quartile_limit - Upper Quartile threshold to detect outliers\n",
    "* test_split - Denotes which Data Split to operate under when it comes to training / validation\n",
    "* nrows - Number of rows to read from csv file\n",
    "* top_n_features - Number of top features to focus on\n",
    "* parallel_degree - Number of parallel threads to train models with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Config\n",
    "tpcds='TPCDS1'\n",
    "debug_mode=True\n",
    "low_quartile_limit = 0\n",
    "upper_quartile_limit = 1\n",
    "test_split=.3\n",
    "nrows=None\n",
    "top_n_features=30\n",
    "parallel_degree=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2352558540b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mrep_hist_sysmetric_summary_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_sysmetric_summary_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mrep_hist_sysstat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_hist_sysstat_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mrep_vsql_plan_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_vsql_plan_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprettify_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Root path\n",
    "#root_dir = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds\n",
    "root_dir = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds\n",
    "\n",
    "# Open Data\n",
    "rep_hist_sysmetric_summary_path = root_dir + '/rep_hist_sysmetric_summary.csv'\n",
    "\n",
    "rep_hist_sysmetric_summary_df = pd.read_csv(rep_hist_sysmetric_summary_path,nrows=nrows)\n",
    "\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "\n",
    "rep_hist_sysmetric_summary_df.columns = prettify_header(rep_hist_sysmetric_summary_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "\n",
    "The correlation of resources consumed (y) per snapshot (X) define our feature space. Since the objective here is to attempt to predict what resources will be incurred ahead of time, the problem can be defined as a number of questions:\n",
    "\n",
    "* Q: What resources can I predict to be in usage at point N in time?\n",
    "* Q: What resources should I be predicting that accurately portray a schedule's workload?\n",
    "* Q: What knowledge/data do I have ahead of time which I can use to base my predictions off?\n",
    "\n",
    "Due to the vast feature space in the available metrics monitored and captured during a workload's execution, it is important to rank which attribute is most beneficial than others. Additionally, it is important to analyze such features individually, and considerate of other features in two types of analysis:\n",
    "\n",
    "* Univariate Analysis\n",
    "* Multivariate Analysis\n",
    "\n",
    "Furthermore, multiple types of feature ranking / analysis techniques ara available, amongst which will be considered:\n",
    "\n",
    "* Filter Methods\n",
    "* Wrapper Methods\n",
    "* Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We apply a number of preprocessing techniques to the presented dataframes, particularly to normalize and/or scale feature vectors into a more suitable representation for downstream estimators:\n",
    "\n",
    "Relative Links:\n",
    "* http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "* https://machinelearningmastery.com/improve-model-accuracy-with-data-pre-processing/\n",
    "* https://machinelearningmastery.com/normalize-standardize-time-series-data-python/\n",
    "\n",
    "### Table Pivots\n",
    "\n",
    "To better handle the following table, a number of table pivots are made on tables:\n",
    "* rep_hist_sysmetric_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Table REP_HIST_SYSMETRIC_SUMMARY\n",
    "rep_hist_sysmetric_summary_df = rep_hist_sysmetric_summary_df.pivot(index='SNAP_ID', columns='METRIC_NAME', values='AVERAGE')\n",
    "rep_hist_sysmetric_summary_df.reset_index(inplace=True)\n",
    "print(rep_hist_sysmetric_summary_df.columns)\n",
    "rep_hist_sysmetric_summary_df[['SNAP_ID']] = rep_hist_sysmetric_summary_df[['SNAP_ID']].astype(int)\n",
    "rep_hist_sysmetric_summary_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "print(\"REP_HIST_SYSMETRIC Shape: \" + str(rep_hist_sysmetric_summary_df.shape))\n",
    "\n",
    "# Refreshing columns with pivoted columns\n",
    "def convert_list_to_upper(col_list):\n",
    "    \"\"\"\n",
    "    Takes a string and converts elements to upper\n",
    "    \"\"\"\n",
    "    upper_col_list = []\n",
    "    for col in col_list:\n",
    "        upper_col_list.append(col.upper())\n",
    "    return upper_col_list\n",
    "\n",
    "rep_hist_sysmetric_summary_df.rename(str.upper, inplace=True, axis='columns')\n",
    "\n",
    "rep_hist_sysmetric_summary_headers = list(rep_hist_sysmetric_summary_df.columns)\n",
    "\n",
    "# DF Shape\n",
    "print('Table [REP_HIST_SYSMETRIC_SUMMARY] - ' + str(rep_hist_sysmetric_summary_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN Values\n",
    "\n",
    "Checking dataframes for potential missing values/data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "\n",
    "print(\"Table REP_HIST_SYSMETRIC_SUMMARY: \" + str(get_na_columns(df=rep_hist_sysmetric_summary_df,headers=rep_hist_sysmetric_summary_headers)) + \"\\n\\n\")\n",
    "\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Populating NaN values with amount '0'\n",
    "rep_hist_sysmetric_summary_df = fill_na(df=rep_hist_sysmetric_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Negative Values\n",
    "\n",
    "A function which retrieves a count per column for nay negative values it might contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def count_neg_df(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns with respective negative value count\n",
    "    \"\"\"\n",
    "    neg_list = []\n",
    "    for head in headers:\n",
    "        count = 0\n",
    "        try:\n",
    "            count = sum(n < 0 for n in df[head].values.flatten())\n",
    "        except Exception:\n",
    "            pass\n",
    "            #print('Non numeric column [' + head + ']')\n",
    "        if count > 0:\n",
    "            neg_list.append([head,count])\n",
    "    return neg_list\n",
    "\n",
    "def fill_neg(df, headers):\n",
    "    \"\"\"\n",
    "    Sets any data anomilies resulting in negative values to 0\n",
    "    \n",
    "    :param headers: list as follows eg: ['column_name', 'negative_count']\n",
    "    \"\"\"\n",
    "    for head in headers:\n",
    "        try:\n",
    "            df[df[head[0]] < 0] = 0\n",
    "        except Exception:\n",
    "            pass\n",
    "            #print('Non numeric column [' + head + ']')\n",
    "    return df\n",
    "\n",
    "# Check For Negative Values within dataframes\n",
    "print('---------------WITH NEGATIVE VALUES---------------')\n",
    "print(\"Table REP_HIST_SYSMETRIC_SUMMARY: \" + str(count_neg_df(df=rep_hist_sysmetric_summary_df,headers=rep_hist_sysmetric_summary_headers)) + \"\\n\\n\")\n",
    "\n",
    "# Replace Negative Values with a minimal threshold of 0\n",
    "rep_hist_sysmetric_summary_df = fill_neg(df=rep_hist_sysmetric_summary_df,headers=count_neg_df(df=rep_hist_sysmetric_summary_df,headers=rep_hist_sysmetric_summary_headers))\n",
    "\n",
    "# Check For Negative Values within dataframes\n",
    "print('\\n\\n---------------WITHOUT NEGATIVE VALUES---------------')\n",
    "print(\"Table REP_HIST_SYSMETRIC_SUMMARY: \" + str(count_neg_df(df=rep_hist_sysmetric_summary_df,headers=rep_hist_sysmetric_summary_headers)) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "\n",
    "rep_hist_sysmetric_summary_df = drop_flatline_columns(df=rep_hist_sysmetric_summary_df)\n",
    "\n",
    "rep_hist_sysmetric_summary_headers = rep_hist_sysmetric_summary_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Feature Distribution & Skewness\n",
    "\n",
    "In order to decide between a normalization strategy, it is important to understand the underlying data spread. Understanding of dataset mean, variance, skewness on a per column/feature basis helps determine whether a standardization or normalization strategy should be utilized on the datasets.\n",
    "\n",
    "### Plotting Data Distribution\n",
    "\n",
    "To better decide which normalization technique ought to be utilized for the technique at hand, a number of feature columns will be plotted as histograms to better convey the distribution spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(df=None, tpc_type=None, table=None, feature_column=None, bin_size=10):\n",
    "    \"\"\"\n",
    "    Plots histogram distribution\n",
    "    \"\"\"\n",
    "    #\n",
    "    try:\n",
    "        df['SNAP_ID'] = df['SNAP_ID'].astype(float)\n",
    "        df[feature_column] = df[feature_column].astype(float)\n",
    "        #\n",
    "        max_val = df[feature_column].max()\n",
    "        start_snap, end_snap = int(df['SNAP_ID'].min()), int(df['SNAP_ID'].max())\n",
    "        #\n",
    "        df[feature_column].hist(bins=10,figsize=(12,8))\n",
    "        plt.ylabel(feature_column)\n",
    "        plt.xlabel('Bin Ranges Of ' + str(int(max_val/bin_size)))\n",
    "        plt.title(tpc_type + ' Table ' + table.upper() + '.' + str(feature_column) + \" between \" + str(start_snap) + \" - \" + str(end_snap))\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        print('Could not plot column: ' + feature_column)\n",
    "\n",
    "def plot_scatter(df=None, tpc_type=None, table=None, feature_column=None):\n",
    "    \"\"\"\n",
    "    Plots scatter plots vs SNAP_ID\n",
    "    \"\"\"\n",
    "    #\n",
    "    try:\n",
    "        df['SNAP_ID'] = df['SNAP_ID'].astype(int)\n",
    "        df[feature_column] = df[feature_column].astype(int)\n",
    "        start_snap, end_snap = int(df['SNAP_ID'].min()), int(df['SNAP_ID'].max())\n",
    "        #\n",
    "        df.plot.scatter(x='SNAP_ID',\n",
    "                        y=feature_column,\n",
    "                        figsize=(12,8))\n",
    "        plt.ylabel(feature_column)\n",
    "        plt.xlabel('SNAP ID')\n",
    "        plt.title(tpc_type + ' Table ' + table.upper() + '.' + str(feature_column) + \" between \" + str(start_snap) + \" - \" + str(end_snap))\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        print('Could not plot column: ' + feature_column)\n",
    "\n",
    "def plot_boxplot(df=None, tpc_type=None, table=None, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Plots quartile plots to estimate mean and sigma (std dev)\n",
    "    \"\"\"\n",
    "    #\n",
    "    try:\n",
    "        for feature_column in feature_columns:\n",
    "            df[feature_column] = df[feature_column].astype(int)\n",
    "        df.boxplot(column=feature_columns, figsize=(12,8), grid=True)\n",
    "        plt.title(tpc_type + ' ' + str(feature_columns))\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        print('Could not plot column: ' + feature_column)\n",
    "\n",
    "if debug_mode is False:\n",
    "    \n",
    "    # Plotting Histograms of data distribution\n",
    "    for header in rep_hist_sysmetric_summary_headers:\n",
    "        print('REP_HIST_SYSMETRIC_SUMMARY - ' + header + ' - OUTLIERS HISTOGRAM')\n",
    "        plot_hist(df=rep_hist_sysmetric_summary_df, tpc_type=tpcds, table='rep_hist_sysmetric_summary', feature_column=header, bin_size=10)\n",
    "    \n",
    "    # Plotting Scatter Plots of data distribution\n",
    "    for header in rep_hist_sysmetric_summary_headers:\n",
    "        print('REP_HIST_SYSMETRIC_SUMMARY - ' + header + ' - OUTLIERS SCATTER')\n",
    "        plot_scatter(df=rep_hist_sysmetric_summary_df, tpc_type=tpcds,table='rep_hist_sysmetric_summary',feature_column=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Handling\n",
    "\n",
    "https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/\n",
    "\n",
    "As can be appreciated from the previous plots, data is heavily skewed on particular (smallest) bins. This skew in the plotted histograms is a result of data point outliers - these need to be evaluated and removed if neccessary.\n",
    "\n",
    "Following the 3 Standard Deviation Rule, we can categorize our dataset into subsets consisting of the following ranges:\n",
    "* 0     - 68.27%\n",
    "* 68.28 - 95.45%\n",
    "* 95.46 - 99.73%\n",
    "* 99.74 - 100%\n",
    "\n",
    "It should be mentioned, that given the time series nature of the dataset, it is not a safe assumption to ignore outliers. By training respective models on outlier insensitive dataset, we would invite a potential problem, which risks blinding any models we train to future predicted spikes of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_outliers(df=None, low_quartile_limit=.01, upper_quartile_limit=.99, headers=None):\n",
    "    \"\"\"\n",
    "    Detect and return which rows are considered outliers within the dataset, determined by :quartile_limit (99%)\n",
    "    \"\"\"\n",
    "    outlier_rows = [] # This list of lists consists of elements of the following notation [column,rowid]\n",
    "    for header in headers:\n",
    "        try:\n",
    "            df[header] = df[header].astype(float)\n",
    "            q = df[header].quantile(upper_quartile_limit)\n",
    "            series_row = (df[df[header] > q].index)\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "            q = df[header].quantile(low_quartile_limit)\n",
    "            series_row = (df[df[header] < q].index)\n",
    "            for id in list(np.array(series_row)):\n",
    "                outlier_rows.append([header,id])\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    unique_ids = []\n",
    "    unique_outlier_rows = []\n",
    "    \n",
    "    for col, rowid in outlier_rows:\n",
    "        if rowid not in unique_ids:\n",
    "            unique_outlier_rows.append([col,rowid])\n",
    "            unique_ids.append(rowid)\n",
    "    return unique_outlier_rows\n",
    "\n",
    "def remove_outliers(df=None, low_quartile_limit=.01, upper_quartile_limit=.99, headers=None):\n",
    "    \"\"\"\n",
    "    Remove rows which are considered outliers within the dataset, determined by :quartile_limit (99%)\n",
    "    \"\"\"\n",
    "    length_before = len(df)\n",
    "    outliers_index = []\n",
    "    for header in headers:\n",
    "        try:\n",
    "            df[header] = df[header].astype(float)\n",
    "            q = df[header].quantile(upper_quartile_limit)\n",
    "            outliers_index.append(list(np.array(df[df[header] > q].index)))\n",
    "            q = df[header].quantile(low_quartile_limit)\n",
    "            outliers_index.append(list(np.array(df[df[header] < q].index)))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    #flat_outliers_index = [item for sublist in l for item in outliers_index]\n",
    "    flat_outliers_index = [item for sublist in outliers_index for item in sublist]\n",
    "    outliers_index = list(set(flat_outliers_index))\n",
    "    df = df.drop(outliers_index)\n",
    "    return df\n",
    "\n",
    "# Defining which columns will be exposed to outliers\n",
    "rep_hist_sysmetric_summary_header_outliers = rep_hist_sysmetric_summary_headers\n",
    "\n",
    "# Printing dataframe before adjustments\n",
    "print('\\n\\nDATAFRAMES WITH OUTLIERS')\n",
    "print(rep_hist_sysmetric_summary_df.shape)\n",
    "print('----------------------------')\n",
    "\n",
    "#Printing outliers to screen\n",
    "rep_hist_sysmetric_summary_df_outliers = get_outliers(df=rep_hist_sysmetric_summary_df, headers=rep_hist_sysmetric_summary_header_outliers,upper_quartile_limit=upper_quartile_limit,low_quartile_limit=low_quartile_limit)\n",
    "print('\\n\\nOUTLIERS')\n",
    "print(len(rep_hist_sysmetric_summary_df_outliers))\n",
    "print('----------------------------')\n",
    "\n",
    "# Dropping Outliers\n",
    "rep_hist_sysmetric_summary_df_pruned = remove_outliers(df=rep_hist_sysmetric_summary_df, headers=rep_hist_sysmetric_summary_header_outliers,upper_quartile_limit=upper_quartile_limit,low_quartile_limit=low_quartile_limit)\n",
    "print('\\n\\nDATAFRAMES WITHOUT OUTLIERS')\n",
    "print(rep_hist_sysmetric_summary_df_pruned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data distribution without outliers\n",
    "\n",
    "Plotting metrics against SNAP_ID, without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if debug_mode is False:\n",
    "    \n",
    "    # Plotting Histograms without Outliers\n",
    "    for header in rep_hist_sysmetric_summary_header_outliers:\n",
    "        print('REP_HIST_SYSMETRIC - ' + header + ' - STRIPPED HISTOGRAM')\n",
    "        plot_hist(df=rep_hist_sysmetric_summary_df_pruned, tpc_type=tpcds, table='rep_hist_sysmetric_summary', feature_column=header, bin_size=10)\n",
    "    \n",
    "    # Plotting Scatter Plots without Outliers\n",
    "    for header in rep_hist_sysmetric_summary_header_outliers:\n",
    "        print('REP_HIST_SYSMETRIC - ' + header + ' - STRIPPED SCATTER')\n",
    "        plot_scatter(df=rep_hist_sysmetric_summary_df_pruned, tpc_type=tpcds,table='rep_hist_sysmetric_summary',feature_column=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Redundant Columns\n",
    "\n",
    "Dropping redundant columns which are not detrimental to the task at hand (NB: This is only the first steps towards feature selection. This step ensures that specific columns which are SURELY not useful are dropped ahead of time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "\n",
    "Based on the above plots, one can argue that the data distribution is uneven, and does not correlate to any particular pattern. A normalization approach (MinMaxScaling and/or RobustScaling) to the presented dataset is a more likely candidate than standardizing of the presented dataset. \n",
    "\n",
    "Reasons behind normalizing the dataset rather than standardizing, is due to the vast standard deviations from the mean for several feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_headers_rep_hist_sysmetric_summary = rep_hist_sysmetric_summary_headers\n",
    "\n",
    "def retain_these_columns(dataframe, headers):\n",
    "    \"\"\"\n",
    "    Drops all columns from dataframe except those passed in the header\n",
    "    \"\"\"\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    return dataframe[headers]\n",
    "\n",
    "shape = rep_hist_sysmetric_summary_df_pruned.shape\n",
    "rep_hist_sysmetric_summary_df_pruned = retain_these_columns(dataframe=rep_hist_sysmetric_summary_df_pruned,headers=retain_headers_rep_hist_sysmetric_summary)\n",
    "print('Before: ' + str(shape) + '| After: ' + str(rep_hist_sysmetric_summary_df_pruned.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Consolidation\n",
    "\n",
    "Grouping datasets on SNAP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_hist_sysmetric_summary_df_pruned = rep_hist_sysmetric_summary_df_pruned[rep_hist_sysmetric_summary_df_pruned['SNAP_ID'] > 0]\n",
    "\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT)\n",
    "rep_hist_sysmetric_summary_df_pruned = rep_hist_sysmetric_summary_df_pruned.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysmetric_summary_df_pruned.reset_index(inplace=True)\n",
    "\n",
    "print(\"REP_HIST_SYSMETRIC_SUMMARY shape after transformation: \" + str(rep_hist_sysmetric_summary_df_pruned.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Relavent Sources:\n",
    "\n",
    "* http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf\n",
    "* https://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a normalization preprocessing technique for data distribution that can closely approximate minimum and maximum observable values per column:\n",
    "\n",
    "<i>\"Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\"</i>\n",
    "\n",
    "Normalization formula is stated as follows: $$y=(x-min)/(max-min)$$\n",
    "\n",
    "### Standardization\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a standardization preprocessing technique for data distributions that observe a Gaussian spread, with a mean of 0 and a standard deviation of 1 (approximately close to these values):\n",
    "\n",
    "<i>\"Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\"</i>\n",
    "\n",
    "Standardization formula is stated as follows: $$y=(x-mean)/StandardDeviation$$\n",
    "Mean defined as: $$mean=sum(x)/count(x)$$\n",
    "Standard Deviation defined as: $$StandardDeviation=sqrt(sum((x-mean)^2)/count(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def robust_scaler(dataframe, headers):\n",
    "    \"\"\"\n",
    "    Normalize df using interquartile ranges as min-max, this way outliers do not play a heavy emphasis on the\n",
    "    normalization of values.\n",
    "    \"\"\"\n",
    "    return preprocessing.robust_scale(dataframe)\n",
    "\n",
    "def minmax_scaler(dataframe, headers):\n",
    "    \"\"\"\n",
    "    Normalize df using min-max ranges for normalization method\n",
    "    \"\"\"\n",
    "    return preprocessing.minmax_scale(dataframe)\n",
    "\n",
    "def normalize(dataframe, headers):\n",
    "    \"\"\"\n",
    "    The normalizer scales each value by dividing each value by its magnitude in n-dimensional space for n number of features. \n",
    "    \"\"\"\n",
    "    return preprocessing.normalize(dataframe)\n",
    "\n",
    "print('------------------BEFORE------------------')\n",
    "print('------------------REP_HIST_SYSMETRIC_SUMMARY------------------')\n",
    "print(rep_hist_sysmetric_summary_df_pruned.shape)\n",
    "print('\\n')\n",
    "#print(rep_hist_snapshot_df_pruned.head())\n",
    "#\n",
    "# ROBUST SCALER\n",
    "# rep_hist_sysmetric_summary_df_pruned_norm = robust_scaler(dataframe=rep_hist_sysmetric_summary_df_pruned,\n",
    "#                                                           headers=retain_headers_rep_hist_sysmetric_summary)\n",
    "#\n",
    "# MINMAX SCALER\n",
    "# rep_hist_sysmetric_summary_df_pruned_norm = minmax_scaler(dataframe=rep_hist_sysmetric_summary_df_pruned,\n",
    "#                                                           headers=retain_headers_rep_hist_sysmetric_summary)\n",
    "#\n",
    "# NORMALIZER\n",
    "rep_hist_sysmetric_summary_df_pruned_norm = normalize(dataframe=rep_hist_sysmetric_summary_df_pruned,\n",
    "                                                      headers=retain_headers_rep_hist_sysmetric_summary)\n",
    "\n",
    "print('\\n\\n------------------AFTER------------------')\n",
    "print('------------------REP_HIST_SYSMETRIC_SUMMARY------------------')\n",
    "print(rep_hist_sysmetric_summary_df_pruned_norm.shape)\n",
    "print('\\n\\n')\n",
    "print('\\n\\nREP_HIST_SYSMETRIC_SUMMARY')\n",
    "print(rep_hist_sysmetric_summary_df_pruned_norm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods\n",
    "\n",
    "* https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "\n",
    "Filter methods allow for the univariate analysis of features. Particularly, the following statistical methods have been considered and dismissed as explained below:\n",
    "\n",
    "* Information Gain - Data at hand is continous by nature, whilst MI is usually applicable against discrete values / binned labels.\n",
    "* Pearson Correlation Coefficient - Applicable to data with linear distributions, which is not the case for the majority of the presented features.\n",
    "\n",
    "### Chi2\n",
    "\n",
    "Therefore, a likely candidate for a first univariate, filter test is a chi2 measure, between X labels 'SNAP_ID', and other output 'y' labels. NB: Due to chi2 requiring non-nagative values, a data normalization strategy was opted for (Refer to above cell).\n",
    "\n",
    "The following cell computes the chi2 value for each feature pertaining in the following tables, in relation to the SNAP_ID feature:\n",
    "* REP_HIST_SYSMETRIC_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chi2_test(top_n_features=30, X_df=None, y_df=None, table=None, headers=None):\n",
    "    \"\"\"\n",
    "    Carries out a chi squared test on passed X,y dataframes, selecting top N features ranked by highest scoring\n",
    "    \"\"\"\n",
    "    chi2_selector = SelectKBest(score_func=chi2, k=top_n_features)\n",
    "    X_kbest = chi2_selector.fit_transform(X_df, y_df)\n",
    "    print('\\n\\nTable [' + table.upper() + ']')\n",
    "    print('Original number of features: ' + str(X_df.shape[1]))\n",
    "    print('Reduced number of features: ' + str(X_kbest.shape[1]) + \"\\n\")\n",
    "    outcome = chi2_selector.get_support()\n",
    "    \n",
    "    scoring_sheet = []\n",
    "    for i in range(0,len(headers)-1):\n",
    "        if outcome[i]:\n",
    "            scoring_sheet.append([headers[i],chi2_selector.scores_[i]])\n",
    "    \n",
    "    scoring_sheet = sorted(scoring_sheet, key=itemgetter(1), reverse=True)\n",
    "    [print('Feature [' + str(row) + '] with score [' + str(score) + ']') for row, score in scoring_sheet[:top_n_features]]\n",
    "    \n",
    "    scoring_sheet = pd.Series((v[1] for v in scoring_sheet[:top_n_features]) )\n",
    "    scoring_sheet[:top_n_features].plot.bar()\n",
    "    plt.ylabel('CHI2 RANKING')\n",
    "    plt.xlabel('FEATURES')\n",
    "    plt.title('Features Ranked By Chi2 Scoring')\n",
    "    plt.rcParams['figure.figsize'] = [20, 15]\n",
    "    plt.show()\n",
    "\n",
    "chi2_test(top_n_features=top_n_features, X_df=rep_hist_sysmetric_summary_df_pruned_norm, y_df=rep_hist_sysmetric_summary_df_pruned['SNAP_ID'], table='REP_HIST_SYSMETRIC_SUMMARY', headers=retain_headers_rep_hist_sysmetric_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Feature Importance\n",
    "\n",
    "Calculating MI (Mutual Information) scoring between data matrix X (feature vectors) and target column y ('SNAP_ID') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rfr_ranking(X_df=None, y_df=None, headers=None, top_n_features=30, parallel_degree=1):\n",
    "    \"\"\"\n",
    "    Ranks features using a filter RFR method, and plots them in descending order (ranked by importance)\n",
    "    \"\"\"\n",
    "    X_df = np.array(X_df)\n",
    "    y_df = np.array(y_df)\n",
    "    rfr = RandomForestRegressor(n_estimators=2000,\n",
    "                                n_jobs=parallel_degree)\n",
    "    rfr.fit(X_df, y_df)\n",
    "    importances = pd.DataFrame({'feature':headers,\n",
    "                                'importance':np.round(rfr.feature_importances_,3)})\n",
    "    importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "    print(importances[:top_n_features])\n",
    "    importances[:top_n_features].plot.bar()\n",
    "    plt.ylabel('FEATURE INFORMATION IMPORTANCE')\n",
    "    plt.xlabel('FEATURES')\n",
    "    plt.title('Features Ranked By RFC Scoring')\n",
    "    plt.rcParams['figure.figsize'] = [20, 15]\n",
    "    plt.show()\n",
    "\n",
    "rfr_ranking(top_n_features=top_n_features,\n",
    "            X_df=rep_hist_sysmetric_summary_df_pruned_norm, \n",
    "            y_df=rep_hist_sysmetric_summary_df_pruned['SNAP_ID'], \n",
    "            headers=retain_headers_rep_hist_sysmetric_summary,\n",
    "            parallel_degree=parallel_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Methods\n",
    "\n",
    "Use a number of machine learning models to evaluate features together and rank by highest. The following machine learning models will be opted for:\n",
    "\n",
    "* Random Forest Classifier\n",
    "* Gradient Boosting\n",
    "\n",
    "In a 'Brute-Fore' approach, these machine learning heuristics will strip away 1 feature at a time in a method referred to as 'Recursive Feature Elimination', and compare accuracy with every variable elimination. This allows the respective classifier to establish an optimum feature configuration with the highest accuracy score.\n",
    "\n",
    "https://www.fabienplisson.com/choosing-right-features/\n",
    "\n",
    "### Random Forest Wrapper (Feature Combination)  (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rfr_wrapper(X_df=None, y_df=None, test_split=.4, random_state=42, table_name=None, top_n_features=10, parallel_degree=1):\n",
    "    \"\"\"\n",
    "    Random Forest Regressor - Takes data matrix and target vector, and evaluates best combination of features \n",
    "    using an RFR model.\n",
    "    \"\"\"\n",
    "    X_df = np.array(X_df)\n",
    "    y_df = np.array(y_df)\n",
    "    #feature_count = len(X_df[0])\n",
    "    #print(len(X_df[0]))\n",
    "    val_op, optimum_features = 0, 0\n",
    "    val_op = 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_split)\n",
    "    model = RandomForestRegressor(n_estimators=top_n_features, \n",
    "                                  n_jobs=parallel_degree)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for test data and evaluate\n",
    "    pred_y = model.predict(X_test)\n",
    "    predictions = [round(value) for value in pred_y]\n",
    "    r2s = r2_score(y_test, predictions)\n",
    "    print(\"Table [\" + table_name + \"] RFR R2 Score: \" + str(r2s))\n",
    "    \n",
    "    # fit model using each importance as a threshold\n",
    "    print('Feature Importance\\n' + str('-'*60))\n",
    "    print(model.feature_importances_)\n",
    "    print(str('-'*60))\n",
    "    thresholds = np.sort(model.feature_importances_)\n",
    "    feature_counts, feature_score = [],[]\n",
    "    for thresh in thresholds:\n",
    "        # selecting features using threshold\n",
    "        selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "        select_train_x = selection.transform(X_train)\n",
    "        \n",
    "        # training model\n",
    "        selection_model = RandomForestRegressor(n_estimators=top_n_features,\n",
    "                                                n_jobs=parallel_degree)\n",
    "        selection_model.fit(select_train_x, y_train)\n",
    "        \n",
    "        # evaluating model\n",
    "        select_test_x = selection.transform(X_test)\n",
    "        pred_y = selection_model.predict(select_test_x)\n",
    "        predictions = [round(value) for value in pred_y]\n",
    "        r2s = r2_score(y_test, predictions)\n",
    "        print(\"Thresh=\" + str(thresh) + \", n=\" + str(select_train_x.shape[1]) + \", R2 Score: \" + str(r2s))\n",
    "        if(r2s > val_op):\n",
    "            val_op = r2s\n",
    "            optimum_features = select_train_x.shape[1]\n",
    "            \n",
    "        # Add/Keep track of '[no of features','r2 score']\n",
    "        feature_counts.append(select_train_x.shape[1])\n",
    "        feature_score.append(r2s)\n",
    "        \n",
    "    # Plot feature count performance\n",
    "    plt.rcParams['figure.figsize'] = [20, 15]\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.title('Feature Pairing Performance')\n",
    "    plt.plot(feature_counts, feature_score)\n",
    "    plt.show()\n",
    "    \n",
    "    return val_op, optimum_features\n",
    "\n",
    "rfr_hist_sysmetric_summary_score, rfr_hist_sysmetric_summary_count = rfr_wrapper(X_df=rep_hist_sysmetric_summary_df_pruned_norm,\n",
    "                                                                                 y_df=rep_hist_sysmetric_summary_df_pruned['SNAP_ID'],\n",
    "                                                                                 test_split=test_split,\n",
    "                                                                                 table_name='REP_HIST_SYSMETRIC_SUMMARY',\n",
    "                                                                                 top_n_features=top_n_features,\n",
    "                                                                                 parallel_degree=parallel_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Wrapper (Feature Combination)  (Regression)\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_wrapper(X_df=None, y_df=None, test_split=.4, random_state=42, table_name=None, top_n_features=10):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regressor - Takes data matrix and target vector, and evaluates best combination of features \n",
    "    using a GBR model.\n",
    "    \"\"\"\n",
    "    X_df = np.array(X_df)\n",
    "    y_df = np.array(y_df)\n",
    "    #feature_count = len(X_df[0])\n",
    "    val_op, optimum_features = 0, 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_split)\n",
    "    model = GradientBoostingRegressor(n_estimators=top_n_features)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for test data and evaluate\n",
    "    pred_y = model.predict(X_test)\n",
    "    predictions = [round(value) for value in pred_y]\n",
    "    r2s = r2_score(y_test, predictions)\n",
    "    print(\"Table [\" + table_name + \"] RFR R2 Score: \" + str(r2s))\n",
    "    \n",
    "    # fit model using each importance as a threshold\n",
    "    print('Feature Importance\\n' + str('-'*60))\n",
    "    print(model.feature_importances_)\n",
    "    print(str('-'*60))\n",
    "    thresholds = np.sort(model.feature_importances_)\n",
    "    feature_counts, feature_score = [],[]\n",
    "    for thresh in thresholds:\n",
    "        # selecting features using threshold\n",
    "        selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "        select_train_x = selection.transform(X_train)\n",
    "        \n",
    "        # training model\n",
    "        selection_model = GradientBoostingRegressor(n_estimators=top_n_features)\n",
    "        selection_model.fit(select_train_x, y_train)\n",
    "        \n",
    "        # evaluating model\n",
    "        select_test_x = selection.transform(X_test)\n",
    "        pred_y = selection_model.predict(select_test_x)\n",
    "        predictions = [round(value) for value in pred_y]\n",
    "        r2s = r2_score(y_test, predictions)\n",
    "        print(\"Thresh=\" + str(thresh) + \", n=\" + str(select_train_x.shape[1]) + \", R2 Score: \" + str(r2s))\n",
    "        if(r2s > val_op):\n",
    "            val_op = r2s\n",
    "            optimum_features = select_train_x.shape[1]\n",
    "            \n",
    "        # Add/Keep track of '[no of features','r2 score']\n",
    "        feature_counts.append(select_train_x.shape[1])\n",
    "        feature_score.append(r2s)\n",
    "    \n",
    "    # Plot feature count performance\n",
    "    plt.rcParams['figure.figsize'] = [20, 15]\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.title('Feature Pairing Performance')\n",
    "    plt.plot(feature_counts, feature_score)\n",
    "    plt.show()\n",
    "    \n",
    "    return val_op, optimum_features\n",
    "\n",
    "gbw_hist_sysmetric_summary_score, gbw_hist_sysmetric_summary_count = gradient_boosting_wrapper(X_df=rep_hist_sysmetric_summary_df_pruned_norm,\n",
    "                                                                                               y_df=rep_hist_sysmetric_summary_df_pruned['SNAP_ID'],\n",
    "                                                                                               test_split=test_split,\n",
    "                                                                                               table_name='REP_HIST_SYSMETRIC_SUMMARY',\n",
    "                                                                                               top_n_features=top_n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rfr_hist_sysmetric_summary_score > gbw_hist_sysmetric_summary_score:\n",
    "    rep_hist_sysmetric_summary_op = rfr_hist_sysmetric_summary_count\n",
    "    rep_hist_sysmetric_summary_model = 0\n",
    "else:\n",
    "    rep_hist_sysmetric_summary_op = gbw_hist_sysmetric_summary_count\n",
    "    rep_hist_sysmetric_summary_model = 1\n",
    "\n",
    "def rfe_selector(X_df=None, y_df=None, test_split=.4, random_state=42, table_name=None, optimum_feature_count=0, model=None, top_n_features=10, parallel_degree=1):\n",
    "    \"\"\"\n",
    "    Recursive Feature Elimination Function\n",
    "    \"\"\"\n",
    "    X_df = np.array(X_df)\n",
    "    y_df = np.array(y_df)\n",
    "    #feature_count = len(X_df[0])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_split)\n",
    "    if model == 0:\n",
    "        model = RandomForestRegressor(n_estimators=top_n_features,\n",
    "                                      n_jobs=parallel_degree)\n",
    "    elif model == 1:\n",
    "        model = GradientBoostingRegressor(n_estimators=top_n_features)\n",
    "    \n",
    "    # create the RFE model and select 4 attributes\n",
    "    rfe_model = RFE(model, optimum_feature_count, step=1)\n",
    "    rfe_model = rfe_model.fit(X_train, y_train)\n",
    "    \n",
    "    # summarize the selection of the attributes\n",
    "    print(rfe_model.support_)\n",
    "    print(rfe_model.ranking_)\n",
    "    \n",
    "    # evaluate the model on testing set\n",
    "    pred_y = rfe_model.predict(X_test)\n",
    "    predictions = [round(value) for value in pred_y]\n",
    "    r2s = r2_score(y_test, predictions)\n",
    "    print(\"Table [\" + table_name + \"] RFR R2 Score: \" + str(r2s) + \" with \" + str(optimum_feature_count) + \" features\")\n",
    "    print(\"\\n\\n------------------------------------------\\n\\n\")\n",
    "    \n",
    "rfe_selector(X_df=rep_hist_sysmetric_summary_df_pruned_norm,\n",
    "             y_df=rep_hist_sysmetric_summary_df_pruned['SNAP_ID'],\n",
    "             test_split=test_split,\n",
    "             table_name='REP_HIST_SYSMETRIC_SUMMARY',\n",
    "             optimum_feature_count=rep_hist_sysmetric_summary_op,\n",
    "             model = rep_hist_sysmetric_summary_model,\n",
    "             top_n_features=top_n_features,\n",
    "             parallel_degree=parallel_degree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
