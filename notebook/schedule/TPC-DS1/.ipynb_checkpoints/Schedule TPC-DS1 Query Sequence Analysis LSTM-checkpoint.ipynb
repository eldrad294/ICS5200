{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Sequence Analysis\n",
    "\n",
    "This notebook focuses on sequence analysis, when presented with a workload schedule / sequence of queries. In an average day to day work activity, particular query patterns can be discerned. This pattern distinction allows us to discern which queries will be susceptible to execution over time, allowing us to know ahead of time which queries will be executed against the database.\n",
    "\n",
    "### Module Installation and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.1.0\n",
      "numpy: 1.15.2\n",
      "pandas: 0.23.4\n",
      "sklearn: 0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# scipy\n",
    "import scipy as sc\n",
    "print('scipy: %s' % sc.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn as sk\n",
    "print('sklearn: %s' % sk.__version__)\n",
    "# keras\n",
    "import keras as ke\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.utils import np_utils\n",
    "print('keras: %s' % ke.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment. \n",
    "NB: This experiment demonstrates at time  step = 1 (1 minute in advance). Further down in experiment, other timestep results are also featured and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Experiment Config\n",
    "tpcds='TPCDS1' # Schema upon which to operate test\n",
    "lag=3 # Time Series shift / Lag Step. Each lag value equates to 1 minute. Cannot be less than 1\n",
    "if lag < 1:\n",
    "    raise ValueError('Lag value must be greater than 1!')\n",
    "#\n",
    "test_split=.2 # Denotes which Data Split to operate under when it comes to training / validation\n",
    "y_label = ['SQL_ID'] # Denotes which label to use for time series experiments\n",
    "#\n",
    "# Forest Config\n",
    "parallel_degree = 1\n",
    "n_estimators = 10\n",
    "#\n",
    "# Net Config\n",
    "batch_size=10\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNAP_ID' 'DBID' 'INSTANCE_NUMBER' 'SQL_ID' 'PLAN_HASH_VALUE'\n",
      " 'OPTIMIZER_COST' 'OPTIMIZER_MODE' 'OPTIMIZER_ENV_HASH_VALUE'\n",
      " 'SHARABLE_MEM' 'LOADED_VERSIONS' 'VERSION_COUNT' 'MODULE' 'ACTION'\n",
      " 'SQL_PROFILE' 'FORCE_MATCHING_SIGNATURE' 'PARSING_SCHEMA_ID'\n",
      " 'PARSING_SCHEMA_NAME' 'PARSING_USER_ID' 'FETCHES_TOTAL' 'FETCHES_DELTA'\n",
      " 'END_OF_FETCH_COUNT_TOTAL' 'END_OF_FETCH_COUNT_DELTA' 'SORTS_TOTAL'\n",
      " 'SORTS_DELTA' 'EXECUTIONS_TOTAL' 'EXECUTIONS_DELTA'\n",
      " 'PX_SERVERS_EXECS_TOTAL' 'PX_SERVERS_EXECS_DELTA' 'LOADS_TOTAL'\n",
      " 'LOADS_DELTA' 'INVALIDATIONS_TOTAL' 'INVALIDATIONS_DELTA'\n",
      " 'PARSE_CALLS_TOTAL' 'PARSE_CALLS_DELTA' 'DISK_READS_TOTAL'\n",
      " 'DISK_READS_DELTA' 'BUFFER_GETS_TOTAL' 'BUFFER_GETS_DELTA'\n",
      " 'ROWS_PROCESSED_TOTAL' 'ROWS_PROCESSED_DELTA' 'CPU_TIME_TOTAL'\n",
      " 'CPU_TIME_DELTA' 'ELAPSED_TIME_TOTAL' 'ELAPSED_TIME_DELTA' 'IOWAIT_TOTAL'\n",
      " 'IOWAIT_DELTA' 'CLWAIT_TOTAL' 'CLWAIT_DELTA' 'APWAIT_TOTAL'\n",
      " 'APWAIT_DELTA' 'CCWAIT_TOTAL' 'CCWAIT_DELTA' 'DIRECT_WRITES_TOTAL'\n",
      " 'DIRECT_WRITES_DELTA' 'PLSEXEC_TIME_TOTAL' 'PLSEXEC_TIME_DELTA'\n",
      " 'JAVEXEC_TIME_TOTAL' 'JAVEXEC_TIME_DELTA' 'IO_OFFLOAD_ELIG_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_ELIG_BYTES_DELTA' 'IO_INTERCONNECT_BYTES_TOTAL'\n",
      " 'IO_INTERCONNECT_BYTES_DELTA' 'PHYSICAL_READ_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_READ_REQUESTS_DELTA' 'PHYSICAL_READ_BYTES_TOTAL'\n",
      " 'PHYSICAL_READ_BYTES_DELTA' 'PHYSICAL_WRITE_REQUESTS_TOTAL'\n",
      " 'PHYSICAL_WRITE_REQUESTS_DELTA' 'PHYSICAL_WRITE_BYTES_TOTAL'\n",
      " 'PHYSICAL_WRITE_BYTES_DELTA' 'OPTIMIZED_PHYSICAL_READS_TOTAL'\n",
      " 'OPTIMIZED_PHYSICAL_READS_DELTA' 'CELL_UNCOMPRESSED_BYTES_TOTAL'\n",
      " 'CELL_UNCOMPRESSED_BYTES_DELTA' 'IO_OFFLOAD_RETURN_BYTES_TOTAL'\n",
      " 'IO_OFFLOAD_RETURN_BYTES_DELTA' 'BIND_DATA' 'FLAG' 'CON_DBID' 'CON_ID'\n",
      " 'SQL_TEXT' 'COMMAND_TYPE' 'STARTUP_TIME' 'BEGIN_INTERVAL_TIME'\n",
      " 'END_INTERVAL_TIME' 'FLUSH_ELAPSED' 'SNAP_LEVEL' 'ERROR_COUNT'\n",
      " 'SNAP_FLAG' 'SNAP_TIMEZONE']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Open Data\n",
    "rep_hist_snapshot_path = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#rep_hist_snapshot_path = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds + '/v2/rep_hist_snapshot.csv'\n",
    "#\n",
    "rep_hist_snapshot_df = pd.read_csv(rep_hist_snapshot_path)\n",
    "#\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "#\n",
    "rep_hist_snapshot_df.columns = prettify_header(rep_hist_snapshot_df.columns.values)\n",
    "#\n",
    "print(rep_hist_snapshot_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Matrix Shapes\n",
    "\n",
    "Changes dataframe shape, in an attempt to drop all numeric data. Below's aggregated data is done so on:\n",
    "* SNAP_ID\n",
    "* INSTANCE_NUMBER\n",
    "* DBID\n",
    "* SQL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Aggregation: (2230, 90)\n",
      "Shape After Aggregation: (1923, 78)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Before Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "#\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT) and drop all numeric\n",
    "rep_hist_snapshot_df = rep_hist_snapshot_df.groupby(['SNAP_ID','DBID','INSTANCE_NUMBER','SQL_ID']).sum()\n",
    "rep_hist_snapshot_df.reset_index(inplace=True)\n",
    "#\n",
    "print(\"Shape After Aggregation: \" + str(rep_hist_snapshot_df.shape))\n",
    "df = rep_hist_snapshot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ordering\n",
    "\n",
    "Sorting of datasets in order of SNAP_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1923, 78)\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=['SNAP_ID'], ascending=True, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging Labels\n",
    "\n",
    "Removes the label column, and adds it at the beginning of the matrix for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Column Switch: (1923, 78)\n",
      "After Column Switch: (1923, 1)\n",
      "           SQL_ID\n",
      "0   03ggjrmy0wa1w\n",
      "26  8mdz49zkajhw3\n",
      "27  8t26unxsrxj72\n",
      "28  93n8wp5a8xyxn\n",
      "29  9ggx4p02d346a\n"
     ]
    }
   ],
   "source": [
    "print('Before Column Switch: ' + str(df.shape))\n",
    "df = df[y_label]\n",
    "print('After Column Switch: ' + str(df.shape))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before label encoding: (1923, 1)\n",
      "After label encoding: (1923,)\n",
      "----------------------------------\n",
      "\n",
      "Available Classes:\n",
      "379\n",
      "['01d5n1nm17r2h' '01tp87bk1t2zv' '03ggjrmy0wa1w' '04kug40zbu4dm'\n",
      " '06dymzb481vnd' '06g9mhm5ba7tt' '09vrdx888wvvb' '0a08ug2qc1j82'\n",
      " '0a7q9v9nd2qc1' '0aq14dznn91rg' '0f60bzgt9127c' '0ga8vk4nftz45'\n",
      " '0hdquu87pydzk' '0hhmdwwgxbw0r' '0jj0ct4x4gy27' '0kcbwucxmazcp'\n",
      " '0kkhhb2w93cx0' '0m78skf1mudnb' '0qbzfjt00pbsx' '0v3dvmc22qnam'\n",
      " '0vcz1xqfrykmw' '0w26sk6t6gq98' '0x6ks1umjn6k6' '0y080mnfaqk3u'\n",
      " '0ym9wzzys5zax' '130r442w3nfny' '13a9r2xkx1bxb' '13ys8ux8xvrbm'\n",
      " '14f5ngrj3cc5h' '14kx436hrv7cc' '193ncz0tf25hw' '1aajwypydy0wu'\n",
      " '1c3x1d7pc0cmt' '1fn8v91f0arf0' '1gfaj4z5hn1kf' '1hxfbnas8xr2j'\n",
      " '1jhyrdp21f2q6' '1k33bhcnpbrwx' '1kz16yhs993h2' '1ms8wj24sqny4'\n",
      " '1p5grz1gs7fjq' '1pgnzc6zf7ctc' '1pv23p59mjs0v' '1r7b985mxqj71'\n",
      " '1rpgk59t8pvs6' '1u97hwfu7dcmz' '1v2b661suttyp' '1wk0t84cwr5ps'\n",
      " '1wpns6pagm2qj' '1wq6da7n103qd' '1wz811srf8xh8' '2046z7kdh823h'\n",
      " '20bqsr6btd9x9' '20vv6ttajyjzq' '248suhpf1dbya' '24u3pyk3mymm4'\n",
      " '26jdypa362wv9' '279ragg4g2fb9' '27ws0jh2nqdwj' '288tccrcj5fyj'\n",
      " '297gc42f2hwdd' '29jd5f9tamanf' '29mjaymwt5p6d' '2afh4r7z1rfv6'\n",
      " '2cpvayu3hyq91' '2d5v40f700a52' '2h0gb24h6zpnu' '2hnpu9m861609'\n",
      " '2j25hzq35w45h' '2j5bk3tn2zt0g' '2mp99nzd9u1qp' '2pz0tqbv91m11'\n",
      " '2sqcyuffj0pnb' '2tzs6v7mwnznt' '2ugyr59f69wwg' '2vkaf4z6ktk9c'\n",
      " '2vv49kn276y0z' '2wdrw5tqputaq' '2wuhkcaz4uhs5' '30rvbvzuuxvvd'\n",
      " '32qq8k1n8ynn9' '33cmj01uf5zfb' '33vw5865cwyyn' '3419gsthd5szh'\n",
      " '350d18utrk6cu' '35j72wf33gaqa' '360qzju916m57' '38243c4tqrkxm'\n",
      " '39m4sx9k63ba2' '39nyc1pykjg41' '3bmwxjakz3ufw' '3c1kubcdjnppq'\n",
      " '3dqrj325buvt8' '3j3q8xkn9qvrv' '3ja745d9jkpc8' '3kwn98m1bt3jp'\n",
      " '3kywng531fcxu' '3m6xpgmgktw3d' '3m8smr0v7v1m6' '3rd3sp2ak89rc'\n",
      " '3ru3r9twxsazy' '3sqgkcng6vx8r' '3t6crmxzc7gwj' '3tjqbzbjtv7kr'\n",
      " '3un99a0zwp4vd' '3wkn3gt9zn7ka' '3y6pgnk2ubw7g' '41ncsfu26db1w'\n",
      " '45zq5xs59sjsj' '46q97td4vcwzk' '47r1y8yn34jmj' '49s332uhbnsma'\n",
      " '4b4wp0a8dvkf0' '4bmxgstv2nr07' '4cgbvpjc134nu' '4fv4r99k9v51q'\n",
      " '4g1u6kabran4u' '4j2p57b4stn2a' '4mmqayxa4hwb0' '4q1rzhn63sgpt'\n",
      " '4r62dbqxw2r10' '4rn02xyj9tujk' '4tmd72wfn52m4' '4u268zn6r57tm'\n",
      " '4v69jmg5x7mvg' '4vcvqy7vvy5zm' '4w3rn6bvafgfc' '4xkj9bw91x8tf'\n",
      " '4yb79gnqp1v90' '512674nmuhtg6' '52p0tj20muz3f' '52xfvkwy3u9xx'\n",
      " '54qdvyrqsg8m6' '55u3mjpyxzv5g' '593hanvpdtfkd' '59zh1b9759nf4'\n",
      " '5g88vmdgd99f7' '5h7w8ykwtb2xt' '5h7xwu611bxv2' '5hrvvu1r771m5'\n",
      " '5jxkb7r2kn817' '5nn31913tabu9' '5pf6jgsps9dtk' '5pkjgqjxz0kd8'\n",
      " '5pxcsstbnzj04' '5qgz1p0cut7mx' '5ry53w1yug897' '5wwkp7spyq2fn'\n",
      " '5y3x5cyp1xn74' '61mh8ky0utssp' '622ufbrgvxdc7' '646bq4xw23aw0'\n",
      " '64fq7u4a68nr1' '664j0xx9u5wvu' '68uhsqwvcfqab' '68xv41yva21a3'\n",
      " '6ajkhukk78nsr' '6ccr6xx4tq5yy' '6d0rbkgfab2xa' '6fvfqaw68q59b'\n",
      " '6hawp1pp037tk' '6kckz8dk87m45' '6n2qqv1brfhpp' '6nc5c1qczss8j'\n",
      " '6ntz6ynyaytgf' '6u001adh62r0f' '6zcux9jb78w36' '6zs29hb3gpcf5'\n",
      " '6zzvnuw5p3tat' '71uursqtj1j2m' '73tu7n31zn5pz' '74anujtt8zw4h'\n",
      " '76ds5wxsv7f5t' '76prtdxhjck3p' '76ur6b49gfsu8' '77mtwmnd4c4pk'\n",
      " '785wb90xs3r0t' '7d6r5amubkffq' '7fbzhzg6ysu25' '7frp5qgh6jpjn'\n",
      " '7gy3y7kjsbfuv' '7h8x6dzapnsua' '7hys3h7ysgf9m' '7hzbv7ux8uhmt'\n",
      " '7j630dz60gqa3' '7jpt4cpfvcy1k' '7m8xtjmn5zv0g' '7mu447a2r8bb8'\n",
      " '7tchj0bmt6tn1' '7ts87fuvk3dhz' '7u49y06aqxg1s' '7u88drn7gy4fw'\n",
      " '7ub921xztd5pt' '7uzyw6t048658' '7vtvbg7s3zcyp' '7w116jy6ysqpm'\n",
      " '80bz53g2s4306' '84aqqjbf6dkt3' '84ntdbh48ctu9' '84zqd7a3ap5ud'\n",
      " '85cmvvurya34f' '865qwpcdyggkk' '86kwhy1f0bttn' '876ytpt4g7dza'\n",
      " '87gaftwrm2h68' '87gtj5jaq4a3t' '88uytsd7r41y9' '8adcur42pddxj'\n",
      " '8axdfz2khnyzs' '8cx2kbtd2w4dn' '8cxgpdw3qqxqg' '8k0qd372mh9td'\n",
      " '8mdz49zkajhw3' '8nhg2pdrzs3ww' '8r5cuhx03m68d' '8swypbbr0m372'\n",
      " '8t26unxsrxj72' '8u809k64x3nzd' '8ws5dkvuusafp' '8yk1b0cdrs5fg'\n",
      " '8ymq1gb13rfab' '8zc85a8249x81' '909ysvbz0uqd8' '934ur8r7tqbjx'\n",
      " '93mj5tk40ap8f' '93n8wp5a8xyxn' '93z4cu03tf4z1' '95y5r5ksf94b1'\n",
      " '96g93hntrzjtr' '97pbnmc9h3ny4' '9ddxxas8hu8sv' '9dnnpagwcg2cu'\n",
      " '9ffbbqqfbgxyu' '9ffht8tuysgx9' '9ggx4p02d346a' '9rmg1ukgcyzpv'\n",
      " '9tgj4g8y4rwy8' '9tpnr4jz10j3y' '9ua42c6f2qs7s' '9vmcsc3prvxpa'\n",
      " '9x8gaksqvta15' '9y7s2afnkqs9d' '9zg9qd9bm4spu' 'a2ttbs5gf0xpu'\n",
      " 'a5ym25h9gmsvr' 'a6fy23us0jz84' 'a6g97rawd3ggv' 'a6ygk0r9s5xuj'\n",
      " 'abr26jpdfxwhy' 'ac805sb6btmg3' 'af28xbfcdrkbb' 'aggcw7yk1a7s6'\n",
      " 'akk9bpbxjkj2g' 'aksaf74grz97n' 'amd5g84rhgp0h' 'antwva8smnnmt'\n",
      " 'as8sqm1c84n07' 'asvzxj61dc5vs' 'at1ygf4rd7cvj' 'at2h5x34yj2c4'\n",
      " 'au8ztarrm6vvs' 'avmf520g02n1j' 'b0r7k3v44k1z8' 'b0stf459a3121'\n",
      " 'b2hr39jj45854' 'b4j3z1g2nwfys' 'b8cjbq1au6kz8' 'b94dthkxngzzr'\n",
      " 'b9c6ffh8tc71f' 'bb926a5dcb8kr' 'bcbpkhm3cq424' 'bj5v9w48937nu'\n",
      " 'bkq9pjcfvm9vn' 'bkw3supfuskbc' 'bndp6dxkmkxyc' 'bp5uk9gssq3kc'\n",
      " 'brsaqch9hhutd' 'bvkckyya5hyqx' 'bvwv6jvt9b0mj' 'bwg61ruy67kmv'\n",
      " 'bwsf4tnh0gcgv' 'c1jdamyak9mqm' 'c1x7a33hxwfyy' 'c20kr1p071kkn'\n",
      " 'c27s55jww516q' 'c2bxq4kd3uj1t' 'c3tb6vfmqkgf1' 'c68zcykp5x303'\n",
      " 'c6awqs517jpj0' 'c9umxngkc3byq' 'cat054a4rkjqr' 'cb21bacyh3c7d'\n",
      " 'cbhzzp1d7dhkc' 'cd1q0zw912c1p' 'cfk18fw3fynvs' 'cfsnf5tz2q74a'\n",
      " 'cgnmvpx2g5jq2' 'ch4vvuszvnv79' 'chbj5w1vwums1' 'cjq93m442uprp'\n",
      " 'cjty8qc84r509' 'ck9067jjt5ht5' 'ckfvfhzy0qrws' 'cqrk7q7f6nk3d'\n",
      " 'crt85hc6g7yrb' 'ct4bbu3duky6v' 'ct6c4h224pxgz' 'cvn54b7yz0s8u'\n",
      " 'cw6vxf0kbz3v1' 'cwzcht2y54876' 'd08vu8gv7xb6w' 'd0dnpw2mcxqxh'\n",
      " 'd134mqkq6kgbu' 'd2tvgg49y2ap6' 'd2zx61xsr2xfn' 'd4hnzu9x4k2gy'\n",
      " 'd5jd9b2xct7xz' 'd6fsyfkfqdd0p' 'd6vqfmt62hypx' 'd6vwqbw6r2ffk'\n",
      " 'd7w1dugmzb9n9' 'da5g5ca8pmau7' 'dasnrr294mm1k' 'dcmtw8yvz8v20'\n",
      " 'ddc2dxdzc9ugm' 'dfffkcnqfystw' 'dg7h5qtrw6dr7' 'dh7u76hwz04bq'\n",
      " 'dm7wjadqjyqna' 'dr23gzy8yj2n8' 'dvpfc386d6dmm' 'dxv968j0352kb'\n",
      " 'dyk4dprp70d74' 'f0f68t7c4105q' 'f0h5rpzmhju11' 'f30jhmhnc9s4k'\n",
      " 'f4498zktd6sv1' 'f80h0xb1qvbsk' 'f9hk3q1y2b8nt' 'fc0va0vju750z'\n",
      " 'fg4skgcja2cyj' 'fgcvncf0ab9dh' 'fhf8upax5cxsz' 'fkc81h2686aqc'\n",
      " 'fnghrcwz4mfb3' 'fr04r8ayb5nxz' 'frjd8zfy2jfdq' 'fs4p95w7yg25b'\n",
      " 'fsprw99ahsvyn' 'ft6n0dg4unah5' 'fur78fafyqrkn' 'fv9zrr2b2b61w'\n",
      " 'fx7sjdj48pn6z' 'fykucbhdkfqv9' 'g0b4snpj74cv5' 'g22f2h8s9vfrz'\n",
      " 'g2gc9wj14hqw9' 'g3bwhq7gjqatj' 'g41n1r60y3d71' 'g4gp07gt2z920'\n",
      " 'g4y6nw3tts7cc' 'g5sbyzcg89kg1' 'g67y24u07t83k' 'g6dr457smp5xj'\n",
      " 'g7mt7ptq286u7' 'g7vkhf01vmwx4' 'gdh0vpjkmbtjw' 'gdhz78y24tvnz'\n",
      " 'gh5w0gcyfaujs' 'ghkg6v5k2uc6g' 'gkjkxbzzptg00' 'gmjt6pxkza5g2'\n",
      " 'gqcb78ccdpmmf' 'grwydz59pu6mc' 'gsfnqdfcvy33q' 'gsqxm6zz3w2by'\n",
      " 'gu5x4z494njku' 'gvcadr1hm4arv' 'gxbdzk4g9tzrx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "### Label Encoding\n",
    "#\n",
    "# Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "print(\"Before label encoding: \" + str(df.shape))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[y_label])\n",
    "df = le.transform(df[y_label])\n",
    "print(\"After label encoding: \" + str(df.shape) + \"\\n----------------------------------\\n\\nAvailable Classes:\")\n",
    "df = pd.DataFrame(df, columns=y_label)\n",
    "print(len(le.classes_))\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Shifting\n",
    "\n",
    "Shifting the datasets N lag minutes, in order to transform the problem into a supervised dataset. Each Lag Shift equates to 60 seconds (due to the way design of the data capturing tool). For each denoted lag amount, the same number of feature vectors will be stripped away at the beginning.\n",
    "\n",
    "Features and Labels are separated into seperate dataframes at this point.\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Features\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(1917, 4)\n",
      "\n",
      "-------------\n",
      "Labels\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(1917, 3)\n",
      "\n",
      "-------------\n",
      "Features After Time Shift\n",
      "Index(['var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "(1917, 4)\n",
      "\n",
      "-------------\n",
      "Labels After Time Shift\n",
      "Index(['var1(t+1)', 'var1(t+2)', 'var1(t+3)'], dtype='object')\n",
      "(1917, 3)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = data\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    if n_in != 0:\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    n_out += 1\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#\n",
    "def remove_n_time_steps(data, n=1):\n",
    "    if n == 0:\n",
    "        return data\n",
    "    df = data\n",
    "    headers = df.columns\n",
    "    dropped_headers = []\n",
    "    #     for header in headers:\n",
    "    #         if \"(t)\" in header:\n",
    "    #             dropped_headers.append(header)\n",
    "    #\n",
    "    for i in range(1,n+1):\n",
    "        for header in headers:\n",
    "            if \"(t+\"+str(i)+\")\" in header:\n",
    "                dropped_headers.append(str(header))\n",
    "    #\n",
    "    return df.drop(dropped_headers, axis=1) \n",
    "#\n",
    "# Frame as supervised learning set\n",
    "shifted_df = series_to_supervised(df, lag, lag)\n",
    "#\n",
    "# Seperate labels from features\n",
    "y_row = []\n",
    "for i in range(lag+1,(lag*2)+2):\n",
    "    y_df_column_names = shifted_df.columns[len(df.columns)*i:len(df.columns)*i + len(y_label)]\n",
    "    y_row.append(y_df_column_names)\n",
    "y_df_column_names = []   \n",
    "for row in y_row:\n",
    "    for val in row:\n",
    "        y_df_column_names.append(val)\n",
    "#\n",
    "# y_df_column_names = shifted_df.columns[len(df.columns)*lag:len(df.columns)*lag + len(y_label)]\n",
    "y_df = shifted_df[y_df_column_names]\n",
    "X_df = shifted_df.drop(columns=y_df_column_names)\n",
    "print('\\n-------------\\nFeatures')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "print('\\n-------------\\nLabels')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)\n",
    "#\n",
    "# Delete middle timesteps\n",
    "X_df = remove_n_time_steps(data=X_df, n=lag)\n",
    "print('\\n-------------\\nFeatures After Time Shift')\n",
    "print(X_df.columns)\n",
    "print(X_df.shape)\n",
    "# y_df = remove_n_time_steps(data=y_df, n=lag)\n",
    "print('\\n-------------\\nLabels After Time Shift')\n",
    "print(y_df.columns)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "One hot encoding target labels for net application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before One Hot Encoding: (1917, 3)\n",
      "After One Hot Encoding: (1917, 3, 379)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# One Hot Encoding\n",
    "print(\"Before One Hot Encoding: \" + str(y_df.shape))\n",
    "y_df = np_utils.to_categorical(y_df)\n",
    "print(\"After One Hot Encoding: \" + str(y_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Regression (Many to Many Approach)\n",
    "### Designing the network\n",
    "\n",
    "- The first step is to define your network.\n",
    "- Neural networks are defined in Keras as a sequence of layers. The container for these layers is the **Sequential class**.\n",
    "- The first step is to create an instance of the Sequential class. Then you can create your layers and add them in the order that they should be connected.\n",
    "- The LSTM recurrent layer comprised of memory units is called LSTM().\n",
    "- A fully connected layer that often follows LSTM layers and is used for outputting a prediction is called Dense().\n",
    "- The first layer in the network must define the number of inputs to expect.\n",
    "- Input must be three-dimensional, comprised of samples, timesteps, and features.\n",
    "    - **Samples:** These are the rows in your data.\n",
    "    - **Timesteps:** These are the past observations for a feature, such as lag variables.\n",
    "    - **Features:** These are columns in your data.\n",
    "- Assuming your data is loaded as a NumPy array, you can convert a 2D dataset to a 3D dataset using the reshape() function in NumPy.\n",
    "\n",
    "### Relavent Links\n",
    "\n",
    "Network structure pointers [https://www.heatonresearch.com/2017/06/01/hidden-layers.html]. Rough heuristics to start with:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "* https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "* https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "* https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/\n",
    "* https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "* https://arxiv.org/pdf/1312.6026.pdf\n",
    "* https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "* https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# KerasModel Class\n",
    "class KerasModel:\n",
    "    \"\"\"\n",
    "    Long Short Term Memory Neural Net Class\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    def __init__(self, X_train, y_train, classification_classes, optimizer):\n",
    "        \"\"\"\n",
    "        Initiating the class creates a net with the established parameters\n",
    "        :param X_train - Training data used to train the model\n",
    "        :param y_train - Test data used to test the model\n",
    "        :param layers - A list of values, where in each value denotes a layer, and the number of neurons for that layer\n",
    "        :param loss_function - Function used to measure fitting of model (predicted from actual)\n",
    "        :param optimizer - Function used to optimize the model (eg: Gradient Descent)\n",
    "        \"\"\"\n",
    "        self.model = ke.models.Sequential()\n",
    "        #self.model.add(Embedding(classification_classes, 10, input_length=X_train.shape[1]))\n",
    "        #self.model.add(Flatten())\n",
    "#         self.model.add(ke.layers.LSTM(X_train.shape[2], input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "#         self.model.add(ke.layers.Dense(X_train.shape[2], activation='softmax'))\n",
    "        #self.model.add(ke.layers.LSTM(layers[i], input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "        #self.model.add(ke.layers.LSTM(layers[i]))\n",
    "        #\n",
    "        #self.model.add(Flatten())\n",
    "        #\n",
    "        self.model.add(ke.layers.LSTM(256, input_shape=(X_train[1], X_train.shape[2])))\n",
    "        self.model.add(ke.layers.Dropout(0.2))\n",
    "        self.model.add(ke.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "        self.loss_func = 'sparse_categorical_crossentropy'\n",
    "        self.model.compile(loss=self.loss_func, optimizer=optimizer, metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "\n",
    "    #\n",
    "    def fit_model(self, X_train, X_test, y_train, y_test, epochs=50, batch_size=50, verbose=2, shuffle=False,\n",
    "                  plot=False):\n",
    "        \"\"\"\n",
    "        Fit data to model & validate. Trains a number of epochs.\n",
    "        \"\"\"\n",
    "        history = self.model.fit(x=X_train,\n",
    "                                 y=y_train,\n",
    "                                 epochs=epochs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 validation_data=(X_test, y_test),\n",
    "                                 verbose=verbose,\n",
    "                                 shuffle=shuffle)\n",
    "        if plot:\n",
    "            plt.rcParams['figure.figsize'] = [20, 15]\n",
    "            plt.plot(history.history['acc'], label='train')\n",
    "            plt.plot(history.history['val_acc'], label='validation')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, y_labels, plot=False):\n",
    "        yhat = self.predict(X)\n",
    "        #\n",
    "        # F1-Score Evaluation\n",
    "        for i in range(y.shape[1]):\n",
    "            print(str(len(y[:, i])))\n",
    "            print(str(len(yhat[:, i])))\n",
    "            print(y[:, i])\n",
    "            print(yhat[:, i])\n",
    "            f1 = f1_score(y[:, i], \n",
    "                          yhat[:, i],\n",
    "                          average='micro')  # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "            print('Test FScore ' + y_labels[0] + ' with LAG value [' + str(i) + ']: ' +  str(f1))\n",
    "        #\n",
    "        if plot:\n",
    "            for i in range(0, y[0]):\n",
    "                plt.rcParams['figure.figsize'] = [20, 15]\n",
    "                plt.plot(y[:, i], label='actual')\n",
    "                plt.plot(yhat[:, i], label='predicted')\n",
    "                plt.legend(['actual', 'predicted'], loc='upper left')\n",
    "                plt.title(y_labels[i%len(y_labels)] + \" +\" + str(math.ceil((i+1)/len(y_label))))\n",
    "                plt.show()\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, batch, depth, epoch, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'batch', 'depth', 'epoch', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'batch': batch,\n",
    "                             'depth':depth,\n",
    "                             'epoch':epoch,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape [(1533, 4)] Type - [<class 'pandas.core.frame.DataFrame'>]\n",
      "y_train shape [(1533, 3, 379)] Type - [<class 'numpy.ndarray'>]\n",
      "X_validate shape [(192, 4)] Type - [<class 'pandas.core.frame.DataFrame'>]\n",
      "y_validate shape [(192, 3, 379)] Type - [<class 'numpy.ndarray'>]\n",
      "X_test shape [(192, 4)] Type - [<class 'pandas.core.frame.DataFrame'>]\n",
      "y_test shape [(192, 3, 379)] Type - [<class 'numpy.ndarray'>]\n",
      "\n",
      "\n",
      "      var1(t-3)  var1(t-2)  var1(t-1)  var1(t)\n",
      "1812      101.0       98.0       73.0       69\n",
      "1452       15.0      115.0      110.0      185\n",
      "1204      350.0      346.0      344.0      343\n",
      "1504      188.0      177.0       47.0      185\n",
      "621       152.0      148.0      147.0      143\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "------------------------------------------------------------\n",
      "      var1(t-3)  var1(t-2)  var1(t-1)  var1(t)\n",
      "1527      152.0       98.0      147.0      139\n",
      "1358      189.0      106.0      101.0       98\n",
      "34         43.0       62.0       67.0       84\n",
      "1417      350.0      344.0      339.0      337\n",
      "1019      301.0      300.0      296.0      287\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "------------------------------------------------------------\n",
      "     var1(t-3)  var1(t-2)  var1(t-1)  var1(t)\n",
      "377       39.0       30.0       25.0       23\n",
      "84        46.0       52.0       56.0       68\n",
      "514      316.0      186.0       19.0      182\n",
      "450       33.0       25.0       23.0        4\n",
      "138      328.0      323.0      320.0      319\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(1533, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-301c9a05e9a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mX_validate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    277\u001b[0m            [5, 6]])\n\u001b[0;32m    278\u001b[0m     \"\"\"\n\u001b[1;32m--> 279\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# a downstream library like 'pandas'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array_wrap__\u001b[1;34m(self, result, context)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_axes_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_ORDERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[1;31m# ideally we would define this to avoid the getattr checks, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[1;32m--> 379\u001b[1;33m                                          copy=copy)\n\u001b[0m\u001b[0;32m    380\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[1;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;31m# by definition an array here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m   7446\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7447\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7448\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Must pass 2-d input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7450\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input"
     ]
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_df, y_df, test_size=test_split)\n",
    "#\n",
    "# X_train = X_train.values\n",
    "# y_train = y_train.values\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"y_train shape [\" + str(y_train.shape) + \"] Type - [\" + str(type(y_train)) + \"]\")\n",
    "#\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, test_size=.5)\n",
    "#\n",
    "# X_validate = X_validate.values\n",
    "# y_validate = y_validate.values\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"y_validate shape [\" + str(y_validate.shape) + \"] Type - [\" + str(type(y_validate)) + \"]\")\n",
    "#\n",
    "# X_test = X_test.values\n",
    "# y_test = y_test.values\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "print(\"y_test shape [\" + str(y_test.shape) + \"] Type - [\" + str(type(y_test)) + \"]\")\n",
    "print(\"\\n\")\n",
    "#\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_validate[0:5])\n",
    "print(y_validate[0:5])\n",
    "print('------------------------------------------------------------')\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "#\n",
    "# Reshape for fitting in LSTM\n",
    "# X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "# X_validate = X_validate.reshape((X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "# X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(X_train.shape)\n",
    "#\n",
    "X_train = np.reshape(X_train,(X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_validate = np.reshape(X_validate,(X_validate.shape[0], 1, X_validate.shape[1]))\n",
    "X_test = np.reshape(X_test,(X_test.shape[0], 1, X_test.shape[1]))\n",
    "print('\\nReshaping Training Frames')\n",
    "print(\"X_train shape [\" + str(X_train.shape) + \"] Type - [\" + str(type(X_train)) + \"]\")\n",
    "print(\"X_validate shape [\" + str(X_validate.shape) + \"] Type - [\" + str(type(X_validate)) + \"]\")\n",
    "print(\"X_test shape [\" + str(X_test.shape) + \"] Type - [\" + str(type(X_test)) + \"]\")\n",
    "#\n",
    "# Train on discrete data (Train > Validation)\n",
    "discrete_model = KerasModel(X_train=X_train,\n",
    "                            y_train=y_train,\n",
    "                            classification_classes=len(le.classes_),\n",
    "                            optimizer='sgd')\n",
    "discrete_model.fit_model(X_train=X_train,\n",
    "                         X_test=X_validate,\n",
    "                         y_train=y_train,\n",
    "                         y_test=y_validate,\n",
    "                         epochs=epochs, \n",
    "                         batch_size=batch_size,\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_validate,\n",
    "                                    y=y_validate,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)\n",
    "#\n",
    "# Train on discrete data (Train + Validation > Test)\n",
    "discrete_model.fit_model(X_train=X_validate,\n",
    "                         X_test=X_test,\n",
    "                         y_train=y_validate,\n",
    "                         y_test=y_test,\n",
    "                         epochs=epochs, \n",
    "                         batch_size=1, # Incremental batch size fitting\n",
    "                         verbose=2, \n",
    "                         shuffle=False,\n",
    "                         plot=True)\n",
    "discrete_model.predict_and_evaluate(X=X_test,\n",
    "                                    y=y_test,\n",
    "                                    y_labels=y_label,\n",
    "                                    plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
