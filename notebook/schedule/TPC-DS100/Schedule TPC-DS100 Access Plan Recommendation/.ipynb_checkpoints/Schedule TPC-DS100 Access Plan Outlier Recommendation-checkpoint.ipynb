{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule Access Plan Outlier Recommendation\n",
    "\n",
    "This notebook is dedicated to model fitting in terms of database access plans. Access plans here are a representation for a particular query syntax, composed of underlying relational operators as to how data will be retrieved. This experiment deals with plan aggregation. Particularly, plans will be aggregated (summed) into a vector representation, which will then be gauged as an inlier or an outlier. Therefore the work contained within this experiment deals with flagging query access plans as eligable for further analysis - in our case, further analysis as to which optimizer statistics should be gathered.\n",
    "\n",
    "It should however be noted that this experiment poses a limitation. Access plans here are merely aggregated into a singular vector, representative of the underlying query cost. This aggregation does not indicate which sub-elements of the query are denoted as excessively expensive - this work is carried out in another experiment entirely.\n",
    "\n",
    "## Query to Access Plan Representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each query undergoes a set of processing steps when executed against a database instance. Apart from syntax and semantic checks which are carried out on the input syntax, a number of access plans are generated. Only one plan is chosen, based on the Cost Based Optimizer's decision in an effort to choose the least expensive one.\n",
    "\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\"><img src='Images/Query_translation.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"padding-bottom:0.5em;\" /><b>Query Translation Process</b></div>\n",
    "\n",
    "The query access plan is composed in a tree-like structure, where in each node of the tree acts as a row source. Each step of the access plan either retrieves rows from the database, or accepts rows from one or more row sources as input. The following example is extracted from - https://docs.oracle.com/database/121/TGSQL/tgsql_sqlproc.htm#TGSQL186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "SELECT e.last_name, j.job_title, d.department_name \n",
    "FROM   hr.employees e, hr.departments d, hr.jobs j\n",
    "WHERE  e.department_id = d.department_id\n",
    "AND    e.job_id = j.job_id\n",
    "AND    e.last_name LIKE 'A%';\n",
    "Execution Plan\n",
    "Plan hash value: 975837011\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "| Id| Operation                     | Name        |Rows|Bytes|Cost(%CPU)|Time  |\n",
    "| 0 | SELECT STATEMENT              |             |  3 | 189 | 7(15)| 00:00:01 |\n",
    "|*1 |  HASH JOIN                    |             |  3 | 189 | 7(15)| 00:00:01 |\n",
    "|*2 |   HASH JOIN                   |             |  3 | 141 | 5(20)| 00:00:01 |\n",
    "| 3 |    TABLE ACCESS BY INDEX ROWID| EMPLOYEES   |  3 |  60 | 2 (0)| 00:00:01 |\n",
    "|*4 |     INDEX RANGE SCAN          | EMP_NAME_IX |  3 |     | 1 (0)| 00:00:01 |\n",
    "| 5 |    TABLE ACCESS FULL          | JOBS        | 19 | 513 | 2 (0)| 00:00:01 |\n",
    "| 6 |   TABLE ACCESS FULL           | DEPARTMENTS | 27 | 432 | 2 (0)| 00:00:01 |\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Predicate Information (identified by operation id):\n",
    " \n",
    "   1 - access(\"E\".\"DEPARTMENT_ID\"=\"D\".\"DEPARTMENT_ID\")\n",
    "   2 - access(\"E\".\"JOB_ID\"=\"J\".\"JOB_ID\")\n",
    "   4 - access(\"E\".\"LAST_NAME\" LIKE 'A%')\n",
    "       filter(\"E\".\"LAST_NAME\" LIKE 'A%')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:80%; text-align:center;\"><img src='Images/Row_source_tree.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"padding-bottom:0.5em;\" /><b>Access Plan Row Source Tree Representation</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 0.24.1\n",
      "numpy: 1.16.1\n",
      "sklearn: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# sklearn\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "print('sklearn: %s' % sk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Config\n",
    "tpcds='TPCDS100' # Schema upon which to operate test\n",
    "test_split=.2\n",
    "y_labels = ['COST',\n",
    "            'CARDINALITY',\n",
    "            'BYTES',\n",
    "            'CPU_COST',\n",
    "            'IO_COST',\n",
    "            'TEMP_SPACE',\n",
    "            'TIME']\n",
    "nrows = 10000\n",
    "dtype={'COST':'int64',\n",
    "       'CARDINALITY':'int64',\n",
    "       'BYTES':'int64',\n",
    "       'CPU_COST':'int64',\n",
    "       'IO_COST':'int64',\n",
    "       'TEMP_SPACE':'int64',\n",
    "       'TIME':'int64'}\n",
    "black_list = ['TIMESTAMP','SQL_ID'] # Columns which will be ignored during type conversion, and later used for aggregation\n",
    "\n",
    "# Random Forest Config\n",
    "parallel_degree = 4\n",
    "n_estimators = 500\n",
    "max_depth = 7\n",
    "criterion='gini'\n",
    "\n",
    "# Isolation Forest Config\n",
    "contamination = .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ('DBID',)    ('SQL_ID',)  ('PLAN_HASH_VALUE',)  ('ID',)    ('OPERATION',)  \\\n",
      "0  2634225673  2j8td2wuthnfv            1917374110        0  SELECT STATEMENT   \n",
      "1  2634225673  2j8td2wuthnfv            1917374110        1      TABLE ACCESS   \n",
      "2  2634225673  2j8td2wuthnfv            1917374110        2              SORT   \n",
      "3  2634225673  2j8td2wuthnfv            1917374110        3      TABLE ACCESS   \n",
      "4  2634225673  9nf3gy0tv9p0u            3537130676        0  SELECT STATEMENT   \n",
      "\n",
      "  ('OPTIONS',) ('OBJECT_NODE',)  ('OBJECT#',) ('OBJECT_OWNER',)  \\\n",
      "0          NaN              NaN           NaN               NaN   \n",
      "1         FULL              NaN        8693.0               SYS   \n",
      "2    AGGREGATE              NaN           NaN               NaN   \n",
      "3         FULL              NaN        8693.0               SYS   \n",
      "4          NaN              NaN           NaN               NaN   \n",
      "\n",
      "  ('OBJECT_NAME',)  ... ('ACCESS_PREDICATES',) ('FILTER_PREDICATES',)  \\\n",
      "0              NaN  ...                    NaN                    NaN   \n",
      "1    WRM$_SNAPSHOT  ...                    NaN                    NaN   \n",
      "2              NaN  ...                    NaN                    NaN   \n",
      "3    WRM$_SNAPSHOT  ...                    NaN                    NaN   \n",
      "4              NaN  ...                    NaN                    NaN   \n",
      "\n",
      "  ('PROJECTION',)  ('TIME',)  ('QBLOCK_NAME',)  ('REMARKS',)  \\\n",
      "0             NaN        NaN               NaN           NaN   \n",
      "1             NaN        1.0      SEL$5C160134           NaN   \n",
      "2             NaN        NaN      SEL$F5B21678           NaN   \n",
      "3             NaN        1.0      SEL$F5B21678           NaN   \n",
      "4             NaN        NaN               NaN           NaN   \n",
      "\n",
      "        ('TIMESTAMP',)                                     ('OTHER_XML',)  \\\n",
      "0  2018-10-08 00:37:51                                                NaN   \n",
      "1  2018-10-08 00:37:51  <other_xml><info type=\"db_version\">12.1.0.2</i...   \n",
      "2  2018-10-08 00:37:51                                                NaN   \n",
      "3  2018-10-08 00:37:51                                                NaN   \n",
      "4  2018-12-27 13:19:00                                                NaN   \n",
      "\n",
      "   ('CON_DBID',)  ('CON_ID',)  \n",
      "0     2634225673            0  \n",
      "1     2634225673            0  \n",
      "2     2634225673            0  \n",
      "3     2634225673            0  \n",
      "4     2634225673            0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "------------------------------------------\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Root path\n",
    "base_dir = 'C:/Users/gabriel.sammut/University/'\n",
    "#base_dir = 'D:/Projects/ICS5200/'\n",
    "root_dir = base_dir + 'Data_ICS5200/Schedule/' + tpcds\n",
    "src_dir = base_dir + 'ICS5200/src/sql/Runtime/TPC-DS/' + tpcds + '/Variants/'\n",
    "\n",
    "rep_vsql_plan_path = root_dir + '/rep_vsql_plan.csv'\n",
    "#rep_vsql_plan_path = root_dir + '/rep_vsql_plan.csv'\n",
    "\n",
    "rep_vsql_plan_df = pd.read_csv(rep_vsql_plan_path,nrows=nrows,dtype=dtype)\n",
    "print(rep_vsql_plan_df.head())\n",
    "\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "\n",
    "rep_vsql_plan_df.columns = prettify_header(rep_vsql_plan_df.columns.values)\n",
    "print('------------------------------------------')\n",
    "print(rep_vsql_plan_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read outlier data from file into pandas dataframes and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1433, 36)\n",
      "  PLAN_ID            TIMESTAMP REMARKS         OPERATION          OPTIONS  \\\n",
      "0   12447  11/20/2018 09:56:46     NaN  SELECT STATEMENT              NaN   \n",
      "1   12447  11/20/2018 09:56:46     NaN             COUNT          STOPKEY   \n",
      "2   12447  11/20/2018 09:56:46     NaN              VIEW              NaN   \n",
      "3   12447  11/20/2018 09:56:46     NaN              SORT  GROUP BY ROLLUP   \n",
      "4   12447  11/20/2018 09:56:46     NaN              VIEW              NaN   \n",
      "\n",
      "  OBJECT_NODE OBJECT_OWNER OBJECT_NAME                OBJECT_ALIAS  \\\n",
      "0         NaN          NaN         NaN                         NaN   \n",
      "1         NaN          NaN         NaN                         NaN   \n",
      "2         NaN     TPCDS100         NaN  from$_subquery$_018@SEL$11   \n",
      "3         NaN          NaN         NaN                         NaN   \n",
      "4         NaN     TPCDS100         NaN                    X@SEL$12   \n",
      "\n",
      "  OBJECT_INSTANCE  ... DISTRIBUTION  CPU_COST  IO_COST TEMP_SPACE  \\\n",
      "0             NaN  ...          NaN  1.35E+11  1176244        NaN   \n",
      "1             NaN  ...          NaN       NaN      NaN        NaN   \n",
      "2              18  ...          NaN  1.35E+11  1176244        NaN   \n",
      "3             NaN  ...          NaN  1.35E+11  1176244        NaN   \n",
      "4              19  ...          NaN  1.35E+11  1176244        NaN   \n",
      "\n",
      "  ACCESS_PREDICATES FILTER_PREDICATES  \\\n",
      "0               NaN               NaN   \n",
      "1               NaN       ROWNUM<=100   \n",
      "2               NaN               NaN   \n",
      "3               NaN               NaN   \n",
      "4               NaN               NaN   \n",
      "\n",
      "                                          PROJECTION TIME QBLOCK_NAME  \\\n",
      "0                                                NaN   47         NaN   \n",
      "1  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...  NaN      SEL$11   \n",
      "2  from$_subquery$_018.\"CHANNEL\"[VARCHAR2,15], \"f...   47      SEL$12   \n",
      "3  (#keys=2) \"CHANNEL\"[VARCHAR2,15], \"ID\"[VARCHAR...   47      SEL$12   \n",
      "4  CHANNEL[VARCHAR2,15], \"ID\"[VARCHAR2,28], \"SALE...   47       SET$4   \n",
      "\n",
      "  Unnamed: 35  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "------------------------------------------\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'REMARKS', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'ID',\n",
      "       'PARENT_ID', 'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES',\n",
      "       'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID',\n",
      "       'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST',\n",
      "       'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION',\n",
      "       'TIME', 'QBLOCK_NAME', 'Unnamed: 35'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# CSV Outlier Paths\n",
    "outlier_hints_q5_path = src_dir + 'hints/output/query_5.csv'\n",
    "outlier_hints_q10_path = src_dir + 'hints/output/query_10.csv'\n",
    "outlier_hints_q14_path = src_dir + 'hints/output/query_14.csv'\n",
    "outlier_hints_q18_path = src_dir + 'hints/output/query_18.csv'\n",
    "outlier_hints_q22_path = src_dir + 'hints/output/query_22.csv'\n",
    "outlier_hints_q27_path = src_dir + 'hints/output/query_27.csv'\n",
    "outlier_hints_q35_path = src_dir + 'hints/output/query_35.csv'\n",
    "outlier_hints_q36_path = src_dir + 'hints/output/query_36.csv'\n",
    "outlier_hints_q51_path = src_dir + 'hints/output/query_51.csv'\n",
    "outlier_hints_q67_path = src_dir + 'hints/output/query_67.csv'\n",
    "outlier_hints_q70_path = src_dir + 'hints/output/query_70.csv'\n",
    "outlier_hints_q77_path = src_dir + 'hints/output/query_77.csv'\n",
    "outlier_hints_q80_path = src_dir + 'hints/output/query_80.csv'\n",
    "outlier_hints_q86_path = src_dir + 'hints/output/query_86.csv'\n",
    "#\n",
    "outlier_predicates_q5_path = src_dir + 'predicates/output/query_5.csv'\n",
    "outlier_predicates_q10_path = src_dir + 'predicates/output/query_10.csv'\n",
    "outlier_predicates_q14_path = src_dir + 'predicates/output/query_14.csv'\n",
    "outlier_predicates_q18_path = src_dir + 'predicates/output/query_18.csv'\n",
    "outlier_predicates_q22_path = src_dir + 'predicates/output/query_22.csv'\n",
    "outlier_predicates_q27_path = src_dir + 'predicates/output/query_27.csv'\n",
    "outlier_predicates_q35_path = src_dir + 'predicates/output/query_35.csv'\n",
    "outlier_predicates_q36_path = src_dir + 'predicates/output/query_36.csv'\n",
    "outlier_predicates_q51_path = src_dir + 'predicates/output/query_51.csv'\n",
    "outlier_predicates_q67_path = src_dir + 'predicates/output/query_67.csv'\n",
    "outlier_predicates_q70_path = src_dir + 'predicates/output/query_70.csv'\n",
    "outlier_predicates_q77_path = src_dir + 'predicates/output/query_77.csv'\n",
    "outlier_predicates_q80_path = src_dir + 'predicates/output/query_80.csv'\n",
    "outlier_predicates_q86_path = src_dir + 'predicates/output/query_86.csv'\n",
    "#\n",
    "outlier_rownum_q5_path = src_dir + 'rownum/output/query_5.csv'\n",
    "outlier_rownum_q10_path = src_dir + 'rownum/output/query_10.csv'\n",
    "outlier_rownum_q14_path = src_dir + 'rownum/output/query_14.csv'\n",
    "outlier_rownum_q18_path = src_dir + 'rownum/output/query_18.csv'\n",
    "outlier_rownum_q22_path = src_dir + 'rownum/output/query_22.csv'\n",
    "outlier_rownum_q27_path = src_dir + 'rownum/output/query_27.csv'\n",
    "outlier_rownum_q35_path = src_dir + 'rownum/output/query_35.csv'\n",
    "outlier_rownum_q36_path = src_dir + 'rownum/output/query_36.csv'\n",
    "outlier_rownum_q51_path = src_dir + 'rownum/output/query_51.csv'\n",
    "outlier_rownum_q67_path = src_dir + 'rownum/output/query_67.csv'\n",
    "outlier_rownum_q70_path = src_dir + 'rownum/output/query_70.csv'\n",
    "outlier_rownum_q77_path = src_dir + 'rownum/output/query_77.csv'\n",
    "outlier_rownum_q80_path = src_dir + 'rownum/output/query_80.csv'\n",
    "outlier_rownum_q86_path = src_dir + 'rownum/output/query_86.csv'\n",
    "#\n",
    "# Read CSV Paths\n",
    "outlier_hints_q5_df = pd.read_csv(outlier_hints_q5_path,dtype=str)\n",
    "outlier_hints_q10_df = pd.read_csv(outlier_hints_q10_path,dtype=str)\n",
    "outlier_hints_q14_df = pd.read_csv(outlier_hints_q14_path,dtype=str)\n",
    "outlier_hints_q18_df = pd.read_csv(outlier_hints_q18_path,dtype=str)\n",
    "outlier_hints_q22_df = pd.read_csv(outlier_hints_q22_path,dtype=str)\n",
    "outlier_hints_q27_df = pd.read_csv(outlier_hints_q27_path,dtype=str)\n",
    "outlier_hints_q35_df = pd.read_csv(outlier_hints_q35_path,dtype=str)\n",
    "outlier_hints_q36_df = pd.read_csv(outlier_hints_q36_path,dtype=str)\n",
    "outlier_hints_q51_df = pd.read_csv(outlier_hints_q51_path,dtype=str)\n",
    "outlier_hints_q67_df = pd.read_csv(outlier_hints_q67_path,dtype=str)\n",
    "outlier_hints_q70_df = pd.read_csv(outlier_hints_q70_path,dtype=str)\n",
    "outlier_hints_q77_df = pd.read_csv(outlier_hints_q77_path,dtype=str)\n",
    "outlier_hints_q80_df = pd.read_csv(outlier_hints_q80_path,dtype=str)\n",
    "outlier_hints_q86_df = pd.read_csv(outlier_hints_q86_path,dtype=str)\n",
    "#\n",
    "outlier_predicates_q5_df = pd.read_csv(outlier_predicates_q5_path,dtype=str)\n",
    "outlier_predicates_q10_df = pd.read_csv(outlier_predicates_q10_path,dtype=str)\n",
    "outlier_predicates_q14_df = pd.read_csv(outlier_predicates_q14_path,dtype=str)\n",
    "outlier_predicates_q18_df = pd.read_csv(outlier_predicates_q18_path,dtype=str)\n",
    "outlier_predicates_q22_df = pd.read_csv(outlier_predicates_q22_path,dtype=str)\n",
    "outlier_predicates_q27_df = pd.read_csv(outlier_predicates_q27_path,dtype=str)\n",
    "outlier_predicates_q35_df = pd.read_csv(outlier_predicates_q35_path,dtype=str)\n",
    "outlier_predicates_q36_df = pd.read_csv(outlier_predicates_q36_path,dtype=str)\n",
    "outlier_predicates_q51_df = pd.read_csv(outlier_predicates_q51_path,dtype=str)\n",
    "outlier_predicates_q67_df = pd.read_csv(outlier_predicates_q67_path,dtype=str)\n",
    "outlier_predicates_q70_df = pd.read_csv(outlier_predicates_q70_path,dtype=str)\n",
    "outlier_predicates_q77_df = pd.read_csv(outlier_predicates_q77_path,dtype=str)\n",
    "outlier_predicates_q80_df = pd.read_csv(outlier_predicates_q80_path,dtype=str)\n",
    "outlier_predicates_q86_df = pd.read_csv(outlier_predicates_q86_path,dtype=str)\n",
    "#\n",
    "outlier_rownum_q5_df = pd.read_csv(outlier_rownum_q5_path,dtype=str)\n",
    "outlier_rownum_q10_df = pd.read_csv(outlier_rownum_q10_path,dtype=str)\n",
    "outlier_rownum_q14_df = pd.read_csv(outlier_rownum_q14_path,dtype=str)\n",
    "outlier_rownum_q18_df = pd.read_csv(outlier_rownum_q18_path,dtype=str)\n",
    "outlier_rownum_q22_df = pd.read_csv(outlier_rownum_q22_path,dtype=str)\n",
    "outlier_rownum_q27_df = pd.read_csv(outlier_rownum_q27_path,dtype=str)\n",
    "outlier_rownum_q35_df = pd.read_csv(outlier_rownum_q35_path,dtype=str)\n",
    "outlier_rownum_q36_df = pd.read_csv(outlier_rownum_q36_path,dtype=str)\n",
    "outlier_rownum_q51_df = pd.read_csv(outlier_rownum_q51_path,dtype=str)\n",
    "outlier_rownum_q67_df = pd.read_csv(outlier_rownum_q67_path,dtype=str)\n",
    "outlier_rownum_q70_df = pd.read_csv(outlier_rownum_q70_path,dtype=str)\n",
    "outlier_rownum_q77_df = pd.read_csv(outlier_rownum_q77_path,dtype=str)\n",
    "outlier_rownum_q80_df = pd.read_csv(outlier_rownum_q80_path,dtype=str)\n",
    "outlier_rownum_q86_df = pd.read_csv(outlier_rownum_q86_path,dtype=str)\n",
    "#\n",
    "# Merge dataframes into a single pandas matrix\n",
    "df_outliers = pd.concat([outlier_hints_q5_df, outlier_hints_q10_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q14_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q18_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q22_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q27_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q35_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q36_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q51_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q67_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q70_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q77_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q80_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_hints_q86_df], sort=False)\n",
    "#\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q5_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q10_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q14_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q18_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q22_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q27_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q35_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q36_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q51_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q67_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q70_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q77_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q80_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_predicates_q86_df], sort=False)\n",
    "#\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q5_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q10_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q14_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q18_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q22_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q27_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q35_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q36_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q51_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q67_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q70_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q77_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q80_df], sort=False)\n",
    "df_outliers = pd.concat([df_outliers, outlier_rownum_q86_df], sort=False)   \n",
    "#\n",
    "print(df_outliers.shape)\n",
    "print(df_outliers.head())\n",
    "print('------------------------------------------')\n",
    "print(df_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A Columns\n",
      "\n",
      "\n",
      "REP_VSQL_PLAN Features 39: ['OPTIONS', 'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME', 'REMARKS', 'OTHER_XML']\n",
      "\n",
      "\n",
      "DF_OUTLIERS Features 36: ['PLAN_ID', 'REMARKS', 'OPERATION', 'OPTIONS', 'OBJECT_NODE', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_INSTANCE', 'OBJECT_TYPE', 'OPTIMIZER', 'SEARCH_COLUMNS', 'ID', 'PARENT_ID', 'DEPTH', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'OTHER_XML', 'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME', 'QBLOCK_NAME', 'Unnamed: 35']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_na_columns(df, headers):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in headers:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "#\n",
    "print('N/A Columns\\n')\n",
    "print('\\nREP_VSQL_PLAN Features ' + str(len(rep_vsql_plan_df.columns)) + ': ' + str(get_na_columns(df=rep_vsql_plan_df,headers=rep_vsql_plan_df.columns)) + \"\\n\")\n",
    "print('\\nDF_OUTLIERS Features ' + str(len(df_outliers.columns)) + ': ' + str(get_na_columns(df=df_outliers,headers=df_outliers.columns)) + \"\\n\")\n",
    "#\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "#\n",
    "# Populating NaN values with amount '0'\n",
    "df = fill_na(df=rep_vsql_plan_df)\n",
    "df_outliers = fill_na(df=df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversion\n",
    "\n",
    "Each column is converted into a column of type values which are Integer64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Dropped column [REMARKS]\n",
      "Dropped column [OPERATION]\n",
      "Dropped column [OPTIONS]\n",
      "Dropped column [OBJECT_NODE]\n",
      "Dropped column [OBJECT_OWNER]\n",
      "Dropped column [OBJECT_NAME]\n",
      "Dropped column [OBJECT_ALIAS]\n",
      "Dropped column [OBJECT_INSTANCE]\n",
      "Dropped column [OBJECT_TYPE]\n",
      "Dropped column [OPTIMIZER]\n",
      "Dropped column [SEARCH_COLUMNS]\n",
      "Dropped column [OTHER_XML]\n",
      "Dropped column [DISTRIBUTION]\n",
      "Dropped column [CPU_COST]\n",
      "Dropped column [IO_COST]\n",
      "Dropped column [ACCESS_PREDICATES]\n",
      "Dropped column [FILTER_PREDICATES]\n",
      "Dropped column [PROJECTION]\n",
      "Dropped column [TIME]\n",
      "Dropped column [QBLOCK_NAME]\n",
      "Dropped column [Unnamed: 35]\n",
      "Index(['DBID', 'SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPERATION', 'OPTIONS',\n",
      "       'OBJECT_NODE', 'OBJECT#', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS',\n",
      "       'OBJECT_TYPE', 'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION',\n",
      "       'SEARCH_COLUMNS', 'COST', 'CARDINALITY', 'BYTES', 'OTHER_TAG',\n",
      "       'PARTITION_START', 'PARTITION_STOP', 'PARTITION_ID', 'OTHER',\n",
      "       'DISTRIBUTION', 'CPU_COST', 'IO_COST', 'TEMP_SPACE',\n",
      "       'ACCESS_PREDICATES', 'FILTER_PREDICATES', 'PROJECTION', 'TIME',\n",
      "       'QBLOCK_NAME', 'REMARKS', 'TIMESTAMP', 'OTHER_XML', 'CON_DBID',\n",
      "       'CON_ID'],\n",
      "      dtype='object')\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'OTHER', 'TEMP_SPACE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def handle_numeric_overflows(x):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe column, and \n",
    "    \"\"\"\n",
    "    try:\n",
    "        #df = df.astype('int64')\n",
    "        x1 = pd.DataFrame([x],dtype='int64')\n",
    "    except ValueError:\n",
    "        x = 9223372036854775807 # Max int size\n",
    "    return x\n",
    "#\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = df[col].apply(handle_numeric_overflows)\n",
    "        df[col].astype('int64',inplace=True)\n",
    "    except:\n",
    "        if col not in black_list:\n",
    "            df.drop(columns=col, inplace=True)\n",
    "            print('Dropped column [' + col + ']')\n",
    "#\n",
    "print('-------------------------------------------------------------')\n",
    "#\n",
    "for col in df_outliers.columns:\n",
    "    try:\n",
    "        df_outliers[col] = df_outliers[col].astype('int64')\n",
    "    except OverflowError:\n",
    "        #\n",
    "        # Handles numeric overflow conversions by replacing such values with max value inside the dataset.\n",
    "        df_outliers[col] = df_outliers[col].apply(handle_numeric_overflows)\n",
    "        df_outliers[col] = df_outliers[col].astype('int64')\n",
    "    except Exception as e:\n",
    "        if col not in black_list:\n",
    "            df_outliers.drop(columns=col, inplace=True)\n",
    "            print('Dropped column [' + col + ']')\n",
    "print(df.columns)\n",
    "print(df_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape before changes: [(10000, 39)]\n",
      "Shape after changes: [(10000, 30)]\n",
      "Dropped a total [9]\n",
      "\n",
      "Shape before changes: [(1433, 15)]\n",
      "Shape after changes: [(1433, 11)]\n",
      "Dropped a total [4]\n",
      "\n",
      "After flatline column drop:\n",
      "(10000, 30)\n",
      "Index(['SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPTIONS', 'OBJECT_NODE', 'OBJECT#',\n",
      "       'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE',\n",
      "       'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION', 'SEARCH_COLUMNS', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_START',\n",
      "       'PARTITION_STOP', 'PARTITION_ID', 'DISTRIBUTION', 'CPU_COST', 'IO_COST',\n",
      "       'TEMP_SPACE', 'TIME', 'QBLOCK_NAME', 'TIMESTAMP', 'OTHER_XML'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------\n",
      "\n",
      "After outlier flatline column drop:\n",
      "(1433, 11)\n",
      "Index(['PLAN_ID', 'TIMESTAMP', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'TEMP_SPACE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def drop_flatline_columns(df):\n",
    "    columns = df.columns\n",
    "    flatline_features = []\n",
    "    for i in range(len(columns)):\n",
    "        try:\n",
    "            #\n",
    "            if columns[i] in black_list:\n",
    "                continue\n",
    "            #\n",
    "            std = df[columns[i]].std()\n",
    "            if std == 0:\n",
    "                flatline_features.append(columns[i])\n",
    "        except:\n",
    "            pass\n",
    "    #\n",
    "    #print('Features which are considered flatline:\\n')\n",
    "    #for col in flatline_features:\n",
    "    #    print(col)\n",
    "    print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "    df = df.drop(columns=flatline_features)\n",
    "    print('Shape after changes: [' + str(df.shape) + ']')\n",
    "    print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "    return df\n",
    "#\n",
    "df = drop_flatline_columns(df=df)\n",
    "df_outliers = drop_flatline_columns(df=df_outliers)\n",
    "#\n",
    "print('\\nAfter flatline column drop:')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "#\n",
    "print('--------------------------------------------------------')\n",
    "print('\\nAfter outlier flatline column drop:')\n",
    "print(df_outliers.shape)\n",
    "print(df_outliers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Converting labels/features into numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels:\n",
      "['OPERATION', 'OPTIONS', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'QBLOCK_NAME']\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "                SQL_ID  PLAN_HASH_VALUE  ID  OPTIONS  OBJECT_NODE  OBJECT#  \\\n",
      "0  9223372036854775807       1917374110   0        0            0      0.0   \n",
      "1  9223372036854775807       1917374110   1        1            0   8693.0   \n",
      "2  9223372036854775807       1917374110   2        1            0      0.0   \n",
      "3  9223372036854775807       1917374110   3        1            0   8693.0   \n",
      "4  9223372036854775807       3537130676   0        0            0      0.0   \n",
      "\n",
      "   OBJECT_OWNER  OBJECT_NAME  OBJECT_ALIAS  OBJECT_TYPE  ...  PARTITION_STOP  \\\n",
      "0             0            0             0            0  ...               0   \n",
      "1             1            1             1            1  ...               0   \n",
      "2             0            0             0            0  ...               0   \n",
      "3             1            1             1            1  ...               0   \n",
      "4             0            0             0            0  ...               0   \n",
      "\n",
      "   PARTITION_ID  DISTRIBUTION  CPU_COST  IO_COST  TEMP_SPACE  TIME  \\\n",
      "0           0.0             0       0.0      0.0         0.0   0.0   \n",
      "1           0.0             0  200629.0      5.0         0.0   1.0   \n",
      "2           0.0             0       0.0      0.0         0.0   0.0   \n",
      "3           0.0             0  200579.0      5.0         0.0   1.0   \n",
      "4           0.0             0       0.0      0.0         0.0   0.0   \n",
      "\n",
      "   QBLOCK_NAME            TIMESTAMP            OTHER_XML  \n",
      "0            0  9223372036854775807                    0  \n",
      "1            1  9223372036854775807  9223372036854775807  \n",
      "2            1  9223372036854775807                    0  \n",
      "3            1  9223372036854775807                    0  \n",
      "4            0  9223372036854775807                    0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Encoded labels:\n",
      "['OPERATION', 'OPTIONS', 'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE', 'OPTIMIZER', 'QBLOCK_NAME']\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "   PLAN_ID            TIMESTAMP  ID  PARENT_ID  DEPTH  POSITION     COST  \\\n",
      "0    12447  11/20/2018 09:56:46   0          0      0   1180457  1180457   \n",
      "1    12447  11/20/2018 09:56:46   1          0      1         1        0   \n",
      "2    12447  11/20/2018 09:56:46   2          1      2         1  1180457   \n",
      "3    12447  11/20/2018 09:56:46   3          2      3         1  1180457   \n",
      "4    12447  11/20/2018 09:56:46   4          3      4         1  1180456   \n",
      "\n",
      "   CARDINALITY   BYTES  OTHER_TAG  TEMP_SPACE  \n",
      "0          100    6400          0           0  \n",
      "1            0       0          0           0  \n",
      "2         4550  291200          0           0  \n",
      "3         4550  291200          0           0  \n",
      "4         4550  291200          0           0  \n"
     ]
    }
   ],
   "source": [
    "def encode(df, encoded_labels):\n",
    "    for col in df.columns:\n",
    "        if col in encoded_labels:\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "#\n",
    "# Determine labels used for encoding\n",
    "encoded_labels = ['OPERATION','OPTIONS','OBJECT_OWNER','OBJECT_NAME','OBJECT_ALIAS','OBJECT_TYPE','OPTIMIZER','QBLOCK_NAME']\n",
    "#\n",
    "df = encode(df=df, encoded_labels=encoded_labels)\n",
    "print('Encoded labels:\\n' + str(encoded_labels) + \"\\n\\n----------------------------------------------\\n\\n\")\n",
    "print(df.head())\n",
    "#\n",
    "df_outliers = encode(df=df_outliers, encoded_labels=encoded_labels)\n",
    "print('Encoded labels:\\n' + str(encoded_labels) + \"\\n\\n----------------------------------------------\\n\\n\")\n",
    "print(df_outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer conversion\n",
    "\n",
    "Each column is converted into a column of type values which are integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Inliers')\n",
    "# df[y_labels] = df[y_labels].astype('int64')\n",
    "# print(type(df))\n",
    "# print(df.shape)\n",
    "# #\n",
    "# print('Outliers')\n",
    "# df_outliers[y_labels] = df_outliers[y_labels].astype('int64')\n",
    "# print(type(df_outliers))\n",
    "# print(df_outliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Plan Resource Aggregation (Per Access Plan Type)\n",
    "\n",
    "This method attempts to tackle the problem of access plan anomolies by aggregating resources per explain plan. Notable resources which are being considered are as follows:\n",
    "\n",
    "* COST\n",
    "* CARDINALITY\n",
    "* BYTES\n",
    "* PARTITION_DELTA (Partition End - Partition Start)\n",
    "* CPU_COST\n",
    "* IO_COST\n",
    "* TEMP_SPACE\n",
    "* TIME\n",
    "\n",
    "The reasoning behind these fields in particular is mainly because these columns can be aggregated together. Aggregation is carried on per access plan type. Aggregation at this phase ensures that each SQL ID is represented as a mean vector, which represents the PLAN_HASH_VALUE as a vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "(10000, 30)\n",
      "Index(['SQL_ID', 'PLAN_HASH_VALUE', 'ID', 'OPTIONS', 'OBJECT_NODE', 'OBJECT#',\n",
      "       'OBJECT_OWNER', 'OBJECT_NAME', 'OBJECT_ALIAS', 'OBJECT_TYPE',\n",
      "       'OPTIMIZER', 'PARENT_ID', 'DEPTH', 'POSITION', 'SEARCH_COLUMNS', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'PARTITION_ID', 'DISTRIBUTION',\n",
      "       'CPU_COST', 'IO_COST', 'TEMP_SPACE', 'TIME', 'QBLOCK_NAME', 'TIMESTAMP',\n",
      "       'OTHER_XML'],\n",
      "      dtype='object')\n",
      "(236, 28)\n",
      "-------------------\n",
      "After\n",
      "(1433, 11)\n",
      "Index(['PLAN_ID', 'ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST',\n",
      "       'CARDINALITY', 'BYTES', 'OTHER_TAG', 'TEMP_SPACE'],\n",
      "      dtype='object')\n",
      "(42, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Before')\n",
    "print(df.shape)\n",
    "df_agg = df.groupby(['SQL_ID','PLAN_HASH_VALUE']).mean()\n",
    "df_agg.reset_index(inplace=True)\n",
    "print(df_agg.columns)\n",
    "print(df_agg.shape)\n",
    "#\n",
    "print('-------------------\\nAfter')\n",
    "print(df_outliers.shape)\n",
    "df_outliers_agg = df_outliers.groupby(['PLAN_ID']).mean()\n",
    "df_outliers_agg.reset_index(inplace=True)\n",
    "print(df_outliers_agg.columns)\n",
    "print(df_outliers_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Reduction\n",
    "\n",
    "Strips further columns unneccessary to the experiment, so as to have the same columns for both training data set and outlier set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES',\n",
      "       'OTHER_TAG', 'TEMP_SPACE'],\n",
      "      dtype='object')\n",
      "(236, 9)\n",
      "------------------------------------------\n",
      "Index(['ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES',\n",
      "       'OTHER_TAG', 'TEMP_SPACE'],\n",
      "      dtype='object')\n",
      "(42, 9)\n"
     ]
    }
   ],
   "source": [
    "for col in df_outliers_agg.columns:\n",
    "    if col not in df_agg.columns:\n",
    "        df_outliers_agg.drop(columns=[col], inplace=True)\n",
    "for col in df_agg.columns:\n",
    "    if col not in df_outliers_agg.columns:\n",
    "        df_agg.drop(columns=[col], inplace=True)\n",
    "#\n",
    "print(df_agg.columns)\n",
    "print(df_agg.shape)\n",
    "print('------------------------------------------')\n",
    "print(df_outliers_agg.columns)\n",
    "print(df_outliers_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training (Random Forest - Per Access Plan Type)\n",
    "\n",
    "The following section oversees the supervised training of inlier/outlier explain plans. This section first splits the training dataset into two: train + test sets, by mixing half the outliers with the inlier training set. Validation is then performed on a 50/50 mix of inlier / outlier vectors. The ability to classify explain plan vectors as inliers / outliers will be gauged.\n",
    "\n",
    "Labels are denoted as follows:\n",
    "* Inliers  - 0\n",
    "* Outliers - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random Forest\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest Class (Regression + Classification)\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, n_estimators, max_depth=None, criterion='gini', parallel_degree=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.parallel_degree=parallel_degree\n",
    "        self.criterion = criterion\n",
    "        self.model = RandomForestClassifier(max_depth=self.max_depth,\n",
    "                                            n_estimators=self.n_estimators,\n",
    "                                            criterion=self.criterion,\n",
    "                                            n_jobs=self.parallel_degree)\n",
    "    #\n",
    "    def fit_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits training data to target labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X,y)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "    #\n",
    "    def predict_and_evaluate(self, X, y, plot=False):\n",
    "        \"\"\"\n",
    "        Runs test data through previously trained model, and evaluate differently depending if a regression of classification model\n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        acc = accuracy_score(y, yhat, normalize=True)\n",
    "        precision = precision_score(y, yhat, average='binary')\n",
    "        recall = recall_score(y, yhat, average='binary')\n",
    "        f1 = f1_score(y, yhat, average='binary')\n",
    "        print('Test Accuracy: ' +  str(acc))\n",
    "        print('Test Precision: ' +  str(precision))\n",
    "        print('Test Recall: ' +  str(recall))\n",
    "        print('Test FScore: ' +  str(f1) + \"\\n\")\n",
    "    #\n",
    "    @staticmethod\n",
    "    def write_results_to_disk(path, iteration, lag, test_split, estimator, score, time_train):\n",
    "        file_exists = os.path.isfile(path)\n",
    "        with open(path, 'a') as csvfile:\n",
    "            headers = ['iteration', 'lag', 'test_split', 'estimator', 'score', 'time_train']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "            writer.writerow({'iteration': iteration,\n",
    "                             'lag': lag,\n",
    "                             'test_split': test_split,\n",
    "                             'estimator': estimator,\n",
    "                             'score': score,\n",
    "                             'time_train': time_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.35000000e+01 1.50208333e+01 3.81250000e+00 ... 4.91416667e+02\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [4.50000000e+00 3.60000000e+00 4.50000000e+00 ... 1.71056880e+06\n",
      "  6.45636043e+18 0.00000000e+00]\n",
      " [2.50000000e+00 1.66666667e+00 2.50000000e+00 ... 4.51695758e+04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [1.20000000e+01 1.02000000e+01 8.20000000e+00 ... 5.15835996e+08\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.25000000e+01 4.87830189e+01 8.95283019e+00 ... 8.09993600e+08\n",
      "  0.00000000e+00 1.79472311e+08]\n",
      " [9.50000000e+00 8.10000000e+00 7.05000000e+00 ... 1.89011888e+08\n",
      "  0.00000000e+00 1.01104000e+07]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=4,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Test Accuracy: 0.8985507246376812\n",
      "Test Precision: 0.9375\n",
      "Test Recall: 0.7142857142857143\n",
      "Test FScore: 0.8108108108108109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(0)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "rfc = RandomForest(n_estimators=n_estimators,\n",
    "                   max_depth=max_depth,\n",
    "                   criterion=criterion,\n",
    "                   parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "rfc.fit_model(X=X_df_train,\n",
    "              y=y_df_train)\n",
    "# Evaluation\n",
    "rfc.predict_and_evaluate(X=X_df_validate, \n",
    "                         y=y_df_validate,\n",
    "                         plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning (Isolation Forest - Per Access Plan Type)\n",
    "\n",
    "The following section attempts to train a generalized version of input access plans using Isolation Forests. These trained models are then used to classify access plan outliers from inlier (normal) access plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationForestWrapper:\n",
    "    #\n",
    "    def __init__(self, contamination=.1, parallel_degree=1):\n",
    "        \"\"\"\n",
    "        Constructor Method\n",
    "        :param X - Numpy Array\n",
    "        :param contamination - Real value\n",
    "        :param parallel_degree - Parellization parameter \n",
    "        \"\"\"\n",
    "        self.model = IsolationForest(n_estimators=100, max_samples=256, contamination=contamination, random_state=0, n_jobs=parallel_degree)\n",
    "        print(self.model)\n",
    "    #\n",
    "    def fit_model(self, X):\n",
    "        \"\"\"\n",
    "        Fits Isolation Model and plots scorings\n",
    "        \"\"\"\n",
    "        self.model.fit(X)        \n",
    "    #\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X) \n",
    "    #\n",
    "    def evaluate_model(self, X, y, plot=False):\n",
    "        yhat = self.predict(X)\n",
    "        acc = accuracy_score(y, yhat, normalize=True)\n",
    "        precision = precision_score(y, yhat, average='binary')\n",
    "        recall = recall_score(y, yhat, average='binary')\n",
    "        f1 = f1_score(y, yhat, average='binary')\n",
    "        print('Test Accuracy: ' +  str(acc))\n",
    "        print('Test Precision: ' +  str(precision))\n",
    "        print('Test Recall: ' +  str(recall))\n",
    "        print('Test FScore: ' +  str(f1) + \"\\n\")\n",
    "        if plot is True:\n",
    "            scores = self.model.decision_function(X)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.hist(scores, bins=50);\n",
    "            plt.title('Isolation Forest Scorings')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IsolationForest(bootstrap=False, contamination=0.05, max_features=1.0,\n",
      "        max_samples=256, n_estimators=100, n_jobs=4, random_state=0,\n",
      "        verbose=0)\n",
      "[[2.50000000e+00 1.66666667e+00 2.50000000e+00 ... 4.51695758e+04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.35000000e+01 9.25000000e+00 8.42857143e+00 ... 1.15050000e+03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.20000000e+01 2.83692308e+01 1.29692308e+01 ... 8.90386769e+04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [6.00000000e+00 4.84615385e+00 5.30769231e+00 ... 1.08090285e+09\n",
      "  0.00000000e+00 1.86784615e+06]\n",
      " [1.55000000e+01 1.23125000e+01 8.96875000e+00 ... 2.07782194e+08\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.25000000e+01 4.87830189e+01 8.95283019e+00 ... 8.09993600e+08\n",
      "  0.00000000e+00 1.79472311e+08]]\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:187: UserWarning: max_samples (256) is greater than the total number of samples (209). max_samples will be set to n_samples for estimation.\n",
      "  % (self.max_samples, n_samples))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8260869565217391\n",
      "Test Precision: 0.8214285714285714\n",
      "Test Recall: 0.9583333333333334\n",
      "Test FScore: 0.8846153846153847\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(1)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(-1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "ifw = IsolationForestWrapper(contamination=contamination,\n",
    "                             parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "ifw.fit_model(X=X_df_train)\n",
    "#\n",
    "# Evaluation\n",
    "ifw.evaluate_model(X=X_df_validate, \n",
    "                   y=y_df_validate,\n",
    "                   plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Plan Resource Aggregation (Per Access Plan Instance)\n",
    "\n",
    "This method attempts to tackle the problem of access plan anomolies by aggregating resources per explain plan. Notable resources which are being considered are as follows:\n",
    "\n",
    "* COST\n",
    "* CARDINALITY\n",
    "* BYTES\n",
    "* PARTITION_DELTA (Partition End - Partition Start)\n",
    "* CPU_COST\n",
    "* IO_COST\n",
    "* TEMP_SPACE\n",
    "* TIME\n",
    "\n",
    "The reasoning behind these fields in particular is mainly because these columns can be aggregated together. Aggregation is carried on per access plan instance. Therefore batches are summed together with every explain plan, per timestamp. Contraty to the previous resource aggregation, whereas before plan aggregation occurred per SQL_ID class, now aggregation is carried out with every SQL_ID occurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before transformation: (10000, 30)\n",
      "Shape after transformation: (10000, 31)\n",
      "Shape before transformation: (1433, 11)\n",
      "Shape after transformation: (1433, 12)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Adds a columns per SQL_ID, PLAN_HASH_VALUE grouping, which can be used to group instances together\n",
    "def add_grouping_column(df, column_identifier):\n",
    "    \"\"\"\n",
    "    Receives a pandas dataframe, and adds a new column which allows dataframe to be aggregated per \n",
    "    SQL_ID, PLAN_HASH_VALUE combination.\n",
    "    \n",
    "    :param: df                - Pandas Dataframe\n",
    "    :param: column_identifier - String denoting matrix column to group by\n",
    "    \n",
    "    :return: Pandas Dataframe, with added column    \n",
    "    \"\"\"\n",
    "    print('Shape before transformation: ' + str(df.shape))\n",
    "    new_grouping_col = []\n",
    "    counter = 0\n",
    "    last_sql_id = df[column_identifier].iloc(0) # Starts with first SQL_ID\n",
    "    for index, row in df.iterrows():\n",
    "        if column_identifier == 'SQL_ID':\n",
    "            if last_sql_id != row.SQL_ID:\n",
    "                last_sql_id = row.SQL_ID\n",
    "                counter += 1\n",
    "        elif column_identifier == 'PLAN_ID':\n",
    "            if last_sql_id != row.PLAN_ID:\n",
    "                last_sql_id = row.PLAN_ID\n",
    "                counter += 1\n",
    "        else:\n",
    "            raise ValueError('Column does not exist!')\n",
    "        new_grouping_col.append(counter)\n",
    "    #\n",
    "    # Append list as new column\n",
    "    new_col = pd.Series(new_grouping_col)\n",
    "    df['PLAN_INSTANCE'] = new_col.values\n",
    "    print('Shape after transformation: ' + str(df.shape))\n",
    "    return df\n",
    "#\n",
    "df = add_grouping_column(df=df,column_identifier='SQL_ID')\n",
    "df_outliers = add_grouping_column(df=df_outliers,column_identifier='PLAN_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES',\n",
      "       'OTHER_TAG', 'TEMP_SPACE'],\n",
      "      dtype='object')\n",
      "(236, 9)\n",
      "------------------------------------------\n",
      "Index(['ID', 'PARENT_ID', 'DEPTH', 'POSITION', 'COST', 'CARDINALITY', 'BYTES',\n",
      "       'OTHER_TAG', 'TEMP_SPACE'],\n",
      "      dtype='object')\n",
      "(42, 9)\n"
     ]
    }
   ],
   "source": [
    "df_agg = df.groupby(['SQL_ID','PLAN_HASH_VALUE','PLAN_INSTANCE']).mean()\n",
    "df_agg.reset_index(inplace=True)\n",
    "#\n",
    "df_outliers_agg = df_outliers.groupby(['PLAN_ID','PLAN_INSTANCE']).mean()\n",
    "df_outliers_agg.reset_index(inplace=True)\n",
    "#\n",
    "for col in df_outliers_agg.columns:\n",
    "    if col not in df_agg.columns:\n",
    "        df_outliers_agg.drop(columns=[col], inplace=True)\n",
    "for col in df_agg.columns:\n",
    "    if col not in df_outliers_agg.columns:\n",
    "        df_agg.drop(columns=[col], inplace=True)\n",
    "#\n",
    "df_agg.drop(columns=['PLAN_INSTANCE'], inplace=True)\n",
    "df_outliers_agg.drop(columns=['PLAN_INSTANCE'], inplace=True)\n",
    "#\n",
    "print(df_agg.columns)\n",
    "print(df_agg.shape)\n",
    "print('------------------------------------------')\n",
    "print(df_outliers_agg.columns)\n",
    "print(df_outliers_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training (Random Forest - Per Access Plan Instance)\n",
    "\n",
    "The following section oversees the supervised training of inlier/outlier explain plans. This section first splits the training dataset into two: train + test sets, by mixing half the outliers with the inlier training set. Validation is then performed on a 50/50 mix of inlier / outlier vectors. The ability to classify explain plan vectors as inliers / outliers will be gauged.\n",
    "\n",
    "Labels are denoted as follows:\n",
    "* Inliers  - 0\n",
    "* Outliers - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.50000000e+00 3.60000000e+00 4.50000000e+00 ... 1.22335440e+06\n",
      "  6.45636043e+18 0.00000000e+00]\n",
      " [3.50000000e+00 2.62500000e+00 3.50000000e+00 ... 2.94204802e+08\n",
      "  5.76460752e+18 0.00000000e+00]\n",
      " [4.50000000e+00 3.60000000e+00 4.50000000e+00 ... 1.75947480e+06\n",
      "  6.45636043e+18 0.00000000e+00]\n",
      " ...\n",
      " [5.25000000e+01 4.87830189e+01 8.95283019e+00 ... 8.09987062e+08\n",
      "  0.00000000e+00 1.79472311e+08]\n",
      " [5.00000000e+00 3.90909091e+00 4.63636364e+00 ... 3.30840458e+08\n",
      "  0.00000000e+00 2.20745455e+06]\n",
      " [1.30000000e+01 1.01481481e+01 5.55555556e+00 ... 3.80868600e+06\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=4,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Test Accuracy: 0.9565217391304348\n",
      "Test Precision: 0.9090909090909091\n",
      "Test Recall: 0.9523809523809523\n",
      "Test FScore: 0.9302325581395349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(0)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "rfc = RandomForest(n_estimators=n_estimators,\n",
    "                   max_depth=max_depth,\n",
    "                   criterion=criterion,\n",
    "                   parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "rfc.fit_model(X=X_df_train,\n",
    "              y=y_df_train)\n",
    "# Evaluation\n",
    "rfc.predict_and_evaluate(X=X_df_validate, \n",
    "                         y=y_df_validate,\n",
    "                         plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning (Isolation Forest - Per Access Plan Instance)\n",
    "\n",
    "The following section attempts to train a generalized version of input access plans using Isolation Forests. These trained models are then used to classify access plan outliers from inlier (normal) access plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IsolationForest(bootstrap=False, contamination=0.05, max_features=1.0,\n",
      "        max_samples=256, n_estimators=100, n_jobs=4, random_state=0,\n",
      "        verbose=0)\n",
      "[[1.00000000e+00 3.33333333e-01 1.00000000e+00 ... 3.45604000e+05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.30000000e+01 9.00000000e+00 8.48148148e+00 ... 6.81067407e+04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.33333333e-01 1.00000000e+00 ... 1.69640000e+05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [7.00000000e+00 5.60000000e+00 5.33333333e+00 ... 3.56935067e+08\n",
      "  0.00000000e+00 2.90226667e+06]\n",
      " [1.30000000e+01 1.01481481e+01 5.55555556e+00 ... 3.80868600e+06\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.45000000e+01 1.23000000e+01 6.80000000e+00 ... 1.15786399e+08\n",
      "  0.00000000e+00 2.65176667e+06]]\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:187: UserWarning: max_samples (256) is greater than the total number of samples (209). max_samples will be set to n_samples for estimation.\n",
      "  % (self.max_samples, n_samples))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.782608695652174\n",
      "Test Precision: 0.7619047619047619\n",
      "Test Recall: 1.0\n",
      "Test FScore: 0.8648648648648648\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAHiCAYAAAANlMFMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGXBJREFUeJzt3XuwpHdd5/HP14yIQCAQBiSXYbywYGQVrQFWtMQSVCAoVi1IWIigSMRdkd3VxaHEgt0tJa53kS0rKoqIIAteWIJKBG+smiWELIqAiWEgMQECIdxUIPrdP7qjh+MkM3O6zyX5vl5VqTmn++nn+Xafp5J3nvl1n+ruAADARJ+x2wMAAMBuEcMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGLjVq6o/qKpv3+JjD1TVx6rqpHXPxT+rqidW1et2ew6AzcQwsKuq6khVPXy3jtfd7+nuO3X3P2zDsbqqPr6M7Y9V1Q3rPsYxjv/VVXX1MbY5o6peVVUfqKoPV9WfV9VT1j1Ld7+0u79u3fsFWNW+3R4A4DbuS7r7iq0+uKr2dfeN6xxok5ck+X9J7p3kE0n+dZLPWecBduA5AGyZK8PAnlFVX1BVf7i8QvmBqvq1Dfc9pKretLzvTVX1kJvZx+dX1Ruq6oPLfby0qk5Z3veSJAeS/O/lldpnVdXB5RXcfcttTquqV1fV9VV1RVU9bcO+n1dVr6iqX66qj1bV26rq0Baf69OW+79+ebzTNtzXVfUfquryJJcvb7tfVV203P6dVfXNG7Z/VFX95XKmv6mq762qOyb57SSnbbgyfdq/GCR5YJJf6u6Pd/eN3f2W7v7tDfv+yqr6k6q6oaquuumqcVXdZfk6XFdV766q51TVZyzve0pV/Z+q+omquj7J85a3vXHTc3x6VV1eVR+qqhdWVS3vO6mqfmz583tXVX3Xpp/RU6rqyuXzfVdVPXErPwOARAwDe8t/T/K6JHdNckaSFyRJVd0tyYVJfjrJqUl+PMmFVXXqUfZRSZ6f5LQkX5jkzCTPS5LuPjfJe5J8w3JpxP84yuNfluTq5eMfm+SHquphG+7/xiQvT3JKklcn+ZkTfZJV9TXLGb85yb2SvHu5z42+KcmDk5y1DNuLkvxqknskeUKS/1lVX7Tc9heSfEd3n5zk/kne0N0fT/LIJNcsn+uduvuao4zzZ0leWFXnVNWBTXMeyCKoX5Bkf5IHJLlsefcLktwlyecleWiSb0nyrRse/uAkVy7n/cGbeSkenUWMf8nytfj65e1PW87+gCRftnwtbprpjlmcB49cPt+HbJgJ4ISJYWAv+VQWf11/Wnf/fXffdCXx7CSXd/dLllcvX5bkHUm+YfMOuvuK7r6ouz/R3ddlEc4PPZ6DV9WZSb4yyfctj39Zkp9Pcu6Gzd7Y3a9drjF+SRYhd0suXV5VvaGqfnp52xOTvKi7L+3uTyR5dpIvr6qDGx73/O6+vrv/LotoPNLdv7h8/pcmeVUWsZ4sXrezqurO3f2h5f3H63FJ/jjJDyR5V1VdVlUP3DDn73X3y7r7U939we6+rBZvNnx8kmd390e7+0iSH9v0Ol3T3S9Yzvt3N3Ps87v7hu5+T5LfzyJ+k0UY/1R3X93dH0py/qbH/WOS+1fVZ3f3td39thN4vgCfRgwDe8mzsriy+3+XSxC+bXn7aVlcPd3o3UlO37yDqrpHVb18uVzgI0l+Jcndj/P4pyW5vrs/egvHee+Gr/82ye1v+uv7m/Fl3X3K8p/vPtrz6e6PJfngpuNcteHreyd58IaoviGLUL1pbe+/TfKoJO9eLjP58mM+038+9oe6+3B3f1GSe2ZxlfU3l0sWzkzy10d52N2T3C6f/jPZ/DpdlWPb/Freafn1aZse/09fL694Pz7J05NcW1UXVtX9juNYAEclhoE9o7vf291P6+7TknxHFksBviDJNVkE4UYHkvzNUXbz/CSd5Iu7+85JnpRFYP/TYW5hhGuS3K2qTj6O46zi057P8q/+T910nI1zXpXkDzdE9SnLZQ/fmSTd/abufkwWSxJ+M8krjrKPY+ruDyT50Sxi9G7L437+UTb9QP75Kv5NNr9OJ3TsTa7NYpnMTc7cNOfvdvfXZrHE5B1Jfm6FYwHDiWFgz6iqx1XVTRH0oSyC6h+SvDbJv6qqf1dV+6rq8UnOSvKao+zm5CQfS3JDVZ2e5L9suv99Waxz/Re6+6okf5Lk+VV1+6r64iRPTfLSFZ/aZr+a5Fur6gFV9VlJfijJxcvlBkfzmiye/7lV9ZnLfx5YVV9YVberxWf43qW7P5XkI1m8ZsniuZ5aVXe5uUGq6oer6v7L1/XkJN+Z5Iru/mAWz/vhVfXNy/tPraoHLJeIvCLJD1bVyVV17yT/OYur8OvwiiTPrKrTa/Hmx+/bMO89q+obl/8D8YksftZr/1g8YA4xDOwlD0xycVV9LIs3pz2zu9+1DLNHJ/meLJYTPCvJo5dXMjf7r1m86erDWbzp7tc33f/8JM9ZLjf43qM8/glJDmZx9fY3kjy3uy9a+Zlt0N2vz2KN7quyuAr6+UnOuYXtP5rk65bbXJPF8oIfTvJZy03OTXJkuSzk6VlcDU93vyOLNwReuXy+R/s0iTtk8TxvyOINb/fO4k2CWa7lfVQWr/v1WSyhuGmN9DOSfHz5mDdmEfgvOrFX4mb9XBZvpHxrkrdk8T9DN2YRvZ+xnOea5UwPTfLv13RcYKDqXuVvsgBge1XVI5P8bHdvXioDsDJXhgHYU6rqs2vx2cn7lktdnpvF1WuAtXNlGIA9parukOQPk9wvyd9lsdzlmd39kV0dDLhNEsMAAIxlmQQAAGOJYQAAxrql35q0dne/+9374MGDO3lIAAAGevOb3/yB7t5/rO12NIYPHjyYSy65ZCcPCQDAQFX17mNvZZkEAACDiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYx4zhqnpRVb2/qv5iw20/UlXvqKq3VtVvVNUp2zsmAACs3/FcGf6lJI/YdNtFSe7f3V+c5K+SPHvNcwEAwLY7Zgx39x8luX7Tba/r7huX3/5ZkjO2YTYAANhW61gz/G1JfnsN+wEAgB21b5UHV9X3J7kxyUtvYZvzkpyXJAcOHFjlcAAArNHBwxee0PZHzj97mybZPVu+MlxVT07y6CRP7O6+ue26+4LuPtTdh/bv37/VwwEAwNpt6cpwVT0iyfcleWh3/+16RwIAgJ1xPB+t9rIkf5rkvlV1dVU9NcnPJDk5yUVVdVlV/ew2zwkAAGt3zCvD3f2Eo9z8C9swCwAA7Ci/gQ4AgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjHXMGK6qF1XV+6vqLzbcdrequqiqLl/+edftHRMAANbveK4M/1KSR2y67XCS13f3fZK8fvk9AADcqhwzhrv7j5Jcv+nmxyR58fLrFyf5pjXPBQAA226ra4bv2d3XJsnyz3usbyQAANgZ2/4Guqo6r6ouqapLrrvuuu0+HAAAHLetxvD7qupeSbL88/03t2F3X9Ddh7r70P79+7d4OAAAWL+txvCrkzx5+fWTk/zWesYBAICdczwfrfayJH+a5L5VdXVVPTXJ+Um+tqouT/K1y+8BAOBWZd+xNujuJ9zMXQ9b8ywAALCj/AY6AADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGCsfbs9AAAA63Hw8IV7av9Hzj97myZZH1eGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOtFMNV9Z+q6m1V9RdV9bKquv26BgMAgO225RiuqtOTfHeSQ919/yQnJTlnXYMBAMB2W3WZxL4kn11V+5LcIck1q48EAAA7Y8sx3N1/k+RHk7wnybVJPtzdr1vXYAAAsN1WWSZx1ySPSfK5SU5LcseqetJRtjuvqi6pqkuuu+66rU8KAABrtsoyiYcneVd3X9fdn0ry60kesnmj7r6guw9196H9+/evcDgAAFivVWL4PUn+TVXdoaoqycOSvH09YwEAwPZbZc3wxUlemeTSJH++3NcFa5oLAAC23b5VHtzdz03y3DXNAgAAO8pvoAMAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY60Uw1V1SlW9sqreUVVvr6ovX9dgAACw3fat+PifSvI73f3YqrpdkjusYSYAANgRW47hqrpzkq9K8pQk6e5PJvnkesYCAIDtt8oyic9Lcl2SX6yqt1TVz1fVHdc0FwAAbLtVlknsS/JlSZ7R3RdX1U8lOZzkBzZuVFXnJTkvSQ4cOLDC4QAA9o6Dhy88oe2PnH/2Nk3CKla5Mnx1kqu7++Ll96/MIo4/TXdf0N2HuvvQ/v37VzgcAACs15ZjuLvfm+Sqqrrv8qaHJfnLtUwFAAA7YNVPk3hGkpcuP0niyiTfuvpIAACwM1aK4e6+LMmhNc0CAAA7ym+gAwBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjrRzDVXVSVb2lql6zjoEAAGCnrOPK8DOTvH0N+wEAgB21UgxX1RlJzk7y8+sZBwAAds6qV4Z/MsmzkvzjGmYBAIAdtW+rD6yqRyd5f3e/uaq++ha2Oy/JeUly4MCBrR4OAFjBwcMXntD2R84/e5smmcvPYG9a5crwVyT5xqo6kuTlSb6mqn5l80bdfUF3H+ruQ/v371/hcAAAsF5bjuHufnZ3n9HdB5Ock+QN3f2ktU0GAADbzOcMAwAw1pbXDG/U3X+Q5A/WsS8AANgprgwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGPt2+0BAGC6g4cvPOHHHDn/7G2YBOZxZRgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAw1pZjuKrOrKrfr6q3V9XbquqZ6xwMAAC2274VHntjku/p7kur6uQkb66qi7r7L9c0GwAAbKstXxnu7mu7+9Ll1x9N8vYkp69rMAAA2G5rWTNcVQeTfGmSi9exPwAA2AmrLJNIklTVnZK8Ksl/7O6PHOX+85KclyQHDhxY9XAA7LKDhy/c9mMcOf/sE9r+RGc60f3vRdv9c9ju/d8WfgbcNqx0ZbiqPjOLEH5pd//60bbp7gu6+1B3H9q/f/8qhwMAgLVa5dMkKskvJHl7d//4+kYCAICdscqV4a9Icm6Sr6mqy5b/PGpNcwEAwLbb8prh7n5jklrjLAAAsKP8BjoAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYKx9uz3ATjh4+MJtP8aR88/e9mNw23ei5+qJnnfbvf+dcFt4DnvNTvw7chqv6bHtxf82+7nN5MowAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYCwxDADAWGIYAICxxDAAAGOJYQAAxhLDAACMJYYBABhLDAMAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGEsMAwAwlhgGAGAsMQwAwFhiGACAscQwAABjiWEAAMYSwwAAjCWGAQAYSwwDADCWGAYAYKyVYriqHlFV76yqK6rq8LqGAgCAnbDlGK6qk5K8MMkjk5yV5AlVdda6BgMAgO22ypXhByW5oruv7O5PJnl5ksesZywAANh+q8Tw6Umu2vD91cvbAADgVqG6e2sPrHpckq/v7m9ffn9ukgd19zM2bXdekvOW3943yTu3Pi63AXdP8oHdHoJbDecLJ8L5wolyzty23bu79x9ro30rHODqJGdu+P6MJNds3qi7L0hywQrH4Takqi7p7kO7PQe3Ds4XToTzhRPlnCFZbZnEm5Lcp6o+t6pul+ScJK9ez1gAALD9tnxluLtvrKrvSvK7SU5K8qLuftvaJgMAgG22yjKJdPdrk7x2TbMwgyUznAjnCyfC+cKJcs6w9TfQAQDArZ1fxwwAwFhimG1VVXerqouq6vLln3c9yjYPqKo/raq3VdVbq+rxuzEru+94zpfldr9TVTdU1Wt2ekZ2X1U9oqreWVVXVNXho9z/WVX1a8v7L66qgzs/JXvFcZwvX1VVl1bVjVX12N2Ykd0lhtluh5O8vrvvk+T1y+83+9sk39LdX5TkEUl+sqpO2cEZ2TuO53xJkh9Jcu6OTcWeUVUnJXlhkkcmOSvJE6rqrE2bPTXJh7r7C5L8RJIf3tkp2SuO83x5T5KnJPnVnZ2OvUIMs90ek+TFy69fnOSbNm/Q3X/V3Zcvv74myfuTHPNDsrlNOub5kiTd/fokH92podhTHpTkiu6+srs/meTlWZw3G208j16Z5GFVVTs4I3vHMc+X7j7S3W9N8o+7MSC7Twyz3e7Z3dcmyfLPe9zSxlX1oCS3S/LXOzAbe88JnS+MdHqSqzZ8f/XytqNu0903JvlwklN3ZDr2muM5XxhupY9WgySpqt9L8jlHuev7T3A/90rykiRP7m7/h34bta7zhbGOdoV388ciHc82zOBc4JjEMCvr7off3H1V9b6quld3X7uM3fffzHZ3TnJhkud0959t06jsAes4Xxjt6iRnbvj+jCTX3Mw2V1fVviR3SXL9zozHHnM85wvDWSbBdnt1kicvv35ykt/avMHy13n/RpJf7u7/tYOzsfcc83xhvDcluU9Vfe7y3x3nZHHebLTxPHpskje0D9Wf6njOF4bzSzfYVlV1apJXJDmQxTt2H9fd11fVoSRP7+5vr6onJfnFJBt/nfdTuvuynZ+Y3XQ858tyuz9Ocr8kd0rywSRP7e7f3aWx2WFV9agkP5nkpCQv6u4frKr/luSS7n51Vd0+iyVXX5rFFeFzuvvK3ZuY3XQc58sDs7ggc9ckf5/kvctPN2IIMQwAwFiWSQAAMJYYBgBgLDEMAMBYYhgAgLHEMAAAY4lhAADGEsMAAIwlhgEAGOv/A7oxWzBtuaaqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_labels_outliers, df_labels = [], []\n",
    "for i in range(df_agg.shape[0]):\n",
    "    df_labels.append(1)\n",
    "for i in range(df_outliers_agg.shape[0]):\n",
    "    df_labels_outliers.append(-1)\n",
    "#\n",
    "# Training / Validation Splits (Inliers + Outliers)\n",
    "X_df_train, X_df_validate, y_df_train, y_df_validate = train_test_split(df_agg, df_labels, test_size=test_split)\n",
    "X_df_outlier_train, X_df_outlier_validate, y_df_outlier_train, y_df_outlier_validate = train_test_split(df_outliers_agg, df_labels_outliers, test_size=.5)\n",
    "#\n",
    "# Mixing of Inlier + Outlier data for validation purposes\n",
    "X_df_train = np.concatenate((X_df_train.values, X_df_outlier_train.values), axis=0)\n",
    "y_df_train = np.concatenate((np.array(y_df_train), np.array(y_df_outlier_train)), axis=0)\n",
    "X_df_validate = np.concatenate((X_df_validate.values, X_df_outlier_validate.values), axis=0)\n",
    "y_df_validate = np.concatenate((np.array(y_df_validate), np.array(y_df_outlier_validate)), axis=0)\n",
    "#\n",
    "# Building Model + Training\n",
    "ifw = IsolationForestWrapper(contamination=contamination,\n",
    "                             parallel_degree=parallel_degree)\n",
    "print(X_df_train)\n",
    "print(y_df_train)\n",
    "ifw.fit_model(X=X_df_train)\n",
    "#\n",
    "# Evaluation\n",
    "ifw.evaluate_model(X=X_df_validate, \n",
    "                   y=y_df_validate,\n",
    "                   plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
