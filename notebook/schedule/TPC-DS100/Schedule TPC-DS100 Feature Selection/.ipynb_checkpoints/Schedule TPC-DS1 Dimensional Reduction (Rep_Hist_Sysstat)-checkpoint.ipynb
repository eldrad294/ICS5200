{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule TPC-DS 10 Dimensional Reduction (REP_HIST_SYSSTAT)\n",
    "\n",
    "This notebook is dedicated to dataset profiling. In this notebook, dimensional reduction techniques will be implemented so as to categorize which features belay the most information to address the problem at hand - Workload Prediction. Due to the vast feature space which have been gathered during a workload's execution, manual techniques at determining which are most detrimental is not sufficient. \n",
    "\n",
    "Therefore the following work puts emphasis on automated techniques so as to determine out of the vast feature space which are most important to base future models upon. The proposed techniques will focus on each of the proposed four datasets:\n",
    "\n",
    "* REP_HIST_SNAPSHOT\n",
    "* REP_HIST_SYSMETRIC_SUMMARY\n",
    "* __REP_HIST_SYSSTAT__\n",
    "* REP_VSQL_PLAN\n",
    "\n",
    "Further more, dimensionality reduction will be covered from two different aspects. First, feature selection techniques will be implemented to elimate features which are considered unimportant using entropy based measures. Once a number of 'static' features are pruned from the original dataset, the remainding features are then processed through a Singular Value Decomposition process, where in features are reduced through PCA (Principal Compoenent Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-91f5d6f42759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Module Import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRFE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAmbiguousTimeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInvalidTimeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNonExistentTimeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_fill_cache\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Module Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression, RFE\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Cell\n",
    "\n",
    "Tweak parametric changes from this cell to influence outcome of experiment\n",
    "\n",
    "* tpcds - Schema upon which to operate test\n",
    "* debug_mode - Determines whether to plot graphs or not, useful for development purposes\n",
    "* low_quartile_limit - Lower Quartile threshold to detect outliers\n",
    "* upper_quartile_limit - Upper Quartile threshold to detect outliers\n",
    "* test_split - Denotes which Data Split to operate under when it comes to training / validation\n",
    "* nrows - Number of rows to read from csv file\n",
    "* top_n_features - Number of top features to focus on\n",
    "* parallel_degree - Number of parallel threads to train models with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Config\n",
    "tpcds='TPCDS10'\n",
    "debug_mode=False \n",
    "low_quartile_limit = .001\n",
    "upper_quartile_limit = .999\n",
    "test_split=.2\n",
    "nrows=None\n",
    "top_n_features=200\n",
    "parallel_degree=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path\n",
    "#root_dir = 'C:/Users/gabriel.sammut/University/Data_ICS5200/Schedule/' + tpcds\n",
    "root_dir = 'D:/Projects/Datagenerated_ICS5200/Schedule/' + tpcds\n",
    "\n",
    "# Open Data\n",
    "rep_hist_sysstat_path = root_dir + '/rep_hist_sysstat.csv'\n",
    "\n",
    "rep_hist_sysstat_df = pd.read_csv(rep_hist_sysstat_path,nrows=nrows)\n",
    "\n",
    "def prettify_header(headers):\n",
    "    \"\"\"\n",
    "    Cleans header list from unwated character strings\n",
    "    \"\"\"\n",
    "    header_list = []\n",
    "    [header_list.append(header.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")) for header in headers]\n",
    "    return header_list\n",
    "\n",
    "rep_hist_sysstat_df.columns = prettify_header(rep_hist_sysstat_df.columns.values)\n",
    "print(rep_hist_sysstat_df.columns)\n",
    "print('\\n\\nTable [REP_HIST_SYSSTAT] - ' + str(rep_hist_sysstat_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "\n",
    "The correlation of resources consumed (y) per snapshot (X) define our feature space. Since the objective here is to attempt to predict what resources will be incurred ahead of time, the problem can be defined as a number of questions:\n",
    "\n",
    "* Q: What resources can I predict to be in usage at point N in time?\n",
    "* Q: What resources should I be predicting that accurately portray a schedule's workload?\n",
    "* Q: What knowledge/data do I have ahead of time which I can use to base my predictions off?\n",
    "\n",
    "Due to the vast feature space in the available metrics monitored and captured during a workload's execution, it is important to rank which attribute is most beneficial than others. Additionally, it is important to analyze such features individually, and considerate of other features in two types of analysis:\n",
    "\n",
    "* Univariate Analysis\n",
    "* Multivariate Analysis\n",
    "\n",
    "Furthermore, multiple types of feature ranking / analysis techniques ara available, amongst which will be considered:\n",
    "\n",
    "* Filter Methods\n",
    "* Wrapper Methods\n",
    "* Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We apply a number of preprocessing techniques to the presented dataframes, particularly to normalize and/or scale feature vectors into a more suitable representation for downstream estimators:\n",
    "\n",
    "Relative Links:\n",
    "* http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "* https://machinelearningmastery.com/improve-model-accuracy-with-data-pre-processing/\n",
    "* https://machinelearningmastery.com/normalize-standardize-time-series-data-python/\n",
    "\n",
    "### Table Pivots\n",
    "\n",
    "To better handle the following table, a number of table pivots are made on tables:\n",
    "* rep_hist_sysstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Table REP_HIST_SYSSTAT\n",
    "rep_hist_sysstat_df = rep_hist_sysstat_df.pivot_table(index='SNAP_ID', columns='STAT_NAME', values='VALUE')\n",
    "rep_hist_sysstat_df.reset_index(inplace=True)\n",
    "print(rep_hist_sysstat_df.columns)\n",
    "rep_hist_sysstat_df[['SNAP_ID']] = rep_hist_sysstat_df[['SNAP_ID']].astype(int)\n",
    "rep_hist_sysstat_df.sort_values(by=['SNAP_ID'],inplace=True,ascending=True)\n",
    "print(\"REP_HIST_SYSSTAT_DF Shape: \" + str(rep_hist_sysstat_df.shape))\n",
    "\n",
    "# Refreshing columns with pivoted columns\n",
    "def convert_list_to_upper(col_list):\n",
    "    \"\"\"\n",
    "    Takes a string and converts elements to upper\n",
    "    \"\"\"\n",
    "    upper_col_list = []\n",
    "    for col in col_list:\n",
    "        upper_col_list.append(col.upper())\n",
    "    return upper_col_list\n",
    "\n",
    "rep_hist_sysstat_df.rename(str.upper, inplace=True, axis='columns')\n",
    "\n",
    "# DF Shape\n",
    "print('Table [REP_HIST_SYSSTAT] - ' + str(rep_hist_sysstat_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN Values\n",
    "\n",
    "Checking dataframes for potential missing values/data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_na_columns(df):\n",
    "    \"\"\"\n",
    "    Return columns which consist of NAN values\n",
    "    \"\"\"\n",
    "    na_list = []\n",
    "    for head in df.columns:\n",
    "        if df[head].isnull().values.any():\n",
    "            na_list.append(head)\n",
    "    return na_list\n",
    "\n",
    "print(\"Table REP_HIST_SYSSTAT: \" + str(get_na_columns(df=rep_hist_sysstat_df)) + \"\\n\\n\")\n",
    "\n",
    "def fill_na(df):\n",
    "    \"\"\"\n",
    "    Replaces NA columns with 0s\n",
    "    \"\"\"\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Populating NaN values with amount '0'\n",
    "rep_hist_sysstat_df = fill_na(df=rep_hist_sysstat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Since this experiment deals with prediction of upcoming SQL_IDs, respectice SQL_ID strings need to labelled as a numeric representation. Label Encoder will be used here to convert SQL_ID's into a numeric format, which are in turn used for training. Evaluation (achieved predictions) is done so also in numeric format, at which point the label encoder is eventually used to decode back the labels into the original, respetive SQL_ID representation.\n",
    "\n",
    "This section of the experiment additionally converts the targetted label into a binarized version of the previous achieved categorical numeric values.\n",
    "\n",
    "* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \"\"\"\n",
    "    Scikit Label Encoder was acting up with the following error whilst using the transform function, even though I tripled \n",
    "    checked that the passed data was exactly the same as the one used for training:\n",
    "    \n",
    "    * https://stackoverflow.com/questions/46288517/getting-valueerror-y-contains-new-labels-when-using-scikit-learns-labelencoder\n",
    "    \n",
    "    So I have rebuilt a similar functionality to categorize my data into numeric digits, as the LabelEncoder is supposed to do.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__class_map = {}\n",
    "        self.__integer_counter = 0\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        :param - X: python list\n",
    "        \"\"\"\n",
    "        for val in X:\n",
    "            if val not in self.__class_map:\n",
    "                self.__class_map[val] = self.__integer_counter\n",
    "                self.__integer_counter += 1\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        param - X: python list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for val in X:\n",
    "            if val in self.__class_map:\n",
    "                value = self.__class_map[val]\n",
    "                encoded_map.append(value)\n",
    "            else:\n",
    "                raise ValueError('Label Mismatch - Encountered a label which was not trained on.')\n",
    "        return encoded_map\n",
    "    \n",
    "    def get_class_map(self):\n",
    "        \"\"\"\n",
    "        Returns original classes as a list\n",
    "        \"\"\"\n",
    "        class_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            class_map.append(key)\n",
    "        return class_map\n",
    "    \n",
    "    def get_encoded_map(self):\n",
    "        \"\"\"\n",
    "        Returns class encodings as a list\n",
    "        \"\"\"\n",
    "        encoded_map = []\n",
    "        for key, value in self.__class_map.items():\n",
    "            encoded_map.append(value)\n",
    "        return encoded_map\n",
    "\n",
    "print('-'*10 + 'BEFORE' + '-'*10)\n",
    "print(rep_hist_sysstat_df.shape)\n",
    "print(rep_hist_sysstat_df.head(10))\n",
    "\n",
    "for col in rep_hist_sysstat_df.columns:\n",
    "    first_ten_vals = rep_hist_sysstat_df[col].iloc[0:nrows]\n",
    "    try:\n",
    "        for val in first_ten_vals:\n",
    "            val = int(val)\n",
    "        continue  # If value is type casted successfully, then move on to the next value since it doesn't require label encoding\n",
    "    except ValueError:\n",
    "        le = LabelEncoder()  # Encode column with labels\n",
    "        val_list = rep_hist_sysstat_df[col].tolist()\n",
    "        le.fit(val_list)\n",
    "        transformed_map = le.transform(val_list)\n",
    "        rep_hist_sysstat_df[col] = pd.DataFrame(transformed_map, columns=[col])\n",
    "\n",
    "print('-'*10 + 'AFTER' + '-'*10)\n",
    "print(rep_hist_sysstat_df.shape)\n",
    "print(rep_hist_sysstat_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Negative Values\n",
    "\n",
    "A function which retrieves a count per column for nay negative values it might contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NegativeHandler:\n",
    "    \"\"\"\n",
    "    This class contains logic pertaining to handling of negative values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        :param df:      (Pandas) Data matrix containing data.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__df = df\n",
    "        self.__headers = df.columns\n",
    "\n",
    "    def count_neg_df(self):\n",
    "        \"\"\"\n",
    "        Return columns with respective negative value count.\n",
    "        :return: (List) Retrieves list of row positions denoting negative occurance.\n",
    "        \"\"\"\n",
    "        neg_list = []\n",
    "        for head in self.__headers:\n",
    "            count = 0\n",
    "            try:\n",
    "                count = sum(n < 0 for n in self.__df[head].values.flatten())\n",
    "            except Exception:\n",
    "                pass\n",
    "                #print('Non numeric column [' + head + ']')\n",
    "            if count > 0:\n",
    "                neg_list.append([head,count])\n",
    "        return neg_list\n",
    "\n",
    "    def fill_neg(self):\n",
    "        \"\"\"\n",
    "        Sets any data anomilies resulting in negative values to 0\n",
    "        :return: (Pandas) Dataframe with ammended negative values.\n",
    "        \"\"\"\n",
    "        headers = self.count_neg_df()\n",
    "        df = self.__df\n",
    "        for head in headers:\n",
    "            try:\n",
    "                df[df[head] < 0] = 0\n",
    "            except Exception:\n",
    "                pass\n",
    "                #print('Non numeric column [' + head + ']')\n",
    "        return df\n",
    "\n",
    "nh = NegativeHandler(df=rep_hist_sysstat_df)\n",
    "\n",
    "# Check For Negative Values within dataframes\n",
    "print('---------------WITH NEGATIVE VALUES---------------')\n",
    "print(\"Table REP_HIST_SYSSTAT: \" + str(nh.count_neg_df()) + \"\\n\\n\")\n",
    "\n",
    "# Replace Negative Values with a minimal threshold of 0\n",
    "rep_hist_sysstat_df = nh.fill_neg()\n",
    "\n",
    "# Check For Negative Values within dataframes\n",
    "print('\\n\\n---------------WITHOUT NEGATIVE VALUES---------------')\n",
    "print(\"Table REP_HIST_SYSSTAT: \" + str(nh.count_neg_df()) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundant Feature Removal\n",
    "\n",
    "In this step, redundant features are dropped. Features are considered redundant if exhibit a standard devaition of 0 (meaning no change in value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropFlatline:\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_flatline_columns(df):\n",
    "        \"\"\"\n",
    "        This function removes columns with a flat standard deviation of 0.\n",
    "        \"\"\"\n",
    "        columns = df.columns\n",
    "        flatline_features = []\n",
    "        for i in range(len(columns)):\n",
    "            try:\n",
    "                std = df[columns[i]].std()\n",
    "                if std == 0:\n",
    "                    flatline_features.append(columns[i])\n",
    "                    print('Dropping feature [' + columns[i]  + ']')\n",
    "            except:\n",
    "                pass\n",
    "        #\n",
    "        #print('Features which are considered flatline:\\n')\n",
    "        #for col in flatline_features:\n",
    "        #    print(col)\n",
    "        print('\\nShape before changes: [' + str(df.shape) + ']')\n",
    "        df = df.drop(columns=flatline_features)\n",
    "        print('Shape after changes: [' + str(df.shape) + ']')\n",
    "        print('Dropped a total [' + str(len(flatline_features)) + ']')\n",
    "        return df\n",
    "\n",
    "rep_hist_sysstat_df = DropFlatline.drop_flatline_columns(df=rep_hist_sysstat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Feature Distribution & Skewness\n",
    "\n",
    "In order to decide between a normalization strategy, it is important to understand the underlying data spread. Understanding of dataset mean, variance, skewness on a per column/feature basis helps determine whether a standardization or normalization strategy should be utilized on the datasets.\n",
    "\n",
    "### Plotting Data Distribution\n",
    "\n",
    "To better decide which normalization technique ought to be utilized for the technique at hand, a number of feature columns will be plotted as histograms to better convey the distribution spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    \"\"\"\n",
    "    This class contains a number of methods dedicated to plotting the underlying data.\n",
    "    \"\"\"\n",
    "    def __init__(self, df=None, tpc_type=None, table=None):\n",
    "        \"\"\"\n",
    "        Constructor method.\n",
    "        :param df:             (Pandas) Data matrix.\n",
    "        :param tpc_type:       (String) TPC type, used for plot title purposes.\n",
    "        :param table:          (String) Denotes which table is being access, used for plot title/label purposes.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__df = df\n",
    "        self.__tpc_type = tpc_type\n",
    "        self.__table = table\n",
    "        \n",
    "    def plot_hist(self, bin_size=10, feature_column=None):\n",
    "        \"\"\"\n",
    "        Plots histogram distribution, split into a number of buckets.\n",
    "        :param bin_size: (Integer) Denotes number of histogram buckets to split data into.\n",
    "        :param feature_column: (String) Denotes which column to plot.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        df = self.__df\n",
    "\n",
    "        try:\n",
    "            df['SNAP_ID'] = df['SNAP_ID'].astype(float)\n",
    "            df[feature_column] = df[feature_column].astype(float)\n",
    "\n",
    "            max_val = df[feature_column].max()\n",
    "            start_snap, end_snap = int(df['SNAP_ID'].min()), int(df['SNAP_ID'].max())\n",
    "\n",
    "            df[feature_column].hist(bins=10,figsize=(12,8))\n",
    "            plt.ylabel(feature_column)\n",
    "            plt.xlabel('Bin Ranges Of ' + str(int(max_val/bin_size)))\n",
    "            plt.title(self.__tpc_type + ' Table ' + self.__table.upper() + '.' + str(feature_column) + \" between \" + str(start_snap) + \" - \" + str(end_snap))\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            print('Could not plot column: ' + feature_column)\n",
    "\n",
    "    def plot_scatter(self, feature_column=None):\n",
    "        \"\"\"\n",
    "        Plots scatter plots vs SNAP_ID.\n",
    "        :param feature_column: (String) Denotes which column to plot.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        df = self.__df\n",
    "\n",
    "        try:\n",
    "            df['SNAP_ID'] = df['SNAP_ID'].astype(int)\n",
    "            df[feature_column] = df[feature_column].astype(int)\n",
    "            start_snap, end_snap = int(df['SNAP_ID'].min()), int(df['SNAP_ID'].max())\n",
    "\n",
    "            df.plot.scatter(x='SNAP_ID',\n",
    "                            y=feature_column,\n",
    "                            figsize=(12,8))\n",
    "            plt.ylabel(feature_column)\n",
    "            plt.xlabel('SNAP ID')\n",
    "            plt.title(self.__tpc_type + ' Table ' + self.__table.upper() + '.' + str(feature_column) + \" between \" + str(start_snap) + \" - \" + str(end_snap))\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            print('Could not plot column: ' + feature_column)\n",
    "\n",
    "    def plot_boxplot(self, feature_column=None):\n",
    "        \"\"\"\n",
    "        Plots quartile plots to estimate mean and sigma (std dev).\n",
    "        :param feature_column: (String) String denoting feature column. Overrides parameter used in class constructor.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        df = self.__df\n",
    "\n",
    "        try:\n",
    "            df[feature_column] = df[feature_column].astype(float)\n",
    "            df.boxplot(column=feature_column, figsize=(12,8), grid=True)\n",
    "            plt.title(self.__tpc_type + ' ' + str(feature_column))\n",
    "            plt.show()\n",
    "        except ValueError:\n",
    "            print('Could not plot column: ' + feature_column)\n",
    "            \n",
    "vis = Visualizer(df=rep_hist_sysstat_df, \n",
    "                 tpc_type=tpcds, \n",
    "                 table='rep_hist_sysstat')\n",
    "\n",
    "if debug_mode is False:\n",
    "    \n",
    "    # Plotting Histograms of data distribution\n",
    "    for header in rep_hist_sysstat_df.columns:\n",
    "        print('REP_HIST_SYSSTAT - ' + header + ' - OUTLIERS HISTOGRAM')\n",
    "        vis.plot_hist(bin_size=10, feature_column=header)\n",
    "    \n",
    "    # Plotting Scatter Plots of data distribution\n",
    "    for header in rep_hist_sysstat_df.columns:\n",
    "        print('REP_HIST_SYSSTAT - ' + header + ' - OUTLIERS SCATTER')\n",
    "        vis.plot_scatter(feature_column=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Handling\n",
    "\n",
    "https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/\n",
    "\n",
    "As can be appreciated from the previous plots, data is heavily skewed on particular (smallest) bins. This skew in the plotted histograms is a result of data point outliers - these need to be evaluated and removed if neccessary.\n",
    "\n",
    "Following the 3 Standard Deviation Rule, we can categorize our dataset into subsets consisting of the following ranges:\n",
    "* 0     - 68.27%\n",
    "* 68.28 - 95.45%\n",
    "* 95.46 - 99.73%\n",
    "* 99.74 - 100%\n",
    "\n",
    "It should be mentioned, that given the time series nature of the dataset, it is not a safe assumption to ignore outliers. By training respective models on outlier insensitive dataset, we would invite a potential problem, which risks blinding any models we train to future predicted spikes of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OutlierHandling:\n",
    "    \"\"\"\n",
    "    This class handles outlier detection and removal methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, low_quartile_limit=.01,upper_quartile_limit=.99):\n",
    "        \"\"\"\n",
    "        Constructor method.\n",
    "        :param df:                   (Pandas) Dataframe consisting of input features to be pruned of outliers.\n",
    "        :param low_quartile_limit:   (Float) Lower percentage threshold for removal of outliers.\n",
    "        :param upper_quartile_limit: (Float) Upper percentage threshold for removal of outliers.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__df = df\n",
    "        self.__low_quartile_limit = low_quartile_limit\n",
    "        self.__upper_quartile_limit = upper_quartile_limit\n",
    "        self.__headers= df.columns\n",
    "        \n",
    "    def get_outliers(self):\n",
    "        \"\"\"\n",
    "        Detect and return which rows are considered outliers within the dataset, determined by :quartile_limit (99%)\n",
    "        :return: (List) A list denoting row positions of detected outliers.\n",
    "        \"\"\"\n",
    "        outlier_rows = [] # This list of lists consists of elements of the following notation [column,rowid]\n",
    "        df = self.__df\n",
    "        for header in self.__headers:\n",
    "            try:\n",
    "                df[header] = df[header].astype(float)\n",
    "                q = df[header].quantile(self.__upper_quartile_limit)\n",
    "                series_row = (df[df[header] > q].index)\n",
    "                for id in list(np.array(series_row)):\n",
    "                    outlier_rows.append([header,id])\n",
    "                q = df[header].quantile(self.__low_quartile_limit)\n",
    "                series_row = (df[df[header] < q].index)\n",
    "                for id in list(np.array(series_row)):\n",
    "                    outlier_rows.append([header,id])\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "        unique_ids = []\n",
    "        unique_outlier_rows = []\n",
    "\n",
    "        for col, rowid in outlier_rows:\n",
    "            if rowid not in unique_ids:\n",
    "                unique_outlier_rows.append([col,rowid])\n",
    "                unique_ids.append(rowid)\n",
    "        return unique_outlier_rows\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        \"\"\"\n",
    "        Remove rows which are considered outliers within the dataset, determined by :quartile_limit (99%).\n",
    "        :return: (Pandas) Dataframe with pruned outliers.\n",
    "        \"\"\"\n",
    "        df = self.__df\n",
    "        length_before = len(df)\n",
    "        outliers_index = []\n",
    "        for header in self.__headers:\n",
    "            try:\n",
    "                df[header] = df[header].astype(float)\n",
    "                q = df[header].quantile(self.__upper_quartile_limit)\n",
    "                outliers_index.append(list(np.array(df[df[header] > q].index)))\n",
    "                q = df[header].quantile(self.__low_quartile_limit)\n",
    "                outliers_index.append(list(np.array(df[df[header] < q].index)))\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "        #flat_outliers_index = [item for sublist in l for item in outliers_index]\n",
    "        flat_outliers_index = [item for sublist in outliers_index for item in sublist]\n",
    "        outliers_index = list(set(flat_outliers_index))\n",
    "        df = df.drop(outliers_index)\n",
    "        return df\n",
    "    \n",
    "oh = OutlierHandling(df=rep_hist_sysstat_df,\n",
    "                     upper_quartile_limit=upper_quartile_limit,\n",
    "                     low_quartile_limit=low_quartile_limit)\n",
    "\n",
    "#Printing outliers to screen\n",
    "outliers = oh.get_outliers()\n",
    "\n",
    "# Printing dataframe before adjustments\n",
    "print('\\n\\nDATAFRAMES WITH OUTLIERS')\n",
    "print(rep_hist_sysstat_df.shape)\n",
    "print('----------------------------')\n",
    "\n",
    "rep_hist_sysstat_df_outliers = oh.get_outliers()\n",
    "print('\\n\\nOUTLIERS')\n",
    "print(len(rep_hist_sysstat_df_outliers))\n",
    "print('----------------------------')\n",
    "\n",
    "# Dropping Outliers\n",
    "rep_hist_sysstat_df_pruned = oh.remove_outliers()\n",
    "print('\\n\\nDATAFRAMES WITHOUT OUTLIERS')\n",
    "print(rep_hist_sysstat_df_pruned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data distribution without outliers\n",
    "\n",
    "Plotting metrics against SNAP_ID, without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis = Visualizer(df=rep_hist_sysstat_df_pruned, \n",
    "                 tpc_type=tpcds, \n",
    "                 table='rep_hist_sysstat')\n",
    "\n",
    "if debug_mode is False:\n",
    "    \n",
    "    # Plotting Histograms without Outliers\n",
    "    for header in rep_hist_sysstat_df_pruned.columns:\n",
    "        print('REP_HIST_SYSSTAT - ' + header + ' - STRIPPED HISTOGRAM')\n",
    "        vis.plot_hist(feature_column=header, bin_size=10)\n",
    "    \n",
    "    # Plotting Scatter Plots without Outliers\n",
    "    for header in rep_hist_sysstat_df_pruned.columns:\n",
    "        print('REP_HIST_SYSSTAT - ' + header + ' - STRIPPED SCATTER')\n",
    "        vis.plot_scatter(feature_column=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Redundant Columns\n",
    "\n",
    "Dropping redundant columns which are not detrimental to the task at hand (NB: This is only the first steps towards feature selection. This step ensures that specific columns which are SURELY not useful are dropped ahead of time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "\n",
    "Based on the above plots, one can argue that the data distribution is uneven, and does not correlate to any particular pattern. A normalization approach (MinMaxScaling and/or RobustScaling) to the presented dataset is a more likely candidate than standardizing of the presented dataset. \n",
    "\n",
    "Reasons behind normalizing the dataset rather than standardizing, is due to the vast standard deviations from the mean for several feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonNumericStrip:\n",
    "    \"\"\"\n",
    "    This class contains a method dedicated to feature removing if they contain non-numeric data\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def remove_non_numeric_columns(df):\n",
    "        \"\"\"\n",
    "        Accepts data matrix, and returns another matrix with stripped columns containing non-numeric data.\n",
    "        :param df: (Pandas) Data matrix\n",
    "        :return: (Pandas) Data matrix\n",
    "        \"\"\"\n",
    "        print('-'*10 + 'Before' + '-'*10)\n",
    "        print(df.columns)\n",
    "        col_list = []\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].astype(float)\n",
    "                col_list.append(col)\n",
    "            except ValueError:\n",
    "                print('Column ' + str(col) + ' contains non numeric data')\n",
    "        df = df[col_list]\n",
    "        print('-'*10 + 'After' + '-'*10)\n",
    "        print(df.columns)\n",
    "        return df\n",
    "rep_hist_sysstat_df_pruned = NonNumericStrip.remove_non_numeric_columns(df=rep_hist_sysstat_df_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Consolidation\n",
    "\n",
    "Grouping datasets on SNAP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"REP_HIST_SYSSTAT shape before transformation: \" + str(rep_hist_sysstat_df_pruned.shape))\n",
    "rep_hist_sysstat_df_pruned = rep_hist_sysstat_df_pruned[rep_hist_sysstat_df_pruned['SNAP_ID'] > 0]\n",
    "\n",
    "# Group By Values by SNAP_ID , sum all metrics (for table REP_HIST_SNAPSHOT)\n",
    "rep_hist_sysstat_df_pruned = rep_hist_sysstat_df_pruned.groupby(['SNAP_ID']).sum()\n",
    "rep_hist_sysstat_df_pruned.reset_index(inplace=True)\n",
    "\n",
    "print(\"REP_HIST_SYSSTAT shape after transformation: \" + str(rep_hist_sysstat_df_pruned.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Relavent Sources:\n",
    "\n",
    "* http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf\n",
    "* https://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a normalization preprocessing technique for data distribution that can closely approximate minimum and maximum observable values per column:\n",
    "\n",
    "<i>\"Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\"</i>\n",
    "\n",
    "Normalization formula is stated as follows: $$y=(x-min)/(max-min)$$\n",
    "\n",
    "### Standardization\n",
    "\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/ recommends a standardization preprocessing technique for data distributions that observe a Gaussian spread, with a mean of 0 and a standard deviation of 1 (approximately close to these values):\n",
    "\n",
    "<i>\"Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\"</i>\n",
    "\n",
    "Standardization formula is stated as follows: $$y=(x-mean)/StandardDeviation$$\n",
    "Mean defined as: $$mean=sum(x)/count(x)$$\n",
    "Standard Deviation defined as: $$StandardDeviation=sqrt(sum((x-mean)^2)/count(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_scaler(dataframe):\n",
    "        \"\"\"\n",
    "        Normalize df using interquartile ranges as min-max, this way outliers do not play a heavy emphasis on the\n",
    "        normalization of values.\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.robust_scale(dataframe.values)\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "    @staticmethod\n",
    "    def minmax_scaler(dataframe):\n",
    "        \"\"\"\n",
    "        Normalize df using min-max ranges for normalization method\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.minmax_scale(dataframe.values)\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(dataframe):\n",
    "        \"\"\"\n",
    "        The normalizer scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.\n",
    "        :param dataframe: (Pandas) Pandas data matrix\n",
    "        :return: (Pandas) Normalized data matrix\n",
    "        \"\"\"\n",
    "        headers = dataframe.columns\n",
    "        X = preprocessing.normalize(dataframe.values)\n",
    "        return pd.DataFrame(X, columns=headers)\n",
    "\n",
    "print('------------------BEFORE------------------')\n",
    "print('------------------REP_HIST_SYSSTAT------------------')\n",
    "print(rep_hist_sysstat_df_pruned.shape)\n",
    "print('\\n')\n",
    "#print(rep_hist_snapshot_df_pruned.head())\n",
    "#\n",
    "# ROBUST SCALER\n",
    "# rep_hist_sysstat_df_pruned_norm = Normalizer.robust_scaler(dataframe=rep_hist_sysstat_df_pruned)\n",
    "#\n",
    "# MINMAX SCALER\n",
    "rep_hist_sysstat_df_pruned_norm = Normalizer.minmax_scaler(dataframe=rep_hist_sysstat_df_pruned)\n",
    "#\n",
    "# NORMALIZER\n",
    "#rep_hist_sysstat_df_pruned_norm = Normalizer.normalize(dataframe=rep_hist_sysstat_df_pruned)\n",
    "\n",
    "print('\\n\\n------------------AFTER------------------')\n",
    "print('------------------REP_HIST_SYSSTAT------------------')\n",
    "print(rep_hist_sysstat_df_pruned_norm.shape)\n",
    "print('\\n\\n')\n",
    "print('\\n\\nREP_HIST_SYSSTAT')\n",
    "print(rep_hist_sysstat_df_pruned_norm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Features / Labels\n",
    "\n",
    "Split the data frame into features and Labels (SNAP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = rep_hist_sysstat_df_pruned_norm['WORKAREA MEMORY ALLOCATED']\n",
    "del rep_hist_sysstat_df_pruned_norm['WORKAREA MEMORY ALLOCATED']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods\n",
    "\n",
    "* https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "\n",
    "Filter methods allow for the univariate analysis of features. Particularly, the following statistical methods have been considered and dismissed as explained below:\n",
    "\n",
    "* Information Gain - Data at hand is continous by nature, whilst MI is usually applicable against discrete values / binned labels.\n",
    "* Pearson Correlation Coefficient - Applicable to data with linear distributions, which is not the case for the majority of the presented features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImportance:\n",
    "    \"\"\"\n",
    "    This class is dedicated to containing methods related to feature ranking. Features are ranked exclusively, or mutually with\n",
    "    repsect to other features.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def f_regression_test(top_n_features=30, X_df=None, y_df=None, table=None):\n",
    "        \"\"\"\n",
    "        Carries out a f_regression test on passed X,y dataframes, selecting top N features ranked by highest scoring.\n",
    "        :param top_n_features: (Integer) Denotes number of top features to plot.\n",
    "        :param x_df:           (Pandas) Pandas feature matrix.\n",
    "        :param y_df:           (Pandas) Pandas label matrix.\n",
    "        :param table:          (String) Denotes which table is being operated on.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        headers = X_df.columns\n",
    "        X_df = X_df.values\n",
    "        y_df = y_df.values\n",
    "        f_regression_selector = SelectKBest(score_func=f_regression, k=top_n_features)\n",
    "        X_kbest = f_regression_selector.fit_transform(X_df, y_df)\n",
    "        print('\\n\\nTable [' + table.upper() + ']')\n",
    "        print('Original number of features: ' + str(X_df.shape[1]))\n",
    "        print('Reduced number of features: ' + str(X_kbest.shape[1]) + \"\\n\")\n",
    "        outcome = f_regression_selector.get_support()\n",
    "\n",
    "        scoring_sheet = []\n",
    "        for i in range(0,len(headers)-1):\n",
    "            if outcome[i]:\n",
    "                scoring_sheet.append([headers[i],f_regression_selector.scores_[i]])\n",
    "\n",
    "        scoring_sheet = sorted(scoring_sheet, key=itemgetter(1), reverse=True)\n",
    "        [print('Feature [' + str(row) + '] with score [' + str(score) + ']') for row, score in scoring_sheet[:top_n_features]]\n",
    "\n",
    "        scoring_sheet = pd.Series((v[1] for v in scoring_sheet[:top_n_features]) )\n",
    "        scoring_sheet[:top_n_features].plot.bar()\n",
    "        \n",
    "        FeatureImportance.__plot_metrics(ylabel='f_regression RANKING',\n",
    "                                         xlabel='FEATURES',\n",
    "                                         title='Features Ranked By f_regression Scoring')\n",
    "      \n",
    "    @staticmethod\n",
    "    def rfr_ranking(X_df=None, y_df=None, top_n_features=30, parallel_degree=1, max_depth=None, max_features='sqrt', n_estimators=100):\n",
    "        \"\"\"\n",
    "        Ranks features using a filter RFR method, and plots them in descending order (ranked by importance)\n",
    "        :param x_df:           (Pandas) Pandas feature matrix.\n",
    "        :param y_df:           (Pandas) Pandas label matrix.\n",
    "        :param top_n_features: (Integer) Denotes number of top features to plot.\n",
    "        :param parallel_degree:(Integer) Denotes model training parallel degree.\n",
    "        :param max_depth:      (Integer) Denotes number of leaves to evaluate during decision tree pruning.\n",
    "        :param max_features:   (Integer) Denotes number of features to consider during random subselection.\n",
    "        :param n_estimators:   (Integer) Number of estimators (trees) to build for decision making.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        headers = X_df.columns\n",
    "        X_df = X_df.values\n",
    "        y_df = y_df.values\n",
    "        rfr = RandomForestRegressor(n_estimators=int(n_estimators), \n",
    "                                    n_jobs=parallel_degree,\n",
    "                                    max_depth=max_depth,\n",
    "                                    max_features=max_features)\n",
    "        rfr.fit(X_df, y_df)\n",
    "        importances = pd.DataFrame({'feature':headers,\n",
    "                                    'importance':np.round(rfr.feature_importances_,3)})\n",
    "        importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "        print(importances[:top_n_features])\n",
    "        importances[:top_n_features].plot.bar()\n",
    "        \n",
    "        FeatureImportance.__plot_metrics(ylabel='RANDOM FOREST RANKING',\n",
    "                                         xlabel='FEATURES',\n",
    "                                         title='Features Ranked By Random Forest Scoring')\n",
    "        \n",
    "    @staticmethod\n",
    "    def __plot_metrics(ylabel, xlabel, title):\n",
    "        \"\"\"\n",
    "        Private method used to plot metrics for statistical feature evaluation\n",
    "        :param ylabel: (String) ylabel title name.\n",
    "        :param xlabel: (String) xlabel title name.\n",
    "        :param title:  (String) plot title name.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.title(title)\n",
    "        plt.rcParams['figure.figsize'] = [20, 15]\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f_regression\n",
    "\n",
    "Therefore, a likely candidate for a first univariate, filter test is a f_regression measure, between X labels 'SNAP_ID', and other output 'y' labels. NB: Due to f_regression requiring non-nagative values, a data normalization strategy was opted for (Refer to above cell).\n",
    "\n",
    "The following cell computes the f_regression value for each feature pertaining in the following tables, in relation to the SNAP_ID feature:\n",
    "* REP_HIST_SYSSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FeatureImportance.f_regression_test(top_n_features=top_n_features, \n",
    "                                    X_df=rep_hist_sysstat_df_pruned_norm, \n",
    "                                    y_df=y_labels, \n",
    "                                    table='REP_HIST_SYSSTAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Feature Importance\n",
    "\n",
    "Calculating MI (Mutual Information) scoring between data matrix X (feature vectors) and target column y ('SNAP_ID') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FeatureImportance.rfr_ranking(top_n_features=top_n_features,\n",
    "                              X_df=rep_hist_sysstat_df_pruned_norm, \n",
    "                              y_df=y_labels, \n",
    "                              parallel_degree=parallel_degree,\n",
    "                              max_depth=None,\n",
    "                              max_features='sqrt',\n",
    "                              n_estimators=rep_hist_sysstat_df_pruned_norm.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Methods\n",
    "\n",
    "Use a number of machine learning models to evaluate features together and rank by highest. The following machine learning models will be opted for:\n",
    "\n",
    "* Random Forest Classifier\n",
    "* Gradient Boosting\n",
    "\n",
    "In a 'Brute-Fore' approach, these machine learning heuristics will strip away 1 feature at a time in a method referred to as 'Recursive Feature Elimination', and compare accuracy with every variable elimination. This allows the respective classifier to establish an optimum feature configuration with the highest accuracy score.\n",
    "\n",
    "https://www.fabienplisson.com/choosing-right-features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleWrappers:\n",
    "    \"\"\"\n",
    "    This class contains wrapper methods, which utilize ensemble methods to gauge feature pairings. Features are combined together\n",
    "    and then stripped one at a time. Each feature combination is evaluated per feature count, so as to establish the optimum\n",
    "    feature count cut off.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def rfr_wrapper(X_df=None, y_df=None, test_split=.4, table_name=None, top_n_features=10, parallel_degree=1, max_depth=None, max_features='sqrt', n_estimators=100):\n",
    "        \"\"\"\n",
    "        Random Forest Regressor - Takes data matrix and target vector, and evaluates best combination of features \n",
    "        using an RFR model.\n",
    "        :param x_df:           (Pandas) Pandas feature matrix.\n",
    "        :param y_df:           (Pandas) Pandas label matrix.\n",
    "        :param test_split:     (Float) Denotes training/testing data split.\n",
    "        :param table_name:     (String) Denotes which table is being operated on.\n",
    "        :param top_n_features: (Integer) Denotes number of top features to plot.\n",
    "        :param parallel_degree:(Integer) Denotes model training parallel degree.\n",
    "        :param max_depth:      (Integer) Denotes number of leaves to evaluate during decision tree pruning.\n",
    "        :param max_features:   (Integer) Denotes number of features to consider during random subselection.\n",
    "        :param n_estimators:   (Integer) Number of estimators (trees) to build for decision making.\n",
    "        :return: Model Scoring, for best feature combination count\n",
    "        :return: Recommended feature count, as per best achieved score\n",
    "        \"\"\"\n",
    "        X_df = X_df.values\n",
    "        y_df = y_df.values\n",
    "\n",
    "        val_op, optimum_features = 0, 0\n",
    "        val_op = 0\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_split)\n",
    "        model = RandomForestRegressor(n_estimators=int(n_estimators),\n",
    "                                      n_jobs=parallel_degree,\n",
    "                                      max_depth=max_depth,\n",
    "                                      max_features=max_features)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # make predictions for test data and evaluate\n",
    "        pred_y = model.predict(X_test)\n",
    "        predictions = [round(value) for value in pred_y]\n",
    "        r2s = r2_score(y_test, predictions)\n",
    "        print(\"Table [\" + table_name + \"] RFR R2 Score: \" + str(r2s))\n",
    "\n",
    "        # fit model using each importance as a threshold\n",
    "        print('Feature Importance\\n' + str('-'*60))\n",
    "        print(model.feature_importances_)\n",
    "        print(str('-'*60))\n",
    "        thresholds = np.sort(model.feature_importances_)\n",
    "        feature_counts, feature_score = [],[]\n",
    "        for thresh in thresholds:\n",
    "            # selecting features using threshold\n",
    "            selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "            select_train_x = selection.transform(X_train)\n",
    "\n",
    "            # training model\n",
    "            selection_model = RandomForestRegressor(n_estimators=int(n_estimators),\n",
    "                                                    n_jobs=parallel_degree,\n",
    "                                                    max_depth=max_depth,\n",
    "                                                    max_features=max_features)\n",
    "            selection_model.fit(select_train_x, y_train)\n",
    "\n",
    "            # evaluating model\n",
    "            select_test_x = selection.transform(X_test)\n",
    "            pred_y = selection_model.predict(select_test_x)\n",
    "            predictions = [round(value) for value in pred_y]\n",
    "            r2s = r2_score(y_test, predictions)\n",
    "            print(\"Thresh=\" + str(thresh) + \", n=\" + str(select_train_x.shape[1]) + \", R2 Score: \" + str(r2s))\n",
    "            if(r2s > val_op):\n",
    "                val_op = r2s\n",
    "                optimum_features = select_train_x.shape[1]\n",
    "\n",
    "            # Add/Keep track of '[no of features','r2 score']\n",
    "            feature_counts.append(select_train_x.shape[1])\n",
    "            feature_score.append(r2s)\n",
    "\n",
    "        # Plot feature count performance\n",
    "        EnsembleWrappers.__plot_metrics(feature_counts=feature_counts,\n",
    "                                        feature_score=feature_score,\n",
    "                                        xlabel='Featuers',\n",
    "                                        ylabel='R2 Score',\n",
    "                                        title='Feature Pairing Performance')\n",
    "\n",
    "        return val_op, optimum_features\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_boosting_wrapper(X_df=None, y_df=None, test_split=.4, table_name=None, top_n_features=10, n_estimators=100, max_features='sqrt', max_depth=None,):\n",
    "        \"\"\"\n",
    "        Gradient Boosting Regressor - Takes data matrix and target vector, and evaluates best combination of features \n",
    "        using a GBR model.\n",
    "        :param x_df:           (Pandas) Pandas feature matrix.\n",
    "        :param y_df:           (Pandas) Pandas label matrix.\n",
    "        :param test_split:     (Float) Denotes training/testing data split.\n",
    "        :param table_name:     (String) Denotes which table is being operated on.\n",
    "        :param top_n_features: (Integer) Denotes number of top features to plot.\n",
    "        :param parallel_degree:(Integer) Denotes model training parallel degree.\n",
    "        :param n_estimators:   (Integer) Number of estimators (trees) to build for decision making.\n",
    "        :param max_features:   (Integer) Denotes number of features to consider during random subselection.\n",
    "        :param max_depth:      (Integer) Denotes number of leaves to evaluate during decision tree pruning.\n",
    "        :return: Model Scoring, for best feature combination count\n",
    "        :return: Recommended feature count, as per best achieved score\n",
    "        \"\"\"\n",
    "        X_df = X_df.values\n",
    "        y_df = y_df.values\n",
    "        val_op, optimum_features = 0, 0\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, \n",
    "                                                            y_df, \n",
    "                                                            test_size=test_split)\n",
    "        model = GradientBoostingRegressor(n_estimators=int(n_estimators),\n",
    "                                          max_features=max_features,\n",
    "                                          max_depth=max_depth)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # make predictions for test data and evaluate\n",
    "        pred_y = model.predict(X_test)\n",
    "        predictions = [round(value) for value in pred_y]\n",
    "        r2s = r2_score(y_test, predictions)\n",
    "        print(\"Table [\" + table_name + \"] RFR R2 Score: \" + str(r2s))\n",
    "        #\n",
    "        # fit model using each importance as a threshold\n",
    "        print('Feature Importance\\n' + str('-'*60))\n",
    "        print(model.feature_importances_)\n",
    "        print(str('-'*60))\n",
    "        thresholds = np.sort(model.feature_importances_)\n",
    "        feature_counts, feature_score = [],[]\n",
    "        for thresh in thresholds:\n",
    "            # selecting features using threshold\n",
    "            selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "            select_train_x = selection.transform(X_train)\n",
    "\n",
    "            # training model\n",
    "            selection_model = GradientBoostingRegressor(n_estimators=int(n_estimators),\n",
    "                                                        max_features=max_features,\n",
    "                                                        max_depth=max_depth)\n",
    "            selection_model.fit(select_train_x, y_train)\n",
    "\n",
    "            # evaluating model\n",
    "            select_test_x = selection.transform(X_test)\n",
    "            pred_y = selection_model.predict(select_test_x)\n",
    "            predictions = [round(value) for value in pred_y]\n",
    "            r2s = r2_score(y_test, predictions)\n",
    "            print(\"Thresh=\" + str(thresh) + \", n=\" + str(select_train_x.shape[1]) + \", R2 Score: \" + str(r2s))\n",
    "            if(r2s > val_op):\n",
    "                val_op = r2s\n",
    "                optimum_features = select_train_x.shape[1]\n",
    "\n",
    "            # Add/Keep track of '[no of features','r2 score']\n",
    "            feature_counts.append(select_train_x.shape[1])\n",
    "            feature_score.append(r2s)\n",
    "\n",
    "        # Plot feature count performance\n",
    "        EnsembleWrappers.__plot_metrics(feature_counts=feature_counts,\n",
    "                                        feature_score=feature_score,\n",
    "                                        xlabel='Featuers',\n",
    "                                        ylabel='R2 Score',\n",
    "                                        title='Feature Pairing Performance')\n",
    "\n",
    "        return val_op, optimum_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def __plot_metrics(feature_counts, feature_score, ylabel, xlabel, title):\n",
    "        \"\"\"\n",
    "        Private method used to plot metrics for statistical feature evaluation\n",
    "        :param feature_counts: (List) List of feature combination counts per achieved score.\n",
    "        :param feature_score:  (List) List of feature scores per combination.\n",
    "        :param ylabel:         (String) ylabel title name.\n",
    "        :param xlabel:         (String) xlabel title name.\n",
    "        :param title:          (String) plot title name.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.title(title)\n",
    "        plt.rcParams['figure.figsize'] = [20, 15]\n",
    "        plt.plot(feature_counts, feature_score)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Wrapper (Feature Combination)  (Regression)\n",
    "\n",
    "Utilizes an ensemble Random Forest method to gauge different feature combination/counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfr_hist_sysstat_score, rfr_hist_sysstat_count = EnsembleWrappers.rfr_wrapper(X_df=rep_hist_sysstat_df_pruned_norm,\n",
    "                                                                              y_df=y_labels,\n",
    "                                                                              test_split=test_split,\n",
    "                                                                              table_name='REP_HIST_SYSSTAT',\n",
    "                                                                              top_n_features=top_n_features,\n",
    "                                                                              parallel_degree=parallel_degree,\n",
    "                                                                              max_depth=None,\n",
    "                                                                              max_features='sqrt',\n",
    "                                                                              n_estimators=rep_hist_sysstat_df_pruned_norm.shape[1]*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Wrapper (Feature Combination)  (Regression)\n",
    "\n",
    "Utilizes an ensemble Gradient Boosting method to gauge different feature combination/counts.\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbw_hist_sysstat_score, gbw_hist_sysstat_count = EnsembleWrappers.gradient_boosting_wrapper(X_df=rep_hist_sysstat_df_pruned_norm,\n",
    "                                                                                            y_df=y_labels,\n",
    "                                                                                            test_split=test_split,\n",
    "                                                                                            table_name='REP_HIST_SYSSTAT',\n",
    "                                                                                            top_n_features=top_n_features,\n",
    "                                                                                            n_estimators=rep_hist_sysstat_df_pruned_norm.shape[1]*2,\n",
    "                                                                                            max_features='sqrt',\n",
    "                                                                                            max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (Regression)\n",
    "\n",
    "Implements a recursive solution, where in features are eliminated based on an ensemble evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEliminator:\n",
    "    \"\"\"\n",
    "    This class is dedicated to housing logic pertaining to feature selection - retaining only labels which are considered\n",
    "    important.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_df, y_df):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        :param X_df: (Pandas) Pandas feature matrix.\n",
    "        :param y_df: (Pandas) Pandas label matrix.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__X_df = X_df\n",
    "        self.__y_df = y_df\n",
    "    \n",
    "    def rfe_selector(self, test_split=.4, optimum_feature_count=0, model=None, parallel_degree=1, max_depth=None, max_features='sqrt', n_estimators=100):\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination Function. Isolates and eliminated features one by one, up till the desired amount, starting\n",
    "        by features which are considered less important.\n",
    "        :param test_split:            (Float) Denotes training/testing data split.\n",
    "        :param optimum_feature_count: (Integer) Denotes the best estimated number of features to retain before a performance drop\n",
    "                                                is estimated.\n",
    "        :param parallel_degree:       (Integer) Denotes model training parallel degree.\n",
    "        :param max_depth:             (Integer) Denotes number of leaves to evaluate during decision tree pruning.\n",
    "        :param max_features:          (Integer) Denotes number of features to consider during random subselection.\n",
    "        :param n_estimators:          (Integer) Number of estimators (trees) to build for decision making.\n",
    "        :return: (List) This list is composed of boolean values, which correspond to the input feature column headers. True List \n",
    "                        values denote columns which have been retained. False values denote eliminated feature headers.\n",
    "        :return: (List) This list denotes feature rankings, which correspond to the input feature column headers. Values of '1',\n",
    "                        denote that features have been retained.\n",
    "        \"\"\"\n",
    "        X_df = self.__X_df.values\n",
    "        y_df = self.__y_df.values\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_split)\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        if model == 0:\n",
    "            model = RandomForestRegressor(n_estimators=int(n_estimators), \n",
    "                                          n_jobs=parallel_degree,\n",
    "                                          max_depth=max_depth,\n",
    "                                          max_features=max_features)\n",
    "        elif model == 1:\n",
    "            model = GradientBoostingRegressor(n_estimators=100,\n",
    "                                              max_depth=1,\n",
    "                                              max_features=max_features)  \n",
    "\n",
    "        # create the RFE model and select N attributes\n",
    "        rfe_model = RFE(model, optimum_feature_count, step=1)\n",
    "        rfe_model = rfe_model.fit(X_train, y_train)\n",
    "\n",
    "        # summarize the selection of the attributes\n",
    "        print(rfe_model.support_)\n",
    "        print(rfe_model.ranking_)\n",
    "\n",
    "        # evaluate the model on testing set\n",
    "        pred_y = rfe_model.predict(X_test)\n",
    "        predictions = [round(value) for value in pred_y]\n",
    "        r2s = r2_score(y_test, predictions)\n",
    "        \n",
    "        return rfe_model.support_, rfe_model.ranking_\n",
    "    \n",
    "    def get_selected_features(self, column_mask):\n",
    "        \"\"\"\n",
    "        Retrieves features which have not been eliminated from the RFE function.\n",
    "        :param column_mask: (List) This list is composed of boolean values, which correspond to the input feature column headers. \n",
    "                                   True list values denote columns which have been retained. False values denote eliminated \n",
    "                                   feature headers. \n",
    "        :return: (Pandas) Pandas data matrix.\n",
    "        \"\"\"\n",
    "        recommended_columns = []\n",
    "        for i in range(len(self.__X_df.columns)):\n",
    "            if (column_mask[i]):\n",
    "                recommended_columns.append(self.__X_df.columns[i])\n",
    "                \n",
    "        return self.__X_df[recommended_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rfr_hist_sysstat_score > gbw_hist_sysstat_score:\n",
    "    rep_hist_sysstat_op = rfr_hist_sysstat_count\n",
    "    rep_hist_sysstat_model = 0\n",
    "else:\n",
    "    rep_hist_sysstat_op = gbw_hist_sysstat_count\n",
    "    rep_hist_sysstat_model = 1\n",
    "\n",
    "if rep_hist_sysstat_op == 0:\n",
    "    rep_hist_sysstat_op = 1  # used to avoid having experiment to crash at this stage.    \n",
    "    \n",
    "print('Recommended Feature Drop [' + str(rep_hist_sysstat_op) + '] using model [' + str(rep_hist_sysstat_model) + ']')\n",
    "fe = FeatureEliminator(X_df=rep_hist_sysstat_df_pruned_norm,\n",
    "                       y_df=y_labels)\n",
    "column_mask, column_rankings = fe.rfe_selector(test_split=test_split,\n",
    "                                               optimum_feature_count=rep_hist_sysstat_op,\n",
    "                                               model = rep_hist_sysstat_model,\n",
    "                                               parallel_degree=parallel_degree,\n",
    "                                               max_depth=None,\n",
    "                                               max_features='sqrt',\n",
    "                                               n_estimators=rep_hist_sysstat_df_pruned_norm.shape[1]*2)\n",
    "print(rep_hist_sysstat_df_pruned_norm.columns)\n",
    "rep_hist_sysstat_df_pruned_norm = fe.get_selected_features(column_mask=column_mask)\n",
    "print(rep_hist_sysstat_df_pruned_norm.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Decomposition\n",
    "\n",
    "Principal component analysis: Factor model in which the factors are based on summarizing the total variance. With PCA, unities are used in the diagonal of the correlation matrix computationally implying that all the variance is common or shared.\n",
    "\n",
    "https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrincipalComponentAnalysisClass:\n",
    "    \"\"\"\n",
    "    This class handles logic related to PCA data transformations.\n",
    "    https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
    "    \"\"\"\n",
    "    def __init__(self, X_df):\n",
    "        \"\"\"\n",
    "        Cosntructor method.\n",
    "        :param X_df: (Pandas) Dataframe consisting of input features, which will be subject to PCA.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.__X_df = X_df\n",
    "        \n",
    "    def get_default_component_variances(self):\n",
    "        \"\"\"\n",
    "        Fitting the PCA algorithm with our Data.\n",
    "        :return: (Numpy array) Array of feature variances.\n",
    "        \"\"\"\n",
    "        pca = PCA().fit(self.__X_df.values)\n",
    "        return np.cumsum(pca.explained_variance_ratio_)\n",
    "        \n",
    "    def get_default_component_count(self, threshold=.99):\n",
    "        \"\"\"\n",
    "        Retrieves the recommended number of component decomposition, above which very little variance \n",
    "        gain is achieved. This treshold will be set at a 0.999 variance threshold.\n",
    "        :param threshold: (Float) Threshold value between 0 and 1. Stops immediately as soon the number\n",
    "                                  of required components exceeds the threshold value.\n",
    "        :return: (Integer) Returns the number of recommended components.\n",
    "        \"\"\"\n",
    "        variance_ratios = self.get_default_component_variances()\n",
    "        n = 0\n",
    "        for val in variance_ratios:\n",
    "            if val < threshold:\n",
    "                n += 1\n",
    "        return n\n",
    "    \n",
    "    def plot_variance_per_reduction(self):\n",
    "        \"\"\"\n",
    "        This method subjects the feature matrix to a PCA decomposition. The number of components is plot\n",
    "        vs the amount of retained variance.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        variance_ratios = self.get_default_component_variances()\n",
    "        \n",
    "        #Plotting the Cumulative Summation of the Explained Variance\n",
    "        plt.figure()\n",
    "        plt.plot(variance_ratios)\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Variance (%)') #for each component\n",
    "        plt.title(tpcds + ' Dataset Explained Variance')\n",
    "        plt.show()\n",
    "        \n",
    "    def apply_PCA(self, n_components):\n",
    "        \"\"\"\n",
    "        Applies Principle Component Analysis on the constructor passed data matrix, on a number of components.\n",
    "        A new pandas data matrix is returned, with renamed 'Principal Component' headers.\n",
    "        :param n_components: (Integer) Denotes number of component breakdown.\n",
    "        :return: (Pandas) Dataframe consisting of new decomposed components.\n",
    "        \"\"\"\n",
    "        pca = PCA(n_components=n_components)\n",
    "        dataset = pca.fit_transform(self.__X_df.values)\n",
    "        header_list = []\n",
    "        for i in range(dataset.shape[1]):\n",
    "            header_list.append('Component_' + str(i))\n",
    "        return pd.DataFrame(data=dataset, columns=header_list)\n",
    "\n",
    "print(rep_hist_sysstat_df_pruned_norm.head())\n",
    "print(rep_hist_sysstat_df_pruned_norm.shape)\n",
    "\n",
    "pcac = PrincipalComponentAnalysisClass(X_df=rep_hist_sysstat_df_pruned_norm)\n",
    "pcac.plot_variance_per_reduction()\n",
    "component_count = pcac.get_default_component_count()\n",
    "rep_hist_sysstat_df_pruned_norm = pcac.apply_PCA(n_components=component_count)\n",
    "\n",
    "print('-'*30)\n",
    "print(rep_hist_sysstat_df_pruned_norm.head())\n",
    "print(rep_hist_sysstat_df_pruned_norm.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
